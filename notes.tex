% Created 2018-12-11 Tue 10:50
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage[left=2cm, right=2cm, bottom=2cm, top=2cm]{geometry}
\usepackage{parskip}
\def\R{\mathbb{R}}
\def\C{\mathbb{C}}
\def\N{\mathbb{N}}
\def\Z{\mathbb{Z}}
\def\Q{\mathbb{Q}}
\def\pos{\operatorname{pos}}
\def\conv{\operatorname{Conv}}
\usepackage[T1]{fontenc}
\author{Harikrishnan Mulackal}
\date{\today}
\title{Functional Analysis 1}
\hypersetup{
 pdfauthor={Harikrishnan Mulackal},
 pdftitle={Functional Analysis 1},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.1 (Org mode 9.1.13)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents

\section{Lecture 2 \textit{<2018-10-17 Wed>}}
\label{sec:orgf3b94e9}

\subsection{Examples of metric spaces}
\label{sec:orgf0187c1}

\subsubsection{The set of continuous functions \(C[a, b]\)}
\label{sec:orgfbd673e}
With a metric \(d(f, g) = \max\vert f(x) - g(x)\vert\)

\subsubsection{The space \(l^p\) of sequences \(x=(x_1, x_2 \cdots,)\) with \(\Vert x \Vert _p < \infty\).}
\label{sec:org5215eb9}
The metric is the norm of the difference. 

To prove that this is indeed a metric, we need the Minkowiski inequality.

There was a complicated proof of the Minkowski inequality that was discussed
in the lecture. I think one can prove in a simpler fashion. 

The proof involved showing that \(\alpha \cdot \beta \le \alpha^{p}/p +
    \beta^{q}/q\) (This is the young inequality.)

The next step was to prove the Holder inequality. 

The next step is to use Holder on Minkowski. 

Holder: \url{https://en.wikipedia.org/wiki/H\%C3\%B6lder\%27s\_inequality}

Minkowski: \url{https://en.wikipedia.org/wiki/Minkowski\_inequality}

For \(p=q=2\), we obtain the Cauchy-Schwarz inequality. 

\subsection{Definitions involving metric spaces}
\label{sec:org4ce50a7}

\subsubsection{Open ball}
\label{sec:orgdd7d80c}
\(B(x_0, r)\) is the set of all points \(x\) such that the distance from \(x\) to \(x_0\) is less than \(r\).

\subsubsection{Closed ball}
\label{sec:orgc245423}
Similar to open ball, except that the distance can be less than or \emph{equal to} \(r\).

\subsubsection{Sphere}
\label{sec:org8ff4633}
The set of all points \(x\) such that the distance from \(x_0\) to \(x\) is exactly \(r\).

\subsubsection{Relation}
\label{sec:orgf053faa}
If we subtract a sphere from the closed ball, we get the open ball.

\subsubsection{Definition of open}
\label{sec:org86edef1}
For every point \(x\), we can find an epsilon ball that is inside the set. 

\subsubsection{Definition of closed}
\label{sec:org118b884}
Basically the complement of the set is open.

\subsubsection{Remark (\(\varepsilon\) neighbourhood)}
\label{sec:org54e0a90}
An open ball \(B(x_0, \varepsilon)\) of radius \(\varepsilon\) is often called
an \(\varepsilon\) neighbourhood. A neighbourhood of \(X\) is that contains an
\(\varepsilon\) neighbourhood of \(X\)

We will use these definition for separable spaces.

\subsubsection{Interior point}
\label{sec:org826e7d2}
We call \(x_0\) an interior point if there exist an neighbourhood of \(x_0\)
contained inside the space.

\subsubsection{Continuity}
\label{sec:orgb7f36bf}
About a function from one metric space to another. It's basically the
standard definition. (the definition in the lecture was an \(\varepsilon\),
\(\delta\) definition.) 

The definition was about continuity at a point \(x\). Then a function is
defined to be continuous if it is continuous at every point \(x\).

\subsubsection{Accumulation point}
\label{sec:org859b223}
\(A\) be a subset of \(X\). Then \(x_0 \in X\) (not necessarily in \(A\)) is called
accumulation point of \(A\) if every neighbourhood of \(X_0\) contains at least
one point \(y \in A\), \(y \neq x_0\).

\section{Lecture 3 \textit{<2018-10-23 Tue>}}
\label{sec:org15f9aab}
\footnote{A14 1126 tutorial}
\subsection{More definitions}
\label{sec:org1f7286a}
\subsubsection{Closure of a set}
\label{sec:orgbb8b424}
\subsubsection{A dense subset of \(X\)}
\label{sec:orga840254}
\subsubsection{Separable}
\label{sec:org18bba35}
If there is a countable subset that is dense in \(X\). \(X\) is defined to be
separable.

Example:
\begin{enumerate}
\item Real numbers are separable.
\item \(l^p\) for \(1 \le p \le \infty\) is separable.
\item \(l^\infty\) not separable. Proof idea: For any set of indices \(I\subset
       \N\), define \((e_I)_n\) is \(1\) is \(h\in I\) and \(0\) otherwise. For every \(I
       \neq y\), we can compute the distance \(d(e_I, e_J) = 1\). How many such
functions are there? The number of such elements are uncountable. (The
proof of this theorem is similar to how \(\{0, 1\}^\N\) is uncountable.)\footnote{Look up at proofs of this fact}
\end{enumerate}
\subsubsection{Definition of convergence}
\label{sec:orge2be736}
Definition was about the limit of distance going to 0. Note that the limit
\(X\) must be a point in \(X\) (Why?)
\subsubsection{Definition of bounded set}
\label{sec:org17cffdd}
\subsubsection{Definition of boundedness of a sequence}
\label{sec:orgdf1535c}
\subsubsection{Lemma}
\label{sec:org73df8d1}
If \(x_n\) is convergent, then it is bounded the limit is unique.
\subsubsection{Lemma}
\label{sec:org98fa55e}
If \(x_n\) converges to \(x\) and \(y_n\) converges to \(y\), then \(d(x_n, y_n)\)
converges to \(d(x, y)\).
\subsubsection{About convergence and Cauchy sequences}
\label{sec:org400e7f0}
\subsubsection{Completeness}
\label{sec:org820a8fd}
The idea that if a sequence is Cauchy, then it is convergent.
\subsubsection{Theorem about subsets of complete metric space}
\label{sec:org7f66018}
\(X\) be a metric space. A subspace \(A\) of \(X\) is complete if and only if \(A\) is closed in \(X\).
\subsubsection{Condition about continuity with respect to convergence}
\label{sec:org02a8001}
A map \(T\colon X \rightarrow Y\) is continuous if and only if \(x_n
    \rightarrow x_0 \implies T(x_n) \rightarrow T(x_0)\).
\subsubsection{Definition of isometry between metric spaces}
\label{sec:orgd5be4ee}
\subsection{Theorem: \(C[a, b]\) is complete}
\label{sec:org09df06f}
Given a closed interval \([a, b]\), the \(C[a, b]\) is complete. 
\subsubsection{Proof}
\label{sec:orgcfd221d}
Given a Cauchy sequence in this space. We use the completeness of \(\R\) to
define a function to which the Cauchy sequence will converge to. We need
uniform convergence here.
\subsection{The set of polynomials on a \([a, b]\) is not complete}
\label{sec:orgcae5e13}
\(p_n(x) = \sum_i \left(\frac{x}{2}\right)^i\). 

\section{Lecture 4 \textit{<2018-10-25 Thu>}}
\label{sec:org310adb3}
\subsection{Clarification}
\label{sec:org2a65a0e}
Some sources say that an Isometry need not be surjective. 

Let \(T\colon X \rightarrow Y\) be injective and \(d(x, y) = \tilde{d}(Tx, Ty)
   \forall x, y \in X\).

Then \(T\) is called isometry (from \(X\)) into \(Y\). 

A couple of examples of isometry were discussed. 

Also we talked about uniform convergence, and also an example of a sequence
that is not uniformly convergent was also discussed. (The idea is that on a
compact set \footnote{Is compactness needed?}, if a uniformly convergent function converges to a
function is continuous.)

Recall that we have already shown that \(C[0, 1]\) is a complete space, meaning
that if a sequence is Cauchy, then it has a limit in the space. Which means
that the limit must also be continuous. (Kinda similar to how uniform
convergence works.)
\subsection{Normed spaces and Banach spaces}
\label{sec:org8a7cd3c}
A Banach space is just a complete normed space.
\subsubsection{Norm}
\label{sec:org5d4c325}
A metric space can be obtained by equipping a vector space with a metric
defined in terms of the union, resulting in the \textbf{normed space}. Complete
normed spaces are called \textbf{Banach space}.

A mapping from one normed space to another is called an \textbf{operator} (\$T\(\colon\)
X \(\rightarrow\) Y). A mapping from a normed space to \(\R\) is called a
\textbf{functional} (\(T\colon X \rightarrow \R\))

It can be shown that a linear operator is continuous if and only if it is
bounded.

The set of all bounded linear operators from a space \(X\) to a space \(Y\) is
again a normed space.

Similarly, the set of all bounded linear functionals is a normed space, the
dual space \(X'\) of \(X\).
\subsubsection{Vector space}
\label{sec:org5df3342}
I didn't write this down. But it's pretty clear. 
\subsubsection{Examples}
\label{sec:org8180375}
\begin{enumerate}
\item \(\R^n\)
\item \(C[a, b]\). It's kinda clear how to define a vector space structure on it.
\end{enumerate}
\subsubsection{Subspace}
\label{sec:orgaed4087}
We defined a subspace of a vector space. 
\subsubsection{Span}
\label{sec:org1c13aa8}
Span was defined.
\subsubsection{Linear independence}
\label{sec:org3822c09}
\subsubsection{Definition of dimension}
\label{sec:orgaf73272}
\subsubsection{Basis of space}
\label{sec:org872df4f}
To construct basis for finite dimensional spaces are clear. 

One can prove the existence of a basis for infinite dimensional spaces, but
the proof is not constructive.
\subsection{Normed spaces}
\label{sec:orgebdfb68}
\subsubsection{Definition of a norm}
\label{sec:org6b92014}
\subsubsection{Examples}
\label{sec:orgc738df6}
\begin{enumerate}
\item \(\R^n\)
\item \(C[a, b]\) with \(\Vert f \Vert\) with \(\Vert f\Vert = \max_{x \in [a, b]}
       \vert f (x) \vert\). Is also a Banach space.
\item \(\Omega \in \R^n\) be a measurable set, then the space \(L^p(\Omega)\) is
the set of all Lebesgue measurable functions from \(\Omega \rightarrow
       \R\). We can define the norm in a straightforward manner.

We can show that this space is a Banach space.
\end{enumerate}
\section{Lecture 5 \textit{<2018-10-30 Tue>}}
\label{sec:org6724a3d}
\subsection{Remark}
\label{sec:orgac858f8}
Not every metric is induced by a norm.

Assume discrete metric is induced by a norm, i.e., \(d(x, y) = \Vert x -
   y\Vert\). The proof is easy.
\subsection{Convergence of sequences in normed spaces}
\label{sec:orgb9019df}
\begin{enumerate}
\item A sequence \((x_n)\) in a normed space \(X\) is convergent if there is an \(x\in
      X\) with \(\lim_{n\rightarrow \infty} \Vert x_n - x\Vert = 0\). We write again
\(\lim_{n\rightarrow \infty} x_n = x\).
\item A sequence is called a cauchy sequence if for all \(\epsilon > 0\), there
exists \(N\), \(\Vert x_m - x_m \Vert < \epsilon\) for all \(m, n > N\). \footnote{This might be needed for the next exercises.}
\end{enumerate}
\subsection{Absolute convergence}
\label{sec:org7affcde}
A series \(S\) is called absolutely convergent if \(\sum \Vert x_i \Vert\)
converges. If \(X\) is complete, absolute convergence implies convergence.

Example 1. The alternating sum \(\sum (-1)^n/n\) is convergent, but not
absolutely convergent. This converges to \(-ln(2)\).

Example 2. \(X = \Q\), \(a_n = \sum \left(\frac{1}{2^{i}} -
   \frac1{(i+1)!}\right)\). The first part converges to \(1\). The second element
converges to \(e-2\). The total is \(3-e\). Let \(b_n\) be \(\sum
   \frac{-1}{(i+1)!}\). This converges to \(2 - e\). Now \(a + b = 5 - 2e\). \(a - b =
   1\). One of them converges to an element in \(\Q\) whereas the other one does
not convert to an element in \(\Q\). Now we construct a new sequence with one
element from the first one and the next element from the second one and
continue doing this \((a_1, b_1, a_2, b_2, \cdots)\). \(S_n\) will be the partial
sums. Then \(\sum x_i\) converges to \(a+b\), but \$\(\sum\) \(\vert{}\) x\(_{\text{i}}\) \(\vert{}\) \$
converges to \(a - b = 1\).
\subsection{Definition of basis}
\label{sec:org600d4ac}
We can now define a basis as follows: Assume that the normed space \(X\)
contains a sequence \((e_n)_n\). Such that every \(x \in X\) in the normed space
\(X\), can be expressed in terms of \((e_n)_n\), i.e., every \(x\) can be expressed
as a weighted combination or linear sums of \((e_n)_n\). \(\Vert x - \sum
   \alpha_i e_i \Vert \rightarrow 0\) as \(n \rightarrow \infty\) and this
expansion is also unique.

If such a basis exists, then \(X\) is separable.
\subsection{Theorem about completeness}
\label{sec:org297b481}
Every finite dimensional subspace \(Y\) of a complete normed space \(X\) is
complete, which implies that all finite dimensional normed spaces are
complete. \footnote{The proof wasn't done in the class. But the idea is the all Cauchy
sequences converges.}
\subsection{Norm equivalence}
\label{sec:orgd5d6f43}
We have two norms. We want to say when two norms are equivalent.

A norm \(\Vert \Vert_1\) on \(X\) is said to be equivalent to a norm \(\Vert
   \Vert_2\) if there exists \(a, b > 0\) such that

\(a \Vert x \vert_1 \le \Vert x \vert_2 \le b \Vert x \Vert_2\).

An example: For finite-dimension vector space, then all norms are equivalent.

We'll show that \(\Vert \Vert_2\), and \(\Vert \Vert_\infty\) are equivalent.

Given any \(X \in \R^n\), \(\Vert x\Vert_2^2 = \sum_{i=1}^{n} x_i^2 \ge \max_i
   x_i^2 = \Vert x\Vert_\infty\). Also \(\Vert x \Vert_2^2 = \sum_{i=1}^{n} x_i^2
   \le n \cdot \max x_i^2 = n \Vert x \Vert_\infty\). Thus \(\Vert x \Vert_\infty
   \le \Vert x\Vert_2 \le \sqrt{n}\Vert x \Vert_\infty\).

\textbf{A non-example}: Consider \(X = C[0, 1]\) and define \(f_n(X) = X^n\). Clearly,
\(f_n \in C[0, 1]\) for al l\(n\).

\(\Vert f_n \Vert_\infty = \max_{x\in [0, 1]} \vert f_n(x)\vert\). Now we
introduce another norm which is the Lebesgue integral. For \(f_n\), this would
evaluate to \(1/{n+1}\). The contradiction is the fact that the maximal norm
would be \(1\) always, whereas, the Lebesgue norm would tend to \(0\) as
\(n\rightarrow \infty\). There is a clear contradiction here.
\subsection{Compactness in metric spaces}
\label{sec:orgc73744b}
A metric space is defined to be compact if every sequence in \(X\) has a
convergent subsequence. A subset \(M\) of \(X\) is defined to be compact if \(M\)
considered as a subspace is compact, i.e., every sequence has a convergent
subsequence and the limit is in \(M\).
\subsection{Theorem}
\label{sec:org3f7cf8f}
If \(X\) is a fininte-dimensional normed space, then any subset \(M \subset X\)
is compact if and only if \$M is closed and bounded.

Consider the sequence \(\{x_n\}\) with \(x_n = (-1)^n\), then \((x_n)\) does not
converge, but it has convergent subsequences.
\subsection{Bolzano-Weistrass theorem}
\label{sec:org836df44}
Any bounded sequence \((x_n) \in l^{\infty}\) has a convergent subsequence.
\subsubsection{Proof}
\label{sec:orgd26fc49}
Without loss of generality, we assume that all elements are with \([0, 1]\),
otherwise we an shift it and normalize it (can we do this?) Divide the
interval \(\{0, 1/2\}\) and \(\{1/2, 1\}\), then one of them must have
infinitely many points. (We can repeat the argument), we have a new sequence
which are elements of \(x_n\) that are in the interval (the interval with
infinitely many points.) We can repeat this process again and the length of
the intervals go to zero. It is clear how to construct a convergent
subsequence.
\subsubsection{Extension to bounded sub-sequences in \(\R^n\).}
\label{sec:org6cc9869}
\section{Lecture 5 \textit{<2018-11-01 Thu>}}
\label{sec:org9a444a1}
\subsection{Review}
\label{sec:org593cb48}
In \(\R^n\), the compact subsets are the closed and bounded subsets so that
close-ness and boundedness can be used to define compactness. This can only
be used for finite dimensional cases.
\subsection{Riesz's lemma}
\label{sec:orgb78189e}
Given a normed space \(X\) with a closed subspace \(y\) and a subspace \(z\) such
that \(y\) is a subset of \(z\). Given any number \(\theta\in (0, 1)\), there
exists, \(z\in Z\) such that \(\vert z \vert = 1\), and the distance \(\Vert z - y
   \Vert \ge \theta\), for all \(y \in Y\).
\subsubsection{Proof}
\label{sec:org9c39948}
Let \(v\in Z \setminus Y\), define \(a = \inf_{y \in Y} \Vert v - y\Vert\) to
be the distance to \(y\). Since \(y\) is closed, \(a>0\). Choose \(\theta \in (0,
    1)\), then there exits a \(y_0 \in y\), with \(a \le \Vert v - y_0 \Vert \le
    \frac{a}{\theta}\).

Define \(z = \vert{1}{\Vert V = y_0}(V-y_0)\). Clearly, \(\Vert z\Vert = 1\).
Furthermore, given \(y\in Y\), it holds that \(\Vert z - y\Vert =
    \Vert\frac{1}{\Vert v - y_0\Vert}(v -y_0) - y\Vert = \frac{1}{\Vert v -
    y_0\Vert} \Vert v - y_0 - \Vert v-y_0\Vert\cdot y\Vert\).

Since \(Y\) is a subspace, \(y_1 \in Y\) and \(\Vert v - y_1\Vert \ge a\) (since a
is the infimum.)

\(\Vert z - y\Vert = \frac{1}{\Vert v - y_0\Vert} \Vert V - y_1\Vert \le
    \frac{\theta}{a}a = \theta\)
\subsection{Theorem}
\label{sec:orgeaf7350}
If the closed unit ball \(M=\{X \vert \Vert X \Vert \le 1\}\) of a normed space
\(X\) is compact, then \(X\) is finite-dimensional.

The proof uses Riesz's lemma. Assume that \(M\) is compact set but \(\dim X =
   \infty\), this leads to a contradiction. Compact sets have compact images
under continuous mappings.
\subsection{Theorem}
\label{sec:orge90611a}
Suppose \(X\) and \(Y\) are metric spaces and \(T \colon X \rightarrow X\) is
continuous. Then any compact subset of \(X\) is mapped to a compact subset of
\(Y\).
\subsubsection{Proof}
\label{sec:orgeff38cd}
Proof is easy.
\subsection{Corollary}
\label{sec:org022b015}
Given a continuous mapping \(T \colon M \rightarrow \R\), where \(M\) is a
compact subset of \(X\). Then \(T\) assumes a maximum and a minimum at some
points of \(M\).
\subsubsection{Proof}
\label{sec:orgae8b72b}
The proof is easy. It's something like take the infimum, it has to be a
point in the space because closed.
\subsection{Example}
\label{sec:org2f62937}
The closed unit ball \(M=\{f \colon \Vert f \Vert \le 1 \}\) of \(C[0, 1]\) is
not compact. To see this, define \(f(X) = \max(1 - \vert X \vert, 0)\) and
\(f_n(X) = f(2n\cdot (n+1)(x - \frac{1}{n}))\) (functions with center \(1/n\) and
decreasing bandwidth converges to the zero function) \footnote{We might not need this property.}

Now \(f_n \in C[0, 1]\) and \(\Vert f_n \Vert_\infty = 1\). Since supports of \(f_n\) do not overlap.

\(\Vert f_n f_m\Vert = \max_{X \in \{0, 1\}} \vert f_n(X) - f_m(X)\vert = 1\)
and the sequence does not have a convergent subsequence.
\subsection{Linear Operators}
\label{sec:org27d1a5a}
We now consider linear operators and their properties.
\subsubsection{Definition}
\label{sec:org0740210}
Let \(T\) be an operator, \(D(T)\) its domain and \(R(T)\) its range. The operator
is called linear if \(T(x + y) = Tx+Ty\) and \(T(\alpha x) = \alpha Tx\)

Note that we typically write \(Tx\) and not \(T(x)\) as it is done for functions.

The null set \(N(T)\) is defined \(N(T) = \{x \in D(T) \colon Tx = 0\}\)

In particular, linearity implies \(T0 = 0\).
\subsubsection{Example}
\label{sec:org5bff74c}
Define \(T\) by \((Tf)(x) = \int_{a}{x} f(\tilde x) d \tilde x\) for \(f\in C[a,
    b]\). Then \(T(cf + cg) = cTf + dTg\). It is easy to show that the operator is
linear.
\subsubsection{Example}
\label{sec:org7cbced8}
\begin{enumerate}
\item Let \(X\) be the space of all polynomials defined on \([a, b]\). We can define a
linear operator to be the derivative, \(Tf = f'\). \footnote{This is an unbounded operator. We still haven't defined what bounded
operators are, though.}
\item Given \(A \in \R^{m\times n}\), \(T \colon \R^n \rightarrow \R^m\), \(X
       \rightarrow Ax\) is linear \(T(ax + by) = aAx + bAy = aTx + bYy\).
\item Let \(k\) be a square-integrable function on \([a, b]^2\) and \(X=L_2[a, b]\),
then define \(f\mapsto\int_{a}^{b} h(x, \cdot) f(x)\ dx\)\footnote{These are called Kernels. But kernels in Machine learning is \(k(x, y)
=\langle \phi(x), \phi(y)\rangle\), for example \(h(x, y) = \exp(-\Vert x -
y\Vert/\varepsilon\) is called a Gaussian kernel. Another example is \(h(x, y) =
(1+\langle x, y\rangle)^p\) polynomial kernel} \textsuperscript{,}\,\footnote{Idea of machine learning. We have two sets of points, we need to find a
function (often linear) that separates the points. But it may not be easy. So
one idea is to find a linear function to a higher dimensional space and then one
may be able to separate the points in the higher dimensional space.}
\end{enumerate}
\subsection{Theorem about linear operators}
\label{sec:orgd06bc90}
Suppose that \(T\) is linear, then
\begin{enumerate}
\item \(R(T)\) is a vector space
\item \(N(T)\) is a vector space.
\end{enumerate}

I think \(R\) and \(N\) are range and kernel, respectively. The proof is kinda
easy.
\section{Lecture 6 \textit{<2018-11-06 Tue>}}
\label{sec:orgbda90e4}
\subsection{Regarding convergence of a function}
\label{sec:orgef8d23f}
\(\sup(f_n)\subset [0, 2/n]\), \(f_n(0) = 0\), consider fixed \(x\in [0, 1]\).
Given any \(\varepsilon > 0\), choose \(N > 2/X\), then \(X > 2/N > 2/N\) for
\(n>N\). Thus \(x \in \sup(f_n)\) and \(f_n(x) = 0\), \(f_n\) converges point-wise to
\(0\).

\(f_n(x) = f(2n(n+1)(x - 1/2))\)
\subsection{Injectivity of operators}
\label{sec:orgd078dd1}
If \(T\) is injective, there exists \(T^{-1}\colon R(T) \rightarrow D(T)\) with
\(T^{-1}y =x\) for \(Tx =y\), i.e., the inverse of \(T\). It follows that \(T^{-1}Tx
   = T^{-1}Ty = x\) and \(TT^{-1}y = Tx = y\).

Example: Given \(A\in \R^{n\times n}\), \(T\colon \R^n \rightarrow \R^m\), \(x
   \mapsto Ax\).

If \(m < n\), \(T\) can be injective if the rank of \(A = n\), but it cannot be
surjective. When \(m > n\), then \(T\) can be surjective, when the Rank of \(A =
   m\), but it cannot be injective. If \(m = n\), then \(T\) is bijective if and only
if the rank of \(A = m = n\).
\subsection{Theorem}
\label{sec:org103d707}
Given vector spaces \(X\) and \(Y\), and a linear operator \(T \colon D(T)
   \rightarrow Y\), then
\begin{enumerate}
\item \(T^{-1}\colon R(T) \rightarrow D(T)\) exists if and only if \(T(x) = 0
      \implies x = 0\), i.e., the null space \(N(T) = 0\). Then \(T^{-1}\) is also a
linear operator.
\item If the dimension of the domain of \(T\) is smaller than \(\infty\), and
\(T^{-1}\) exists, then \(\dim R(T) = \dim D(T)\).
\end{enumerate}
\subsubsection{Proof}
\label{sec:orgcc5ae65}
Assume that \(T^{-1}\) exists then \(T\) is injective and \(Tx = 0 \implies x =
    0\). Conversely, assume \(Tx_1 = Tx_2\), then \(Tx_1 - Tx_2 = T(x_1 - x_2) = 0\).
Thus, by assumption \(x_1 - x_2 = 0 \implies x_1 = x_2\) and \(T\) is injective.
Hence the inverse \(T^{-1}\) exists.

To show that \(T^{-1}\) is a linear operator: Given \(y_i = T x_i\) and \(x_i =
    T^{-1}y\), for \(i = 1, 2\), then it follows that \(T(\alpha x_1 + \beta x_2) =
    \alpha T x_1 + \beta T x_2 = \alpha y_1 + \beta y_2\) and \(T^{-1}T(\alpha
    x_1 + \beta x_2) = T^{-1}(\alpha y_1 + \beta y_2)\). From this one can show
that if \(T^{-1}\) exists, it is also a linear operator.

\begin{enumerate}
\item Let \(\dim D(T) = n < \infty\), then \(\dim R(T) \le n\), i.e., \(\dim R(T)
       \le \dim D(T)\). This can be seen as follows: choose \(n+1\) elements \(y_1,
       \cdots, y_{n+1} \in R(T)\) and we choose them arbitrary, then we can find
pre-images \(x_1, \cdots, x_{n+1}\in D(T)\), it holds that \(Tx_i = y_i\),
for all \(i\). Since we assumed \(\dim D(T) = n\), the \(x_i\) are linearly
dependent, i.e., there exist \(\alpha_i \in \R\) such that \(\sum \alpha_i
       x_i = 0\) where not all \(\alpha_i\) are zero. Now \(T \sum_i^{n+1} \alpha_i
       x_i= \sum_{i=1}^{n+1} \alpha_i y = 0\), but not all \(\alpha_i\) are zero.
Thus we have found a set of linearly dependent vectors. Since we chose
the vectors arbitrarily, we see that the dimension must be less than or
equal to \(n\).

If we apply the same reasoning to the inverse operator, which we assume
exists, we obtain in similar fashion that the \(\dim D(T) \le \dim R(T)\).
Thus, \(\dim R(T) = \dim D(T)\).
\end{enumerate}
\subsection{Lemma}
\label{sec:org7840562}
Let \(X, Y, Z\) be vector spaces \(T \colon X \rightarrow Y\), and \(S \colon Y
   \rightarrow Z\) bijective operators, then we claim that \((ST)^{-1} = T^{-1}
   S^{-1}\).
\subsection{Definition: Bounded operators}
\label{sec:org918b554}
Given normed spaces \(X\) and \(Y\) and a linear operator, \(T \colon D(T)
   \rightarrow Y\), \(D(T) \subset X\), \(T\) is defined to be bounded if there
exists a constant \(c\) such that \(\Vert Tx \Vert \le c \Vert x \Vert\) and this
has to hold for all \(x \in D(T)\). A bounded linear operator maps bounded sets
in \(D(T)\) onto bounded sets in \(Y\).

We define the norm \(\Vert T \Vert = \sup_{x \in D(T), \lambda \neq 0}
   \frac{\Vert T x \Vert}{\Vert x \Vert}\) to be the norm of \(T\). It is
straightforward to verify that this satisfies the properties for norm.

For bounded linear operators, the bound can be computed by the supremum over
\(\Vert x \Vert = 1\). This is straightforward to see.
\subsection{Example}
\label{sec:org598c97f}
Given \(A \in \R^{n \times n}\), the linear operator \(T \colon \R^{n}
   \rightarrow \R^{n}\) is bounded since \((\sup_{\Vert x \Vert= 1} \Vert A x
   \Vert)^2 = \sup x^{T} A^{T} A x = \lambda \max(A^T A)\), the largest
eigenvalue of \(A^{T}A\) (Rayleigh-Ritz theorem.)

Note that for \(A = U \sum V^T\), so that \(A^{T} A = V \sigma^2 V^{T}\), thus
\(\lambda_{\max} = \delta_{1}^2\), where \(\delta_1\) is the largest singular
value of \(A\).

Let \(X\) be the space of all polynomials on \([0, 1]\) and \(\Vert f \Vert =
   \max_{x \in [0, 1]} \vert f (x) \vert\). The differentiation operator \(T\) with
\(Tf = f'\) is not bounded. Since for \(f_n(x) = x^n\), \(\Vert f_n\Vert = 1\), all
these functions are bounded above by \(1\), hence the norm is \(1\), whereas, the
norm of the derivative is \(n\) and this is unbounded.

\(T \colon C[0, 1] \rightarrow C[0, 1]\) by \((Tf)(x) = \int_{0}^{x} f(t)\ dt\).
Then \(\Vert T f\Vert = \max_{x \in [0, 1]} \vert \int_{0}^{1} f(t)\ dt\vert
   \le \max_{x \in [0, 1]} \int_{0}^{1} \vert f(t) \vert dt \le (1-0) \max_{x\in
   [0, 1]} \vert f(x) \vert = \Vert f \Vert\), thus \(\Vert T \Vert \le 1\), but we
can choose the function identical to \(1\), then the norm is exactly equal to
\(1\).

Define \(c = \{x_n \in l_1 \vert \exists N \in \N \colon x_n = 0, \forall n >
   N\}\), with the \(l_1\) norm. We define the operator \(T\colon (x_1, x_2, x_3,
   \cdots) = (x_1, 2x_2, 3x_3, \cdots)\) is unbounded, we can compute \(\Vert T
   e_i \Vert = i\), so if we have the sequence but the \(\Vert e_i \Vert = 1\).
\subsection{Theorem}
\label{sec:org1a10724}
Every linear operator on a finite dimensional normed space is bounded.
\subsubsection{Proof}
\label{sec:orga5fb774}
Let \(n\) be the dimension of \(X\) and \(\{e_1, \cdots, e_n\}\) a basis.

Any \(x\in X\) can be written as \(X = \sum \alpha_i e_i\), thus \(\Vert Tx \Vert
    = \Vert T \sum \alpha_i e_i\Vert \le \sum \Vert T \alpha_i e_i\Vert = \sum
    \vert \alpha_i \vert \le \max_{i} \Vert T e_i \Vert . \sum \vert \alpha_i
    \vert\). We define \(\Vert x \Vert_0 = \sum \vert \alpha_i \vert\) defines a
norm. Since all norms on finite-dimensional spaces are equivalent, there
exist \(c\) such that \(\Vert x \Vert_0 \le c \Vert x \Vert\). Thus \(\Vert T
    x\Vert \le \max \Vert T e_i \Vert \cdot c \cdot \Vert x \Vert\) and \(T\) is
bounded.
\section{Lecture 7 \textit{<2018-11-08 Thu>}}
\label{sec:org50a6dda}
\subsection{Theorem about Bounded operators}
\label{sec:orgf460388}
For linear operators, continuity and boundedness are equivalent.

Continuity of \(T\) means that \(\forall \varepsilon > 0\), there exists a
\(\delta > 0\), such that \(\Vert x - x_0 \Vert < \delta \implies \vert Tx -
   Tx_0 \vert < \varepsilon\).
\subsubsection{Proof}
\label{sec:org3dd76c5}
There is nothing to show for \(T=0\), now we assume that \(T\) is not the zero
operator, i.e., \(\exists r\) such that \(\Vert T x \Vert \le r \Vert x \Vert\).
Using linearity, we can write that \(\Vert Tx - Tx_0\Vert = \Vert T (x -
    x_0)\Vert \le \delta \Vert x - x_0\Vert\). Choose \(\delta =
    \varepsilon/\delta\), thus \(\Vert x -x_0 \Vert < \delta \implies \Vert Tx -
    Tx_0 \Vert \delta \Vert x - x_0\Vert\le \gamma\cdot \delta =\varepsilon\) and
\(T\) is continuous.

Conversely, assume that \(T\) is continuous. Take arbitrary \(y\in D(T)\),
define \(X = X_0 + \frac{\delta}{\Vert y \Vert} y\). Thus \(\Vert x - x_0 \Vert
    = \Vert \frac{\delta}{\Vert y \Vert} \cdot y \Vert = \delta\). Since \(T\) is
continuous, \(\Vert Tx - Tx_0 \Vert < \varepsilon\). Now \(\Vert Tx - Tx_0\Vert
    = \Vert T \delta/\Vert y \Vert y \Vert = \frac{\delta}{\Vert y \Vert}\Vert T
    y \Vert < \varepsilon\) Multiplying by \(\Vert y \Vert / \delta\), we get
\$\Vert T y \Vert < \(\varepsilon\)/\(\delta \cdot \Vert y \Vert\) We call the last
term \(\gamma\).

The second part of the proof shows that continuity in one point suffices to
show boundedness. And boundedness means continuity at all points. Thus
continuity at one point implies continuity at all points. Pretty interesting!
\subsection{Corollary}
\label{sec:org58ef4c8}
Given a linear bounded operator \(T\), it holds that 
\begin{enumerate}
\item \(x_n \rightarrow x\) implies that \(Tx_n \rightarrow Tx\)
\item The null space of such an operator is closed
\end{enumerate}
\subsubsection{Proof}
\label{sec:org8c85937}
These are basically properties of continuous functions.
\subsection{Definition}
\label{sec:orgd68b334}
We write that \(T_1 = T_2\) if \(D(T_1) = D(T_2)\) and \(T_1x = T_2x\) for all \(x
   \in D(T_1) = D(T_2)\). Furthermore \(T\vert_B\) denotes the restriction to the
set \(B\), i.e., \(T\vert_B \colon B \rightarrow Y\) with \(T\vert_B x = Tx\) for
all \(x \in B\).

The extension of \(T\), denoted by \(\tilde{T}\colon M \rightarrow Y\) where \(D(T)
   \subset M\) is defined by \(\tilde{T}x = Tx\), for all \(x\in D(T)\).

Example of extension. The set \(X = Y = \R\), define \(T\) by \(Tx=x\), and \$D(T) =
[0, 1]. Then \(\tilde{T}\) defined by \(\tilde{T}x = \vert x \vert\) with
\(D(\tilde{T}) = [-1, 1]\) is an extension of \(T\)
\subsection{Theorem}
\label{sec:orga4107b2}
If \(T \colon D(T) \rightarrow Y\) is a bounded linear operator, \(D(T)\), part
of a normed space, \(Y\), a Banach space, then there is an extension
\(\tilde{T}\colon \bar{D(T)} \rightarrow Y\) with \(\Vert \tilde{T} \Vert =
   \Vert T \Vert\). Furthermore, \(\tilde{T}\) is a bounded linear operator.
\subsubsection{Proof}
\label{sec:orgd3ac517}
For any \(x \in \bar{D}(T)\) consider the sequence \((x_n)_n\) in \(D(T)\) that
converges to \(x\).

\(\Vert Tx_n - Tx_m \Vert = \Vert T(x_n - x_m)\Vert \le \Vert T \Vert \Vert
    x_n - x_m\Vert\). Since \(x_m\) converges, \(Tx_n\) is Cauchy and converges as we
assumed \(Y\) to be complete. The rest of the argument is trivial.
\subsection{Linear functionals}
\label{sec:org4dfcc50}
A functional is a map from \(X\) to \(\R\) or \(\C\). Given a functional \(f\),
\(D(f)\) denotes the domain, \(R(f)\) denotes the range of \(f\). 

For functionals, we typically write \(f(x)\) and not \(fx\), although \(f\) is
still an operator.
\subsubsection{Example}
\label{sec:orgcc72b18}
\begin{enumerate}
\item For a normed space \(X\), \(\Vert . \Vert \colon X \rightarrow \R\) is a functional.
\item For \(X=\R\), and \(x_0 in X\), \(f\colon X\rightarrow \R\), \(x\mapsto x_0^{T}x
       = \langle x_0, x\rangle\).
\item Linearity is defined as before, with the difference that \(y\) is now \(\R\)
if \(X\) is a real or \(\C\), if \(X\) is a complex space.
\end{enumerate}
\subsubsection{Example}
\label{sec:org5e160f2}
\begin{enumerate}
\item The norm functional is not a linear functional. \(\Vert \alpha x + \beta y
       \Vert \neq \alpha \Vert x \Vert + \beta \Vert y \Vert\). This is not true
in general.
\item Define \(f(x) = x_0^{T}x\) is linear, clearly, because the scalar product
is bilinear. It is linear in each variable.
\item The evaluation functional given by the Dirac delta function \(\delta_x f =
       f(x)\).
\item The definite integral is a functional. Let \(l\) denote the functional such
that for \(f\) in \(C[a, b]\), \(l(f) = \int_{a}^{b} f(x) dx\), then it is
straightforward to verify that this is linear.
\end{enumerate}
\subsubsection{Remark}
\label{sec:org9627843}
Similarly, boundedness is again defined as: A linear functional is bounded
if there exists a constant \(c\) such that \(\vert f(x) \vert \le c \cdot \Vert
    x \Vert\). and \(\Vert v \Vert = \sup \vert f(x) \vert / \Vert x \Vert =
    \sup_{\Vert x \Vert = 1} \vert f(x) \vert\).
\subsubsection{Theorem}
\label{sec:org57012d7}
Let \(f\colon D(f) \rightarrow K\) be a linear functional, then \(f\) is
continuous if and only if it is bounded.
\subsubsection{Example}
\label{sec:orgf0845af}
\begin{enumerate}
\item For integrable functions on \([a, b]\), it is easy to see that the integral
functional is bounded.
\item The dot product example can be extended to \(l_2\) by choosing a fixed
element \(a\) in \(l_2\) and setting \(f(x) = \sum a_i x_i\). Due to
Cauchy-Schwarz inequality \(\vert \langle x, y\rangle\vert \le \Vert x
       \Vert \cdot \Vert y \Vert\).
\end{enumerate}
\section{Lecture 8 \textit{<2018-11-07 Wed>}}
\label{sec:org582d7a1}
\subsection{Example}
\label{sec:org3071956}
\((x_n)_n\), it's basically a sequence of sequences. \(x_n \in l_\infty\).

\(\delta_X f = f(x)\).

The evaluation functional \(\delta_X\) on \(C[a, b]\) with norm \(\Vert \cdot
   \Vert_\infty\) is bounded. But it might be unbounded with respect to another
norm.
\subsection{Linear functionals}
\label{sec:org4217aed}
The set of all linear functionals forms a vector space denoted by \(X^{*}\).

The algebraic dual space. For defining a vector vector space, we need the
basic operations \(+\) (addition) and \$.\$ scalar multiplication, which can be
defined as follows: we take two functionals \(f_1\) and \(f_2\) and a scalar
\(\alpha\), is easy to write \((f_1 + f_2)(x) = f_1(x) + f_2(x)\) etc. This part
is obvious.

We can also consider the dual of the dual space \(X^{**}\), the second
algebraic dual space. It is clear that there is a canonical isomorphism
between \(X\) and \(X^{**}\).

There was an example with \(V\) along with an orthogonal basis.
\subsection{Definition of isomorphism of vector spaces}
\label{sec:org7c5dfe1}
\subsection{About finite linear operators}
\label{sec:org3fccc79}
Let us now consider finite dimensional vector spaces. Any linear operator
between two fininte-dimensional vector space can be regarded as a matrix. To
see this we have two finite dimensional spaces \(X\) and \(Y\) and a linear
operator \(T \colon X \rightarrow Y\). let \(\{x_1, \cdots, x_n\}\) be a basis of
\(X\) and \(\{z_1, \cdots, z_n\}\) be a basis of \(Y\). 

Then for each \(x\in X\), we can write \(X = \sum \alpha_i X_i\) and \(y = Tx =
   \sum \alpha_i Tx_i\) and define \(Tx_i = y_i\). Thus by knowing the images
\(y_i\), \(T\) is uniquely defined. For any \(z \in Y\), it can be written as \(z =
   \sum \beta_j z_j\) as well as \(y_j = Tx_i = \sum \gamma_{ji}z_j\).

Now we have two representations in \(z_j\), \(j = 1, \cdots, m\). It follows that
\(\beta_j = \sum_{i=1}{n} \gamma_{ji} \alpha_i\) and that \(y = Tx\) is
determined by knowing the coefficients \(\gamma_{ji}\), which can be written in
matrix form as

$$T_\mu = [\gamma_{ji}]_{j=1, \cdots m; i =1, \cdots m}$$

Then \(\beta = T_\mu \alpha\).
\subsection{Example}
\label{sec:orge6e18cb}
Consider the discrete dynamical system \(\phi \colon \R^2 \rightarrow \R^2\) by
\(\phi(x) = [\lambda x_n, \mu x_2 + (\lambda^2 - \mu)x^2n]^T\)

The Korpman operator \(K\) is an infinite-dimensional operator defined by \(Kf =
   f\circ \phi\), i.e., \((Kf)(\lambda) = f(\phi(x))\) for \(f \in L_{\infty}\). This
operator apparently plays an important role in Dynamical system.
\section{Lecture 10 \textit{<2018-11-15 Thu>}}
\label{sec:orged50af9}
\subsection{Koopman operator}
\label{sec:org2341c04}
\(Kf = f\circ I\)

Consider the discrete dynamical system \(\phi \colon \R^2 \rightarrow \R^2\) by
\(\phi(x) = [\lambda x_n, \mu x_2 + (\lambda^2 - \mu)x^2n]^T\)

Then the space is spanned by functions \$$\backslash${x\(_{\text{1}}\), x\(_{\text{2}}\), x\(_{\text{1}}^{\text{2}}\)$\backslash$} forms a so-called
Koopman-invariant subspace. Let \(f_1(\bf{x}) = x_1\), \(f_2(\bf{x}) = x_2\),
\(f_e(\bf{x}) = x_n^2\), then any function from this subspace can be written as
\(f = \sum_{i=1}^{3} \alpha_i f_i\) and \(g=Kf = K\sum(\alpha_i f_i) = \sum
   \alpha_i Kf_i\). We call the term \(Kf_i = g_i\).

\(g_1(x) = Kf_1(x) = f_1(\phi(x)) = \lambda x_1 =\lambda f_1(x)\).

\(g_2(x) = Kf_2(x) = f_2(\phi(x)) =\mu x_2 + (\lambda^2 - \mu)x_1^2 - \mu
   f_2(x) + (\lambda^2 - \mu)f_3(\lambda)\)

\(g_3(x) = Kf_3(x) = f_3(\phi(x)) = \lambda^2 x_1^2 = \lambda^2 f_3(x)\)

Thus \(g = Kf = \alpha_1 \lambda f_1 + \alpha_2[\mu f_2 + (\lambda^2 -
   \mu)f_3] + \alpha_3 \lambda^2 f_3 = \alpha_1\lambda f_1 + \alpha_2 \mu f_2 +
   (\alpha_2(\lambda^2 - \mu) + \alpha_3 \lambda^2)f_3\)

Now we can write this as a matrix.

It follows that

\(\gamma_{11} = \lambda\), \(\gamma_{12} = 0\), \(\gamma_{13} = 0\)

\(\gamma_{21} = 0, \gamma_{22} = \mu, \gamma_{13} = 0\)

\(\gamma_{31} = 0, \gamma_{32} = \lambda^2 - \mu, \gamma_{23} = \lambda^2\)

The following is a matrix representation: 

\begin{center}
\begin{tabular}{lll}
\(\mu\) & 0 & 0\\
0 & \(\mu\) & \(0\)\\
\(0\) & \(\mu^2 - \mu\) & \(\lambda^2\)\\
\end{tabular}
\end{center}

That is defining, \(\bar{f} = [f1, f2, f3]^T\) and \(\alpha = [\alpha_1,
   \alpha_2, \alpha_3]^T\).

We obtain \(f = \alpha^T f\) and \(g=(T_\mu \alpha)^T\bar{f}\).

Note that \(\varphi_1(x) = x_1\), \(\varphi_2(x)=x_2 - x_n^2\), \(\varphi_3(x) = x_1-x_n^2\) are
eigenfunctions. Corresponding to the eigenvalues

\(\lambda_1 = \lambda \cdot; Ke_1 = \lambda \varphi_1\)

\(\lambda_2 = \lambda^2; K\varphi_2 = \lambda^2e_2\)

\(\lambda_3 = \mu; Ke_3 = [\mu X_2 + (\lambda^2 - \mu)x_1^2 - \lambda^2 x_n^2]
   = \mu x_2 - \mu x_n^2 = \mu(x_2 - x_n^2) = \mu \varphi_3\)
\subsection{Matrix representation}
\label{sec:org84878cc}
Assume now again that \(X\) is a vector space with \(\dim X = n\) and that
\(\{x_1, \cdots, x_n\}\) forms a basis. Given a linear functional \(f\), we
obtain for \(X = \sum_{i=1}^{n} \alpha_i X_i\) that \(f(x) = f(\sum \alpha_i
   x_i) = \sum \alpha_i f(x_i) = \sum\alpha_ic_i\). We call the last term our
coefficient \(c_i = f(x_i)\). Thus \(f\) is uniquely determined by the values
\(c_i, i=1, \cdots, n\).

Conversely, any set of values values \(\alpha_i, i =1, \cdots, n\) uniquely
defines a linear functional. A special set of functionals is defined as
follows: \(f_j(x_i) = \delta_{ij}\). It's one when \(i=j\), and \(0\) otherwise. We
call this the dual basis of \(\{x_1, \cdots, x_n\}\).
\subsection{Theorem}
\label{sec:org919b9aa}
Let \(X\) be again an \$n\$-dimensional vector space with basis \(\{x_1, \cdots,
   x_n\}\). Then \(\{f_1, \cdots, f_n\}\) as defined above is a basis of \(X^{*}\).
As a result, we have \(\dim X^{*} = \dim X\).
\subsubsection{Proof}
\label{sec:orgcbe20d3}
It's kinda easy. We just show that the maps \(f_j\) is a basis and we're done.
\subsection{Theorem}
\label{sec:orgd0e78fa}
A fininte-dimensional vector space is algebraically reflexive, i.e., the
canonical embedding is an isomorphism between \(X\) and \(X^{**}\).
\subsection{Normed space of operators}
\label{sec:org37dea5f}
Let \(X\) and \(Y\) be arbitrary normed spaces, then the set \(B(X, Y)\) of all
bounded linear operators from \(X\) to \(Y\) is again a normed space.

We need addition, scalar multiplication and a norm, and define:
\begin{enumerate}
\item \((T_1 + T_2)x = T_1 x + T_2x\) for any \(T_1, T_2 \in B(X, Y)\)
\item \((\alpha T) x = \alpha Tx\) for any \(T\in B(X, Y), \alpha \in K\)
\item \(\Vert T\Vert\) is the supremum norm that we have already defined.
\end{enumerate}
\subsection{Theorem}
\label{sec:org480e0e9}
\(B(X, Y)\) is a Banach space if \(Y\) is a Banach space, i.e., \(B(X, Y)\) is
complete if \(Y\) is complete.\footnote{We didn't prove this.}
\subsection{Definition}
\label{sec:orgc8fcc00}
The set of all bounded linear functionals on \(X\) is a normed space with
\(\Vert f \Vert = \sup \vert f x\vert / \Vert x \Vert\) for \(x \neq 0\).

From the above theorem, since \(\R\) is complete, the space of all bounded
linear functionals converge. This is called this \textbf{dual space} (the
continuous/topological dual) and is denoted by  \(X'\).

Remark: The algebraic dual space \(X^{*}\) contains all linear functionals of
\(X\), whereas \(X'\) contains only the bounded linear operators. 
\subsection{About \(X'\) and \(X^{*}\)}
\label{sec:org8d28446}
The space of all bounded linear functionals on \(X\), given by \(X'\), forms a
linear subspace of \(X^{*}\).

Assume that \(f\) and \(g\) are bounded by \(a\) and \(b\), \(\vert f(x) \vert \le a
   \Vert x \Vert\), \(\vert g(x) \vert \le b \Vert x \Vert \forall x\in X\).

Using triangle inequality, we can see that \(af + bg\) is bounded if \(f\) and
\(g\) are bounded. For scalars multiplication, it is similarly true. Thus it
forms a linear subspace.
\subsection{Examples}
\label{sec:org608a8ca}
\begin{enumerate}
\item \((\R^n)' = \R^n\)
\item \((l^1)' = l^{\infty}\)
\item For \(1 < p < \infty\) and \(\frac{1}{p} + \frac{1}{q} = 1\), \((l^p)' = l^q\).
Here \(=\) means there exists an isomorphism.
\end{enumerate}
\subsection{About \(l^p\) and \(l^q\)}
\label{sec:org13fda63}
Given \(1 \le p \le \infty\), with \(\frac{1}{p} + \frac{1}{q} = 1\). Take any
\((y_n)_n \in l^p\), then \(f\colon l^p \rightarrow \R\), \((x_n)_n \mapsto \sum
   x_n y_n\) is a bounded linear functional. The norm of this functional is
\(\Vert f \Vert = \Vert y \Vert_q\).
\section{Lecture 11 \textit{<2018-11-20 Tue>}}
\label{sec:org9cea8bb}
\subsection{About \(l^p\) and \(l^q\)}
\label{sec:org49f4ebc}
\(\frac{1}{q} + \frac1p = 1\), \((y_n)_n \in L^q\), \(f\colon l^p \rightarrow \R\),
\((x_n)_n \mapsto \sum x_n y_n\)

Note that \(\sum \vert x_n y_n\vert \le \Vert x \Vert_p \cdot \Vert x \Vert_q\)
done to the Holder inequality.
\subsection{Theorem}
\label{sec:org93c9634}
For \(1\le p < \infty\), \((l^p)' \equiv l^q\) and an isometric isomorphism is
given by \(T \colon l^q \rightarrow (l^p)'\), \((Ty)(x) = \sum_{n=1}^{\infty}
   x_n y_n\).
\subsubsection{Proof}
\label{sec:orgafa9635}
\(\Vert Tt \Vert_{l^p}' = \Vert y \Vert_q\) as shown above and \(T\) is linear.
The \(N(T)\) is only the sequence \(0\), meaning that the mapping is injective.
Show that the mapping is also surjective. So for any given functional, we
need to find a corresponding \(y\).

Take any \(x' \in (l^p)^{*}\) and define \(y = (y_n)_n\) with \(y_n = x'(e_n)\)
then \(y \in l^q\): Fix \(N \in \N\) for \(p > 1\), (can be shown for \(p=1\))
construct the sequence: \(\sum_{n=1}^{N} \vert y_n \vert^q = \vert_{n=1}^{N}
    \vert y_n \vert^q = \sum \frac{\vert y_n \vert^q}{y_n} y_n = \sum
    \frac{\vert y_n\vert^q}{y_n} x'(e_n) = x'(\sum \frac{\vert y_n \vert^q}{y_n}
    e_n\) (this is using the linearity). Now we can use that \(x'\) is bounded.

\(X'(\sum \frac{\vert y_n \vert^q}{y_n} e_n \le \Vert X' \Vert \sum \vert
    y_n\vert^q}{y_n} e_n \Vert = \Vert X'\Vert \left(\sum (\vert
    y_n\vert^{q-1})^p\right)^{1/p} = \Vert X'\Vert \left( \vert y_n
    \vert^q\right)^{1/p}\)

Here \(\frac1p + \frac1q = 1\) implies that \(\frac1p = \frac{q-1}{q} \implies
    (q-1)p = q\).

Divide by \(\sum_{i=1}^{N} \vert y_n \vert^q)^{1/p}\), then
\(\left(\sum_{n=1}^{N} \vert y_n \vert^q\right)^{1-1/p} \le \Vert X'\Vert\).
For \(N \rightarrow \infty\), we obtain \(\Vert y \Vert_q \le \Vert X'\Vert <
    \infty\)

We have shown \(y\in l^q\), i.e., for each functional \(x'\), we can find such a
\(y\). Now, \((T_y)(e_n) = (y_n)\) and also we define \(y_n = X'(e_n)\). This
means that it is really an isomorphism.

Thus \(Ty = x'\) on span \(\{e_1, \cdots,\}\) which is a basis of \(l^p\).
\subsection{About reflexiveness}
\label{sec:orgc4154f2}
The dual of \(l^\infty\) is not \(l^1\), but a different one.
\subsection{Inner Product spaces and Hilbert spaces}
\label{sec:org673df53}
\subsubsection{Definition}
\label{sec:orgb1cd85d}
In vector spaces we have addition and scalar multiplication. In normed
vector spaces, we additionally have a norm \(\Vert . \Vert\) which gives us
lengths of vectors. A norm induces a metric by \(d(x, y) = \Vert x - y
    \Vert\).

However, we have not yet defined the notion of orthogonality in \(\R^n\). Two
vectors are orthogonal if \(\langle x, y \rangle = x^{T} y = x \cdot y = 0\).

The inner product induces a norm and thus also a metric by \(\Vert x \vert =
    \langle x, x \rangle^{1/2}\). Here \(\langle x, x \rangle = x^{T} x =
    \sum_{i=1}^{n} \hat{x}_i = \Vert x \Vert_2^2\) induces the standard Euclidean
norm.

With the inner product, we can thus compute angles in \(\R^n\). This concept
will be generalized in what follows. The resulting space with the inner
product is called an inner product space. Inner product spaces that are
complete are called \textbf{Hilbert spaces}

\begin{center}
\begin{tabular}{ll}
normed space & complete normed space = Banach Space\\
inner product space & complete inner product space = Hilbert space\\
\end{tabular}
\end{center}
\subsubsection{Definition}
\label{sec:orgc52dc01}
Let \(X\) be a vector space. A mapping \(\langle ., \cdot \rangle \colon X
    \times X \rightarrow K\) (\(=\R\) or \(=\C\) as usual) is called an inner product
space (or scalar product.) if 
\begin{enumerate}
\item \(\langle x_1 + x_2 , y \rangle = \langle x_n , y \rangle + \langle x_2, y\rangle\), \(x_1, x_2, y \in X\)
\item \(\langle \lambda x, y \rangle = \lambda \langle x, y \rangle\), \(x, y, \in X, \lambda \in K\).
\item \(\langle x, y\rangle = \overline{\langle y, x \rangle}\)
\item \(\langle x, x \rangle \ge 0\), for all \(x\in X\).
\item \(\langle x, x \rangle = 0 \iff x = 0\).
\end{enumerate}
\subsubsection{Remark}
\label{sec:orgeb56f27}
It follows \(\langle x, y_1 + y_2 \rangle = \overline{\langle y_1 + y_2, x
    \rangle} = \overline{\langle y_1, x\rangle} + \overline{\langle y_2,
    x\rangle}\) and

\(\langle x, \lambda y\rangle = \lambda \langle x, y\rangle\) similarly.
\subsubsection{Theorem}
\label{sec:orgc391bba}
Let \(X\) be a vector space and \(\langle . , . \rangle\) an inner product, then
\(\vert \langle x, y \rangle\vert^2 = \langle x, x \rangle \cdot \langle y, y
    \rangle\) for all \(x, y \in X\). Equality when \(x\) and \(y\) are linearly
dependent.
\begin{enumerate}
\item Proof
\label{sec:orgf9a8e38}
For arbitrary \(\lambda \in K\), we can write \(0 \le \langle x + \lambda y,
     x + \lambda y \rangle = \langle x, x \rangle + \langle \lambda y, x
     \rangle + \langle x, \lambda y \rangle + \langle \lambda x, \lambda y
     \rangle\).

Now it's kinda easy to see when we put \(\lambda = -\frac{\langle x, y
     \rangle}{\langle y, y \rangle}\) for \(y \neq 0\).
\end{enumerate}
\subsubsection{Lemma}
\label{sec:org75976fa}
Assigning \(x \mapsto \langle x, x \rangle^{1/2}\) indeed defines a norm.
\begin{enumerate}
\item Proof
\label{sec:orgc89d979}
The proof is not too hard hence skipped.
\end{enumerate}
\section{Lecture 12 \textit{<2018-11-22 Thu>}}
\label{sec:org5cdd245}
\subsection{Definition}
\label{sec:org309da4e}
A normed space \((X, \Vert . \Vert)\) is called an inner product space (or
pre-Hilbert space) if an inner product \(\langle ., .\rangle\) exits such that
\(\langle x, x \rangle^{1/2} = \Vert X \Vert\) for all \(x\in X\).

An easy example is \(\R^n\) with the standard inner product.

We have seen that an inner product induces a norm. What about the other case?
Given a norm, can we get an inner product? We can do this in the following way:

\(\langle x, y \rangle = \frac{1}{4}(\Vert x + y \Vert^2 - \Vert x - y
   \Vert^2)\) (for real numbers.)

\(\langle x, y\rangle = \frac{1}{4}\left(\Vert x + y\Vert^2 - \Vert x - y
   \Vert^2 + i \Vert x +iy\Vert^2 - i\Vert x - iy\Vert^2\right)\). For complex
numbers.\footnote{Apparently the \(1\) norm cannot be induced by an inner product. There is
something more going on here. That's because the statement is only true if
\(\langle x, x \rangle = \Vert x \Vert^2\), i.e., the space should already be
induced by an inner product. This is not true in the first place for \(l^1\) norm.}

Furthermore, the so-called parallelogram law holds:

\(\Vert x + y \Vert^2 + \Vert x - y \Vert^2 = 2(\Vert x \Vert^2 + \Vert y
   \Vert^2)\), this can be noticed easily. It's called Parallelogram law because
it has something to do with parallelograms.
\subsection{Lemma}
\label{sec:org57f69b2}
The inner product is a continuous mapping from \(X \times X\) to \(K\).
\subsubsection{Proof}
\label{sec:org1428b88}
Let \(x_n \rightarrow x\) and \(y_n \rightarrow y\), we need to show that
\(\langle x_n, y_n \rangle\) converges to \(\langle x, y \rangle\). This is
straightforward to verify. It involves triangle inequality and
Cauchy-Schwarz.
\subsection{Theorem (When is a normed space, an inner product space?)}
\label{sec:orge2a839a}
A normed space \((\lambda, \Vert . \Vert)\) is an inner product space if and
only if the parallelogram law holds for all vectors.
\subsubsection{Proof}
\label{sec:orgb8817b9}
We have already seen that an inner product satisfies the parallelogram law.
Now, one have to prove this the other way around.

We consider only \(\R\). We need to show that \(\langle x_1 + x_2, y \rangle =
    \langle x_1, y \rangle + \langle x_2, y \rangle\).

Define \(\langle x, y \rangle = \frac{1}{4}(\Vert x + y \Vert^2 - \Vert x-
    y\Vert^2)\) as shown, then, \(\langle x_1 + x_2, y \rangle =
    \frac{1}{4}\left(\Vert x_1 + x_2 + y \Vert^2 - \Vert x_1 +x_2 - y \Vert^2\).

Some lengthy calculations and we end up with the result.
\subsection{Examples}
\label{sec:org285a12d}
\begin{enumerate}
\item \(\C^n\) with the inner product \(\langle x, y \rangle = \sum x_i \bar{y_i}\)
is a Hilbert space.
\item \(l_2\) is a Hilbert space with \(\langle x, y \rangle = \sum x_i \bar{y}_i\)
\item \(l_2(\Omega)\) where \(\Omega \in \R^n\) is an open subset is a Hilbert space
with \(\langle f, g \rangle = \int f(x)\overline{g(x)} dx\)
\item \$C([a, b]) with \(\langle f, g \rangle = \int_{a}^{b} f(x)g(x)\, dx\) (only
real-valued functions.) This is apparently not a Hilbert space. This is an
inner product space. This example is similar to an exercise in one of the
tutorials.
\item Let \(\R^{m\times n}\) denote the set of all real \(m\times n\) matrices.
Define \(\langle A, B\rangle = \tr(A^TB)\). For \(A = (a_{ij}), B= (b_{ij})\),
we obtain \([A^TB]_{ij} = \sum [A^T]_{ik} [B]_{kj} = \sum a_{ki}b{kj}\) and
\(\tr(A^TB) = \sum_{i}(A^TB)_{ii} = \sum_i\sum_j a_{ki}b_{ki}\). Then
induced norm, is \(\Vert A \Vert = \sum A, A\rangle^{1/2} = \left(\sum_i
      \sum_k a_{ki}^2\right)^{1/2} = \Vert A \Vert_F\), i.e., the Frobenius norm.

Apparently, we can use the parallelogram law to show that \(l^p\) when
\(p\neq 2\) is not a Hilbert space.
\end{enumerate}
\section{Lecture 13 \textit{<2018-11-27 Tue>}}
\label{sec:org1fa5792}
\subsection{Example (parallelogram law is invalid)}
\label{sec:org8df33b5}
The space \(l^p\) for \(p \neq 2\) is not a Hilbert space since it does not
satisfy the parallelogram law.

Define \(x = (1, 1, 0, \cdots)\) and \(y = (1, -1,0, \cdots)\). Both of them are
in \(l^p\).

The norm of \(x\) is \(2^{1/p}\) and it is same as the norm of \(y\).

The norm of \(x +y\) is \(2\).

The norm is \(x - y\) is \(2\).

The parallelogram law says \(4 + 4 = 4(4^{1/p})\) which is not true for \(p \neq
   2\).
\subsection{Orthogonality}
\label{sec:org3975f38}
With the aid of the inner product, we can now introduce the notion of
orthogonality as already mentioned above.

Let \(X\) be an inner product space, then \(x, y \in X\) are called \textbf{orthogonal}
denoted by \(x \perp y\) if \(\langle x, y \rangle = 0\).

Two subsets \(A\) and \(B\) are orthogonal if \(\langle x, y \rangle = 0\) for
every \(x\in A\) and \(y\in B\).

Now, given a set \(A\), we want to know the set of all elements that are
orthogonal to \(A\). \(A^{\perp} =\{y \in X\vert x \perp y, \forall x \in A\}\)
is called orthogonal complement of \(A\).

\(A^{\perp}\) is a closed subset of \(X\). (Kinda easy to see in terms of
continuity of the inner product and \(y\) being the inverse of a closed set)
The proof in the class used the fact that if a sequence in \(A^\perp\)
converges, then the limit will be orthogonal to \(x\) as well.
\subsection{Definition}
\label{sec:org0f5bc24}
Given the elements \(x, y\) of a vector space \(X\), the segment joining \(X\) and
\(Y\) is defined as \(\{z \vert z = \alpha x + (1-\alpha)y, 0 \le \alpha \le
   1\}\)

A subset \(A \subset X\) is said to be \textbf{convex} if for every combination \(x, y
   \in A\) is in the set \(A\).
\subsection{Theorem}
\label{sec:org13cbb29}
Let \(H\) be a Hilbert space and \(K \subset H\) closed and convex. Furthermore,
let \(x_0 \in H\), then there exists a unique \(x\) in the set \(A\) which has the
shortest distance with \(\Vert x - x_0 \Vert = \inf_{y \in K} \Vert y =
   x_0\Vert\)\footnote{This is similar to the theorem we did in Discrete Geometry 1}
\subsubsection{Proof}
\label{sec:org65e5db3}
If \(x_0 \in K\) simply choose \(x = x_0\). The proof is easy.
\subsection{Lemma}
\label{sec:org5f8a160}
Let \(K\) be a closed and convex subset of \(H\). For \(x \in K\), the following
statements are equivalent:

\begin{enumerate}
\item \(\Vert x_0 - x\Vert = \inf_{y\in Y} \Vert x_0 - y\Vert\)
\item \(Re \langle x_0 - x, y -x \rangle \le 0\) for all \(y \in K\).
\end{enumerate}
\subsubsection{Proof}
\label{sec:org776dbba}
Geometric interpretation: assume that \(K=\R\), then \(\langle a, b\rangle =
    \Vert a \Vert \Vert b\Vert \cos(a, b)\) and \(\langle a, b \rangle <0\) implies
the angle is obtuse.

Thus \(Re\langle x_0 - x, y - x\rnagle \le 0\) means that the angle between
\(x_0 - x\) and \(y-x\) is obtuse (\(K = \R\))
\subsection{Definition}
\label{sec:orgbdb3f3e}
A vector space \(X\) is defined to be the direct sum of the subspaces \(Y\) and
\(Z\), denoted by \(Y = Y \oplus Z\) if each \(x \in X\) have a unique
decomposition such that \(x = y + z\), where \(y\in Y\) and \(z \in Z\).

The mapping defined above is in general a non-linear projection. (a diagram
was drawn about circles)

Reminder: a projection onto a vector space is a mapping \(P\) with \(P^2 = P\)
\subsection{Theorem}
\label{sec:org597eccd}
Let \(U \neq \{0\}\) be a closed subspace (now just a subspace, not convex) of
a Hilbert space \(H\). Then there exists a linear projection \(P_u\) from \(H\)
onto \(U\) with \(\Vert P_u \Vert = 1\) and \(N(P_u) = U^{\perp}\). Furthermore,
\(Id - P_u\) is a projection onto \(U^{\perp}\) with \(\Vert Id - P_u\Vert = 1\).
It holds that we can split this Hilbert space into \(H = U \oplus U^{\perp}\).
And this i a linear projection \footnote{In general projection need not be linear, but here it is linear.}
\section{Lecture 15 \textit{<2018-12-06 Thu>}}
\label{sec:org70daed3}
\subsection{Review (Bessel's inequality)}
\label{sec:org87d31c4}
\(\sum \vert \langle x, e_i \rangle \vert^2 \le \Vert x\Vert^2\)

Some inequality that happened last Thursday.
\subsection{Fourier coefficients}
\label{sec:orgcd0848d}
The inner product \(\langle x_i, e_i\rangle\) are called Fourier coefficients.

If \(\{e_i \vert i \in \N\}\) is a orthonormal basis, we obtain equality \(\Vert
   x \Vert^2 = \sum \vert \langle x_i, e_i \rangle \vert^2\).
\subsection{Lemma}
\label{sec:org8931a4c}
Let \(\{e_i, i \in \N\}\) be an orthonormal system and \(x,y \in H\). Then you
can show that \(\sum_{i=1}^{\infty} \vert\langle x_i, e_i \rangle \langle e_i
   y\rangle \vert < \infty\).
\subsubsection{Proof}
\label{sec:org1113177}
The proof simply uses the Holder's inequality.

\(\sum \vert \langle x, e_i \rangle \langle e_i, y\rangle \vert \le (\sum
    \vert \langle x, e_i \rangle \vert^2)^{1/2} (\sum \vert \langle e_i, y
    \rangle \vert^2)^{1/2} \le \Vert x \Vert \Vert y \Vert < \infty\)
\subsection{Difference between orthonormal basis and orthonormal system}
\label{sec:orgf76fc8a}
Consider \(\R^3\), then \(e_1\) and \(e_2\) form an orthonormal system. But it is
not a basis, clearly.

One can calculate the bessel's inequality thing.

There was something about Paiseval's equality which was mentioned in the
lecture.
\subsection{Theorem}
\label{sec:org4c73621}
For an infinite-dimensional Hilbert space \(H\). The following statements are equivalent

\begin{enumerate}
\item \(H\) is separable.
\item All orthonormal basis are countable.
\item There is at least one orthonormal basis
\end{enumerate}
\subsubsection{Proof (idea)}
\label{sec:org3edc79a}
1 to 2. Start with an orthonormal basis, we must show that it must be
countable. Take two vectors \(e_i\) and \(e_j \in S\), then \(\Vert e_i - e_j
    \Vert^2 = \sum \langle e_i - e_j, e_i - e_j\rangle= \Vert e_i\Vert^2 + \Vert
    e_j \Vert^2 = 2\). This means that the distance between two basis vectors are
always two.

Take the neighbourhood \(B(e_i, \frac\sqrt{2}{3}) \cap B(e_j \sqrt{2}, 3) =
    \emptyset\). It is empty in \(S\).

This is apparently similar to the fact that \(l^\infty\) is not separable?

So if \(S\) were uncountable, then we could have an uncountable number of
disjoint sets. This contradicts the fact that \(H\) is separable. Why? \(H\) has
a countable basis, but then if we have a set of disjoint open sets, then we
can define an injection between our uncountable set and the countable set.

2 to 3 is clear.

3 to 1. The idea is that we take linear coefficients that are rational. Then
we show that this set is dense in \(H\) and then you're done.
\subsection{How does non-separable Hilbert spaces?}
\label{sec:org61df578}
Consider the space of functions \(f\colon \R \rightarrow \R\) with the property
that \(f(x)\neq 0\) only for a countable set and the property that \(\Vert f
   \Vert < \infty\), where \(\Vert \cdot \Vert\) is the norm induced by the inner
product. \(\langle f, g \rangle = \sum_{x\in \R} f(x)g(x)\)

How do we show that this is not separable? 

Define \(f_y(X) = 1\) when \(x \neq y\) and \(0\) otherwise. For \(y_0 \neq y_1,
   \Vert f_{y_0} - f_{y_1} \Vert = \Vert f_{y_0}\Vert^2 - 2\langle f_{y_0},
   f_{y_1}\rangle + \Vert f_{y_1}\Vert^2 = 2\)

We obtain an uncountable number of disjoint sets. Thus the space cannot be
separable.
\subsection{Fourier series}
\label{sec:org36e1d69}
We consider the space \(L^2[0, 2\pi]\), we define the set of basis functions:
\(S = \{\frac{1}{\sqrt{2\pi}} I} \cup \{\frac{1}{\sqrt{\pi} \cos(nx)}\} \cup
   \{\frac{1}{\sqrt{\pi}}\sin(nx)\}\) See example 3.18 (iii).

The inner product is defined by \(\langle f, g \rangle = \int_0^{2\pi}
   f(x)g(x)\ dx\). We can use the following identities to show that \(S\) is indeed
an orthonormal system.

\(\int_0^{2\pi} \cos(mx) \cos(nx)\dx\) is \(0\) when \(m\neq n\) and \(2\pi\), when
\(m\) and \(n\) are \(0\), and it is \(\pi\) if \(m = n\), but not both \(0\).

Similarly, for the \(\sin\), \(\int_{0}^{2\pi} \sin{mx}\sin{nx}\ dx\) is \(0\),
when \(m\neq n\) or \(m=n=0\) or \(\pi\), when \(m=n\neq 0\).

Also \(\int_0^{2\pi} \sin mx \cos nx\ dx = 0\). Thus \(\langle
   \frac{1}{\sqrt{\pi}} \cos mx, \frac{1}{\sqrt{\pi}} \cos nx\rangle =
   \delta_{m, n}\) for \(m \neq 0, n \neq 0\).

Holds the same for all other combinations. Hence \(S\) is an orthonormal
system.
\subsection{Example}
\label{sec:orgbbef1a1}
A trigonometric series is defined by \(a_0 + \sum_{n=1}^\infty][a_k \cos(kx) +
   b_k \sin(kx)]\). Let \(f\) be \(2\pi\) periodic and continuous. Then the
coefficients are defined by

\(a_0 = \frac{1}{2 \pi} \int_{0}^{2\pi} f(x)\ dx, a_k = \frac{1}{\pi}
   \int_0^{2\pi} f(x) \cos(kx)\ dx, b_k = \frac{1}{\pi} \int_0^{2\pi}
   f(x)\sin(kx)\ dx\).

We and write the Fourier series as \(\tilde{a_0} \frac{1}{\sqrt{2\pi}} I
   +\sum_{n=1}^{\infty} \tilde{a}_n \frac{1}{\sqrt{\pi} \cos(kx) + \tilde{b}_n
   \frac{1}{\sqrt{\pi} \sin(kx)}\).

We know that \(\tilde{a}_0 = a_0 \sqrt{2\pi} = \langle f, \frac{1}{\sqrt{2\pi}}I\rangle\).

Similarly, we can express everything as a an inner product.

That is, we write \(f\) as \(\sum_{e\in S} \langle f, e \rangle e\)
\subsection{Example}
\label{sec:org81096e3}
Let us consider the so called square wave function defined by, \(f(x) = 1\)
between \(0\) and \(\pi\) and it is \(-1\) between \(\pi\) and \(2\pi\).

We can get that \(a_k = 0\). \(b_k = 1\) if \(k\) is odd and \(-1\) if \(k\) is even.

Another function is the sawtooth function.
\section{Lecture 16 \textit{<2018-12-11 Tue>}}
\label{sec:org24de908}
\subsection{Theorem}
\label{sec:org24796ba}
Suppose \(e_n\) is an orthonormal sequence in a Hilbert space \(H\) (if an
orthonormal set is countable, then we can arrange it as a sequence)

\begin{enumerate}
\item The series \(\sum_{i=1}^{\infty} a_ne_n\) converges if and only if
\(\sum_{n=1}^{\infty} \vert a_n \vert^2\) converges.
\item If \(\sum_{n=1}^{\infty} a_ne_n\) converges, then the coefficients \(a_n\) are
the Fourier coefficients \(\langle x, e_n\rangle\) where \(x\) is the element
in \(H\) we are approximating. Thus we can write \(x = \sum_{n=1}^{\infty}
      \langle x, e_n \rangle e_n\).
\end{enumerate}
\subsubsection{Proof}
\label{sec:org95cd040}
\(H\) and \(\R\) are both complete. Thus, a Cauchy sequence in \(H\) converge if
and only if the corresponding sequence in \(\R\) is cauchy. To show this,
define, \(s_n = \sum_{k=1}^{n} a_ke_k\) and \(\delta_n = \sum_{k=1}^{n} \vert
    a_n \vert^2\).

Due to orthonormality, wep
\section{Homeworks}
\label{sec:orgb7e5f42}
\subsection{Homework 8}
\label{sec:org86049d1}
\subsubsection{8.1}
\label{sec:org5c0f120}
A subset \(M\) of a vector space is said to be convex if for two points \(m_1,
    m_2\) in \(M\), the line segment joining \(m_1\) and \(m_2\) lies in \(M\), i.e.,
\(\lambda m_1 + (1-\lambda)m_2 \in M\).

Given \(n\) vectors \(x_1, \cdots, x_n\in M\) and \(\alpha_i \in K\) such that
\(\sum_1^n \alpha_i = 1\), we need to show that \(\sum_1^n \alpha_i x_i \in M\).

We'll prove this by induction on \(n\).

The base case \(n=1\) is trivially true, since the only case is \(\alpha_1 = 1\)
and \(1\cdot x_1 \in M\). Now we assume that the statement is true for \(n-1\)
vectors.

The sum \(\alpha_1x_1 + \cdots + \alpha_{n-1}x_{n-1} + \alpha_nx_n=
    (\sum_1^{n-1}\alpha_i)(\lambda\alpha_1 x_1 + \cdots +
    \lambda\alpha{n-1}x_{n-1}) + \alpha_n x_n\). Where \(\lambda =
    \frac{1}{\sum_1^{n-1} \alpha_i}\). Notice that \(\sum_1^{n-1} \lambda_i
    \alpha_i = 1\), thus we can use the inductive hypothesis to see that
\(\lambda\alpha_1 x_1 + \cdots + \lambda\alpha{n-1}x_{n-1} = y \in M\). Now,
\((\sum_1^{n-1}\alpha_i)y + \alpha_n x_n \in M\) by convexity, since
\(\sum_1^{n-1}\alpha_i + \alpha_n = 1\).

Hence by induction, the statement is true for all values of \(n\).
\subsubsection{8.2}
\label{sec:org28aee79}
If \(y \in M\) and \(x - y \in M^{\perp}\), then \(\langle x - y, y \rangle = 0
    \iff \langle x, y \rangle = \langle y, y \rangle\).

From 1, we know that \(y = P_Mx\) if and only if \(y \in M\) and \(x - y \in
    M^\perp\),

Let \(a = P_M x\) and \(b = P_m y\), then \(a, b \in M\) and \(x-a, y-b \in
    M^\perp\).

$$\langle P_M x, y \rangle = \langle a, y \rangle = \langle a, y \rangle - \langle a, y - b \rangle =
    \langle a, b\rangle$$

Similarly

$$\langle x, P_M y \rangle = \langle b, x \rangle = \langle b, x \rangle - \langle b, x - a \rangle =
    \langle a, b\rangle$$

Thus \(\langle a, b \rangle = \langle P_M x, y \rangle = \langle x, P_M y
    \rangle = \langle a, b\rangle\).
\end{document}