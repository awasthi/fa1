#+TITLE: Functional Analysis 1
#+AUTHOR: Hari
#+LATEX_HEADER: \usepackage[left=2cm, right=2cm, bottom=2cm, top=2cm]{geometry}
#+LATEX_HEADER: \usepackage{parskip}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{mathrsfs}
#+LATEX_HEADER: \def\R{\mathbb{R}}
#+LATEX_HEADER: \def\C{\mathbb{C}}
#+LATEX_HEADER: \def\Re{\operatorname{Re}}
#+LATEX_HEADER: \def\N{\mathbb{N}}
#+LATEX_HEADER: \def\Z{\mathbb{Z}}
#+LATEX_HEADER: \def\Q{\mathbb{Q}}
#+LATEX_HEADER: \def\tr{\operatorname{tr}}
#+LATEX_HEADER: \def\pos{\operatorname{pos}}
#+LATEX_HEADER: \def\conv{\operatorname{Conv}}
#+LATEX_HEADER: \def\sgn{\operatorname{sgn}}
#+LATEX_HEADER: \usepackage[T1]{fontenc}
* Lecture 2 <2019-10-17 Thu>

** Examples of metric spaces

*** The set of continuous functions $C[a, b]$
    With a metric $d(f, g) = \max\vert f(x) - g(x)\vert$

*** The space $l^p$ of sequences $x=(x_1, x_2 \cdots,)$ with $\Vert x \Vert _p < \infty$. 
    The metric is the norm of the difference. 

    To prove that this is indeed a metric, we need the Minkowiski inequality.

    There was a complicated proof of the Minkowski inequality that was discussed
    in the lecture. I think one can prove in a simpler fashion. 

    The proof involved showing that $\alpha \cdot \beta \le \alpha^{p}/p +
    \beta^{q}/q$ (This is the young inequality.)

    The next step was to prove the Holder inequality. 

    The next step is to use Holder on Minkowski. 

    Holder: [[https://en.wikipedia.org/wiki/H%C3%B6lder%27s_inequality]]

    Minkowski: [[https://en.wikipedia.org/wiki/Minkowski_inequality]]

    For $p=q=2$, we obtain the Cauchy-Schwarz inequality. 

** Definitions involving metric spaces

*** Open ball
    $B(x_0, r)$ is the set of all points $x$ such that the distance from $x$ to $x_0$ is less than $r$.

*** Closed ball
    Similar to open ball, except that the distance can be less than or /equal to/ $r$.

*** Sphere
    The set of all points $x$ such that the distance from $x_0$ to $x$ is exactly $r$.

*** Relation
    If we subtract a sphere from the closed ball, we get the open ball.

*** Definition of open
    For every point $x$, we can find an epsilon ball that is inside the set. 

*** Definition of closed
    Basically the complement of the set is open.

*** Remark ($\varepsilon$ neighbourhood)
    An open ball $B(x_0, \varepsilon)$ of radius $\varepsilon$ is often called
    an $\varepsilon$ neighbourhood. A neighbourhood of $X$ is that contains an
    $\varepsilon$ neighbourhood of $X$

    We will use these definition for separable spaces.

*** Interior point 
    We call $x_0$ an interior point if there exist an neighbourhood of $x_0$
    contained inside the space.

*** Continuity
    About a function from one metric space to another. It's basically the
    standard definition. (the definition in the lecture was an $\varepsilon$,
    $\delta$ definition.) 

    The definition was about continuity at a point $x$. Then a function is
    defined to be continuous if it is continuous at every point $x$.

*** Accumulation point
    $A$ be a subset of $X$. Then $x_0 \in X$ (not necessarily in $A$) is called
    accumulation point of $A$ if every neighbourhood of $X_0$ contains at least
    one point $y \in A$, $y \neq x_0$.

* Lecture 3 <2018-10-23 Tue>
  [fn:1]
** More definitions
*** Closure of a set
*** A dense subset of $X$
*** Separable
    If there is a countable subset that is dense in $X$. $X$ is defined to be
    separable.

    Example:
    1. Real numbers are separable.
    2. $l^p$ for $1 \le p \le \infty$ is separable.
    3. $l^\infty$ not separable. Proof idea: For any set of indices $I\subset
       \N$, define $(e_I)_n$ is $1$ is $h\in I$ and $0$ otherwise. For every $I
       \neq y$, we can compute the distance $d(e_I, e_J) = 1$. How many such
       functions are there? The number of such elements are uncountable. (The
       proof of this theorem is similar to how $\{0, 1\}^\N$ is uncountable.)[fn:2]
*** Definition of convergence
    Definition was about the limit of distance going to 0. Note that the limit
    $X$ must be a point in $X$ (Why?)
*** Definition of bounded set
*** Definition of boundedness of a sequence
*** Lemma
    If $x_n$ is convergent, then it is bounded the limit is unique.
*** Lemma
    If $x_n$ converges to $x$ and $y_n$ converges to $y$, then $d(x_n, y_n)$
    converges to $d(x, y)$.
*** About convergence and Cauchy sequences
*** Completeness
    The idea that if a sequence is Cauchy, then it is convergent.
*** Theorem about subsets of complete metric space
    $X$ be a metric space. A subspace $A$ of $X$ is complete if and only if $A$ is closed in $X$.
*** Condition about continuity with respect to convergence
    A map $T\colon X \rightarrow Y$ is continuous if and only if $x_n
    \rightarrow x_0 \implies T(x_n) \rightarrow T(x_0)$.
*** Definition of isometry between metric spaces
** Theorem: $C[a, b]$ is complete
   Given a closed interval $[a, b]$, the $C[a, b]$ is complete. 
*** Proof
    Given a Cauchy sequence in this space. We use the completeness of $\R$ to
    define a function to which the Cauchy sequence will converge to. We need
    uniform convergence here.
** The set of polynomials on a $[a, b]$ is not complete
   $p_n(x) = \sum_i \left(\frac{x}{2}\right)^i$. 

* Lecture 4 <2018-10-25 Thu>
** Clarification
   Some sources say that an Isometry need not be surjective. 

   Let $T\colon X \rightarrow Y$ be injective and $d(x, y) = \tilde{d}(Tx, Ty)
   \forall x, y \in X$.

   Then $T$ is called isometry (from $X$) into $Y$. 

   A couple of examples of isometry were discussed. 

   Also we talked about uniform convergence, and also an example of a sequence
   that is not uniformly convergent was also discussed. (The idea is that on a
   compact set [fn:3], if a uniformly convergent function converges to a
   function is continuous.)

   Recall that we have already shown that $C[0, 1]$ is a complete space, meaning
   that if a sequence is Cauchy, then it has a limit in the space. Which means
   that the limit must also be continuous. (Kinda similar to how uniform
   convergence works.)
** Normed spaces and Banach spaces
   A Banach space is just a complete normed space.
*** Norm
    A metric space can be obtained by equipping a vector space with a metric
    defined in terms of the union, resulting in the *normed space*. Complete
    normed spaces are called *Banach space*.

    A mapping from one normed space to another is called an *operator* ($T\colon
    X \rightarrow Y$). A mapping from a normed space to $\R$ is called a
    *functional* ($T\colon X \rightarrow \R$)

    It can be shown that a linear operator is continuous if and only if it is
    bounded.

    The set of all bounded linear operators from a space $X$ to a space $Y$ is
    again a normed space.

    Similarly, the set of all bounded linear functionals is a normed space, the
    dual space $X'$ of $X$.
*** Vector space
    I didn't write this down. But it's pretty clear. 
*** Examples
    1. $\R^n$
    2. $C[a, b]$. It's kinda clear how to define a vector space structure on it. 
*** Subspace
    We defined a subspace of a vector space. 
*** Span
    Span was defined.
*** Linear independence
*** Definition of dimension
*** Basis of space
    To construct basis for finite dimensional spaces are clear. 

    One can prove the existence of a basis for infinite dimensional spaces, but
    the proof is not constructive.
** Normed spaces
*** Definition of a norm
*** Examples
    1. $\R^n$
    2. $C[a, b]$ with $\Vert f \Vert$ with $\Vert f\Vert = \max_{x \in [a, b]}
       \vert f (x) \vert$. Is also a Banach space.
    3. $\Omega \in \R^n$ be a measurable set, then the space $L^p(\Omega)$ is
       the set of all Lebesgue measurable functions from $\Omega \rightarrow
       \R$. We can define the norm in a straightforward manner.

       We can show that this space is a Banach space.
* Lecture 5 <2018-10-30 Tue>
** Remark
   Not every metric is induced by a norm.

   Assume discrete metric is induced by a norm, i.e., $d(x, y) = \Vert x -
   y\Vert$. The proof is easy.
** Convergence of sequences in normed spaces
   1. A sequence $(x_n)$ in a normed space $X$ is convergent if there is an $x\in
      X$ with $\lim_{n\rightarrow \infty} \Vert x_n - x\Vert = 0$. We write again
      $\lim_{n\rightarrow \infty} x_n = x$.
   2. A sequence is called a cauchy sequence if for all $\epsilon > 0$, there
      exists $N$, $\Vert x_m - x_m \Vert < \epsilon$ for all $m, n > N$. [fn:4]
** Absolute convergence
   A series $S$ is called absolutely convergent if $\sum \Vert x_i \Vert$
   converges. If $X$ is complete, absolute convergence implies convergence.

   Example 1. The alternating sum $\sum (-1)^n/n$ is convergent, but not
   absolutely convergent. This converges to $-ln(2)$.

   Example 2. $X = \Q$, $a_n = \sum \left(\frac{1}{2^{i}} -
   \frac1{(i+1)!}\right)$. The first part converges to $1$. The second element
   converges to $e-2$. The total is $3-e$. Let $b_n$ be $\sum
   \frac{-1}{(i+1)!}$. This converges to $2 - e$. Now $a + b = 5 - 2e$. $a - b =
   1$. One of them converges to an element in $\Q$ whereas the other one does
   not convert to an element in $\Q$. Now we construct a new sequence with one
   element from the first one and the next element from the second one and
   continue doing this $(a_1, b_1, a_2, b_2, \cdots)$. $S_n$ will be the partial
   sums. Then $\sum x_i$ converges to $a+b$, but $\sum \vert x_i \vert$
   converges to $a - b = 1$.
** Definition of basis
   We can now define a basis as follows: Assume that the normed space $X$
   contains a sequence $(e_n)_n$. Such that every $x \in X$ in the normed space
   $X$, can be expressed in terms of $(e_n)_n$, i.e., every $x$ can be expressed
   as a weighted combination or linear sums of $(e_n)_n$. $\Vert x - \sum
   \alpha_i e_i \Vert \rightarrow 0$ as $n \rightarrow \infty$ and this
   expansion is also unique.

   If such a basis exists, then $X$ is separable.
** Theorem about completeness
   Every finite dimensional subspace $Y$ of a complete normed space $X$ is
   complete, which implies that all finite dimensional normed spaces are
   complete. [fn:5]
** Norm equivalence
   We have two norms. We want to say when two norms are equivalent.

   A norm $\Vert \Vert_1$ on $X$ is said to be equivalent to a norm $\Vert
   \Vert_2$ if there exists $a, b > 0$ such that

   $a \Vert x \Vert_1 \le \Vert x \vert_2 \le b \Vert x \Vert_2$.

   An example: For finite-dimension vector space, then all norms are equivalent.

   We'll show that $\Vert \Vert_2$, and $\Vert \Vert_\infty$ are equivalent.

   Given any $X \in \R^n$, $\Vert x\Vert_2^2 = \sum_{i=1}^{n} x_i^2 \ge \max_i
   x_i^2 = \Vert x\Vert_\infty$. Also $\Vert x \Vert_2^2 = \sum_{i=1}^{n} x_i^2
   \le n \cdot \max x_i^2 = n \Vert x \Vert_\infty$. Thus $\Vert x \Vert_\infty
   \le \Vert x\Vert_2 \le \sqrt{n}\Vert x \Vert_\infty$.

   *A non-example*: Consider $X = C[0, 1]$ and define $f_n(X) = X^n$. Clearly,
   $f_n \in C[0, 1]$ for all $n$.

   $\Vert f_n \Vert_\infty = \max_{x\in [0, 1]} \vert f_n(x)\vert$. Now we
   introduce another norm which is the Lebesgue integral. For $f_n$, this would
   evaluate to $1/{n+1}$. The contradiction is the fact that the maximal norm
   would be $1$ always, whereas, the Lebesgue norm would tend to $0$ as
   $n\rightarrow \infty$. There is a clear contradiction here.
** Compactness in metric spaces
   A metric space is defined to be compact if every sequence in $X$ has a
   convergent subsequence. A subset $M$ of $X$ is defined to be compact if $M$
   considered as a subspace is compact, i.e., every sequence has a convergent
   subsequence and the limit is in $M$.
** Theorem
   If $X$ is a finite-dimensional normed space, then any subset $M \subset X$
   is compact if and only if $M is closed and bounded.

   Consider the sequence $\{x_n\}$ with $x_n = (-1)^n$, then $(x_n)$ does not
   converge, but it has convergent subsequences.
** Bolzano-Weistrass theorem
   Any bounded sequence $(x_n) \in l^{\infty}$ has a convergent subsequence.
*** Proof
    Without loss of generality, we assume that all elements are with $[0, 1]$,
    otherwise we an shift it and normalize it (can we do this?) Divide the
    interval $\{0, 1/2\}$ and $\{1/2, 1\}$, then one of them must have
    infinitely many points. (We can repeat the argument), we have a new sequence
    which are elements of $x_n$ that are in the interval (the interval with
    infinitely many points.) We can repeat this process again and the length of
    the intervals go to zero. It is clear how to construct a convergent
    subsequence.
*** Extension to bounded sub-sequences in $\R^n$.
* Lecture 6 <2018-11-01 Thu>
** Review
   In $\R^n$, the compact subsets are the closed and bounded subsets so that
   close-ness and boundedness can be used to define compactness. This can only
   be used for finite dimensional cases.
** Riesz's lemma
   Given a normed space $X$ with a closed subspace $Y$ and a subspace $Z$ such
   that $Y$ is a subset of $Z$. Given any number $\theta\in (0, 1)$, there
   exists, $z\in Z$ such that $\vert z \vert = 1$, and the distance $\Vert z - y
   \Vert \ge \theta$, for all $y \in Y$.
*** Proof
    Let $v\in Z \setminus Y$, define $a = \inf_{y \in Y} \Vert v - y\Vert$ to
    be the distance to $y$. Since $y$ is closed, $a>0$. Choose $\theta \in (0,
    1)$, then there exits a $y_0 \in y$, with $a \le \Vert v - y_0 \Vert \le
    \frac{a}{\theta}$.

    Define $z = \vert{1}{\Vert V = y_0}(V-y_0)$. Clearly, $\Vert z\Vert = 1$.
    Furthermore, given $y\in Y$, it holds that $$\Vert z - y\Vert =
    \Vert\frac{1}{\Vert v - y_0\Vert}(v -y_0) - y\Vert = \frac{1}{\Vert v -
    y_0\Vert} \left\Vert v - y_0 - \Vert v-y_0\Vert\cdot y\right\Vert$$

    Since $Y$ is a subspace, $y_1 \in Y$ and $\Vert v - y_1\Vert \ge a$ (since a
    is the infimum.)

    $\Vert z - y\Vert = \frac{1}{\Vert v - y_0\Vert} \Vert V - y_1\Vert \le
    \frac{\theta}{a}a = \theta$
** Theorem 
   If the closed unit ball $M=\{X \vert \Vert X \Vert \le 1\}$ of a normed space
   $X$ is compact, then $X$ is finite-dimensional.

   The proof uses Riesz's lemma. Assume that $M$ is compact set but $\dim X =
   \infty$, this leads to a contradiction. Compact sets have compact images
   under continuous mappings.
** Theorem
   Suppose $X$ and $Y$ are metric spaces and $T \colon X \rightarrow X$ is
   continuous. Then any compact subset of $X$ is mapped to a compact subset of
   $Y$.
*** Proof
    Proof is easy.
** Corollary
   Given a continuous mapping $T \colon M \rightarrow \R$, where $M$ is a
   compact subset of $X$. Then $T$ assumes a maximum and a minimum at some
   points of $M$.
*** Proof
    The proof is easy. It's something like take the infimum, it has to be a
    point in the space because closed.
** Example
   The closed unit ball $M=\{f \colon \Vert f \Vert \le 1 \}$ of $C[0, 1]$ is
   not compact. To see this, define $f(X) = \max(1 - \vert X \vert, 0)$ and
   $f_n(X) = f(2n\cdot (n+1)(x - \frac{1}{n}))$ (functions with center $1/n$ and
   decreasing bandwidth converges to the zero function) [fn:6]

   Now $f_n \in C[0, 1]$ and $\Vert f_n \Vert_\infty = 1$. Since supports of $f_n$ do not overlap.

   $\Vert f_n f_m\Vert = \max_{X \in \{0, 1\}} \vert f_n(X) - f_m(X)\vert = 1$
   and the sequence does not have a convergent subsequence.
** Linear Operators
   We now consider linear operators and their properties.
*** Definition
    Let $T$ be an operator, $D(T)$ its domain and $R(T)$ its range. The operator
    is called linear if $T(x + y) = Tx+Ty$ and $T(\alpha x) = \alpha Tx$

    Note that we typically write $Tx$ and not $T(x)$ as it is done for functions.

    The null set $N(T)$ is defined $N(T) = \{x \in D(T) \colon Tx = 0\}$

    In particular, linearity implies $T0 = 0$.
*** Example
    Define $T$ by $(Tf)(x) = \int_{a}{x} f(\tilde x) d \tilde x$ for $f\in C[a,
    b]$. Then $T(cf + cg) = cTf + dTg$. It is easy to show that the operator is
    linear.
*** Example
    1. Let $X$ be the space of all polynomials defined on $[a, b]$. We can define a
       linear operator to be the derivative, $Tf = f'$. [fn:7]
    2. Given $A \in \R^{m\times n}$, $T \colon \R^n \rightarrow \R^m$, $X
       \rightarrow Ax$ is linear $T(ax + by) = aAx + bAy = aTx + bYy$.
    3. Let $k$ be a square-integrable function on $[a, b]^2$ and $X=L_2[a, b]$,
       then define $f\mapsto\int_{a}^{b} h(x, \cdot) f(x)\ dx$[fn:8] [fn:9]
** Theorem about linear operators
   Suppose that $T$ is linear, then
   1. $R(T)$ is a vector space
   2. $N(T)$ is a vector space.

   I think $R$ and $N$ are range and kernel, respectively. The proof is kinda
   easy.
* Lecture 7 <2018-11-06 Tue>
** Regarding convergence of a function
   $\sup(f_n)\subset [0, 2/n]$, $f_n(0) = 0$, consider fixed $x\in [0, 1]$.
   Given any $\varepsilon > 0$, choose $N > 2/X$, then $X > 2/N > 2/N$ for
   $n>N$. Thus $x \in \sup(f_n)$ and $f_n(x) = 0$, $f_n$ converges point-wise to
   $0$.

   $f_n(x) = f(2n(n+1)(x - 1/2))$
** Injectivity of operators
   If $T$ is injective, there exists $T^{-1}\colon R(T) \rightarrow D(T)$ with
   $T^{-1}y =x$ for $Tx =y$, i.e., the inverse of $T$. It follows that $T^{-1}Tx
   = T^{-1}Ty = x$ and $TT^{-1}y = Tx = y$.

   Example: Given $A\in \R^{n\times n}$, $T\colon \R^n \rightarrow \R^m$, $x
   \mapsto Ax$.

   If $m < n$, $T$ can be injective if the rank of $A = n$, but it cannot be
   surjective. When $m > n$, then $T$ can be surjective, when the Rank of $A =
   m$, but it cannot be injective. If $m = n$, then $T$ is bijective if and only
   if the rank of $A = m = n$.
** Theorem
   Given vector spaces $X$ and $Y$, and a linear operator $T \colon D(T)
   \rightarrow Y$, then
   1. $T^{-1}\colon R(T) \rightarrow D(T)$ exists if and only if $T(x) = 0
      \implies x = 0$, i.e., the null space $N(T) = 0$. Then $T^{-1}$ is also a
      linear operator.
   2. If the dimension of the domain of $T$ is smaller than $\infty$, and
      $T^{-1}$ exists, then $\dim R(T) = \dim D(T)$.
*** Proof
    Assume that $T^{-1}$ exists then $T$ is injective and $Tx = 0 \implies x =
    0$. Conversely, assume $Tx_1 = Tx_2$, then $Tx_1 - Tx_2 = T(x_1 - x_2) = 0$.
    Thus, by assumption $x_1 - x_2 = 0 \implies x_1 = x_2$ and $T$ is injective.
    Hence the inverse $T^{-1}$ exists.

    To show that $T^{-1}$ is a linear operator: Given $y_i = T x_i$ and $x_i =
    T^{-1}y$, for $i = 1, 2$, then it follows that $T(\alpha x_1 + \beta x_2) =
    \alpha T x_1 + \beta T x_2 = \alpha y_1 + \beta y_2$ and $T^{-1}T(\alpha
    x_1 + \beta x_2) = T^{-1}(\alpha y_1 + \beta y_2)$. From this one can show
    that if $T^{-1}$ exists, it is also a linear operator.

    1. Let $\dim D(T) = n < \infty$, then $\dim R(T) \le n$, i.e., $\dim R(T)
       \le \dim D(T)$. This can be seen as follows: choose $n+1$ elements $y_1,
       \cdots, y_{n+1} \in R(T)$ and we choose them arbitrary, then we can find
       pre-images $x_1, \cdots, x_{n+1}\in D(T)$, it holds that $Tx_i = y_i$,
       for all $i$. Since we assumed $\dim D(T) = n$, the $x_i$ are linearly
       dependent, i.e., there exist $\alpha_i \in \R$ such that $\sum \alpha_i
       x_i = 0$ where not all $\alpha_i$ are zero. Now $T \sum_i^{n+1} \alpha_i
       x_i= \sum_{i=1}^{n+1} \alpha_i y = 0$, but not all $\alpha_i$ are zero.
       Thus we have found a set of linearly dependent vectors. Since we chose
       the vectors arbitrarily, we see that the dimension must be less than or
       equal to $n$.

       If we apply the same reasoning to the inverse operator, which we assume
       exists, we obtain in similar fashion that the $\dim D(T) \le \dim R(T)$.
       Thus, $\dim R(T) = \dim D(T)$.
** Lemma
   Let $X, Y, Z$ be vector spaces $T \colon X \rightarrow Y$, and $S \colon Y
   \rightarrow Z$ bijective operators, then we claim that $(ST)^{-1} = T^{-1}
   S^{-1}$.
** Definition: Bounded operators
   Given normed spaces $X$ and $Y$ and a linear operator, $T \colon D(T)
   \rightarrow Y$, $D(T) \subset X$, $T$ is defined to be bounded if there
   exists a constant $c$ such that $\Vert Tx \Vert \le c \Vert x \Vert$ and this
   has to hold for all $x \in D(T)$. A bounded linear operator maps bounded sets
   in $D(T)$ onto bounded sets in $Y$.

   We define the norm $\Vert T \Vert = \sup_{x \in D(T), \lambda \neq 0}
   \frac{\Vert T x \Vert}{\Vert x \Vert}$ to be the norm of $T$. It is
   straightforward to verify that this satisfies the properties for norm.

   For bounded linear operators, the bound can be computed by the supremum over
   $\Vert x \Vert = 1$. This is straightforward to see.
** Example
   Given $A \in \R^{n \times n}$, the linear operator $T \colon \R^{n}
   \rightarrow \R^{n}$ is bounded since $(\sup_{\Vert x \Vert= 1} \Vert A x
   \Vert)^2 = \sup x^{T} A^{T} A x = \lambda \max(A^T A)$, the largest
   eigenvalue of $A^{T}A$ (Rayleigh-Ritz theorem.)

   Note that for $A = U \sum V^T$, so that $A^{T} A = V \sigma^2 V^{T}$, thus
   $\lambda_{\max} = \delta_{1}^2$, where $\delta_1$ is the largest singular
   value of $A$.

   Let $X$ be the space of all polynomials on $[0, 1]$ and $\Vert f \Vert =
   \max_{x \in [0, 1]} \vert f (x) \vert$. The differentiation operator $T$ with
   $Tf = f'$ is not bounded. Since for $f_n(x) = x^n$, $\Vert f_n\Vert = 1$, all
   these functions are bounded above by $1$, hence the norm is $1$, whereas, the
   norm of the derivative is $n$ and this is unbounded.

   $T \colon C[0, 1] \rightarrow C[0, 1]$ by $(Tf)(x) = \int_{0}^{x} f(t)\ dt$.
   Then $\Vert T f\Vert = \max_{x \in [0, 1]} \vert \int_{0}^{1} f(t)\ dt\vert
   \le \max_{x \in [0, 1]} \int_{0}^{1} \vert f(t) \vert dt \le (1-0) \max_{x\in
   [0, 1]} \vert f(x) \vert = \Vert f \Vert$, thus $\Vert T \Vert \le 1$, but we
   can choose the function identical to $1$, then the norm is exactly equal to
   $1$.

   Define $c = \{x_n \in l_1 \vert \exists N \in \N \colon x_n = 0, \forall n >
   N\}$, with the $l_1$ norm. We define the operator $T\colon (x_1, x_2, x_3,
   \cdots) = (x_1, 2x_2, 3x_3, \cdots)$ is unbounded, we can compute $\Vert T
   e_i \Vert = i$, so if we have the sequence but the $\Vert e_i \Vert = 1$.
** Theorem
   Every linear operator on a finite dimensional normed space is bounded.
*** Proof
    Let $n$ be the dimension of $X$ and $\{e_1, \cdots, e_n\}$ a basis.

    Any $x\in X$ can be written as $X = \sum \alpha_i e_i$, thus $\Vert Tx \Vert
    = \Vert T \sum \alpha_i e_i\Vert \le \sum \Vert T \alpha_i e_i\Vert = \sum
    \vert \alpha_i \vert \le \max_{i} \Vert T e_i \Vert . \sum \vert \alpha_i
    \vert$. We define $\Vert x \Vert_0 = \sum \vert \alpha_i \vert$ defines a
    norm. Since all norms on finite-dimensional spaces are equivalent, there
    exist $c$ such that $\Vert x \Vert_0 \le c \Vert x \Vert$. Thus $\Vert T
    x\Vert \le \max \Vert T e_i \Vert \cdot c \cdot \Vert x \Vert$ and $T$ is
    bounded.
* Lecture 8 <2018-11-08 Thu>
** Theorem about Bounded operators
   For linear operators, continuity and boundedness are equivalent.

   Continuity of $T$ means that $\forall \varepsilon > 0$, there exists a
   $\delta > 0$, such that $\Vert x - x_0 \Vert < \delta \implies \vert Tx -
   Tx_0 \vert < \varepsilon$.
*** Proof
    There is nothing to show for $T=0$, now we assume that $T$ is not the zero
    operator, i.e., $\exists r$ such that $\Vert T x \Vert \le r \Vert x \Vert$.
    Using linearity, we can write that $\Vert Tx - Tx_0\Vert = \Vert T (x -
    x_0)\Vert \le \delta \Vert x - x_0\Vert$. Choose $\delta =
    \varepsilon/\delta$, thus $\Vert x -x_0 \Vert < \delta \implies \Vert Tx -
    Tx_0 \Vert \delta \Vert x - x_0\Vert\le \gamma\cdot \delta =\varepsilon$ and
    $T$ is continuous.

    Conversely, assume that $T$ is continuous. Take arbitrary $y\in D(T)$,
    define $X = X_0 + \frac{\delta}{\Vert y \Vert} y$. Thus $\Vert x - x_0 \Vert
    = \Vert \frac{\delta}{\Vert y \Vert} \cdot y \Vert = \delta$. Since $T$ is
    continuous, $\Vert Tx - Tx_0 \Vert < \varepsilon$. Now $\Vert Tx - Tx_0\Vert
    = \Vert T \delta/\Vert y \Vert y \Vert = \frac{\delta}{\Vert y \Vert}\Vert T
    y \Vert < \varepsilon$ Multiplying by $\Vert y \Vert / \delta$, we get
    $\Vert T y \Vert < \varepsilon/\delta \cdot \Vert y \Vert$ We call the last
    term $\gamma$.

    The second part of the proof shows that continuity in one point suffices to
    show boundedness. And boundedness means continuity at all points. Thus
    continuity at one point implies continuity at all points. Pretty interesting!
** Corollary
   Given a linear bounded operator $T$, it holds that 
   1. $x_n \rightarrow x$ implies that $Tx_n \rightarrow Tx$
   2. The null space of such an operator is closed
*** Proof
    These are basically properties of continuous functions.
** Definition
   We write that $T_1 = T_2$ if $D(T_1) = D(T_2)$ and $T_1x = T_2x$ for all $x
   \in D(T_1) = D(T_2)$. Furthermore $T\vert_B$ denotes the restriction to the
   set $B$, i.e., $T\vert_B \colon B \rightarrow Y$ with $T\vert_B x = Tx$ for
   all $x \in B$.

   The extension of $T$, denoted by $\tilde{T}\colon M \rightarrow Y$ where $D(T)
   \subset M$ is defined by $\tilde{T}x = Tx$, for all $x\in D(T)$.

   Example of extension. The set $X = Y = \R$, define $T$ by $Tx=x$, and $D(T) =
   [0, 1]. Then $\tilde{T}$ defined by $\tilde{T}x = \vert x \vert$ with
   $D(\tilde{T}) = [-1, 1]$ is an extension of $T$
** Theorem
   If $T \colon D(T) \rightarrow Y$ is a bounded linear operator, $D(T)$, part
   of a normed space, $Y$, a Banach space, then there is an extension
   $\tilde{T}\colon \overline{D(T)} \rightarrow Y$ with $\Vert \tilde{T} \Vert =
   \Vert T \Vert$. Furthermore, $\tilde{T}$ is a bounded linear operator.
*** Proof
    For any $x \in \overline{D}(T)$ consider the sequence $(x_n)_n$ in $D(T)$ that
    converges to $x$.

    $\Vert Tx_n - Tx_m \Vert = \Vert T(x_n - x_m)\Vert \le \Vert T \Vert \Vert
    x_n - x_m\Vert$. Since $x_m$ converges, $Tx_n$ is Cauchy and converges as we
    assumed $Y$ to be complete. The rest of the argument is trivial.
** Linear functionals
   A functional is a map from $X$ to $\R$ or $\C$. Given a functional $f$,
   $D(f)$ denotes the domain, $R(f)$ denotes the range of $f$. 

   For functionals, we typically write $f(x)$ and not $fx$, although $f$ is
   still an operator.
*** Example
    1. For a normed space $X$, $\Vert . \Vert \colon X \rightarrow \R$ is a functional.
    2. For $X=\R$, and $x_0 \in X$, $f\colon X\rightarrow \R$, $x\mapsto x_0^{T}x
       = \langle x_0, x\rangle$.
    3. Linearity is defined as before, with the difference that $y$ is now $\R$
       if $X$ is a real or $\C$, if $X$ is a complex space.
*** Example
    1. The norm functional is not a linear functional. $\Vert \alpha x + \beta y
       \Vert \neq \alpha \Vert x \Vert + \beta \Vert y \Vert$. This is not true
       in general.
    2. Define $f(x) = x_0^{T}x$ is linear, clearly, because the scalar product
       is bilinear. It is linear in each variable.
    3. The evaluation functional given by the Dirac delta function $\delta_x f =
       f(x)$.
    4. The definite integral is a functional. Let $l$ denote the functional such
       that for $f$ in $C[a, b]$, $l(f) = \int_{a}^{b} f(x) dx$, then it is
       straightforward to verify that this is linear.
*** Remark
    Similarly, boundedness is again defined as: A linear functional is bounded
    if there exists a constant $c$ such that $\vert f(x) \vert \le c \cdot \Vert
    x \Vert$. and $\Vert v \Vert = \sup \vert f(x) \vert / \Vert x \Vert =
    \sup_{\Vert x \Vert = 1} \vert f(x) \vert$.
*** Theorem
    Let $f\colon D(f) \rightarrow K$ be a linear functional, then $f$ is
    continuous if and only if it is bounded.
*** Example
    1. For integrable functions on $[a, b]$, it is easy to see that the integral
       functional is bounded.
    2. The dot product example can be extended to $l_2$ by choosing a fixed
       element $a$ in $l_2$ and setting $f(x) = \sum a_i x_i$. Due to
       Cauchy-Schwarz inequality $\vert \langle x, y\rangle\vert \le \Vert x
       \Vert \cdot \Vert y \Vert$.
* Lecture 9 <2018-11-08 Thu>
** Example
   $(x_n)_n$, it's basically a sequence of sequences. $x_n \in l_\infty$.

   $\delta_X f = f(x)$.

   The evaluation functional $\delta_X$ on $C[a, b]$ with norm $\Vert \cdot
   \Vert_\infty$ is bounded. But it might be unbounded with respect to another
   norm.
** Linear functionals
   The set of all linear functionals forms a vector space denoted by $X^{*}$.
   
   The algebraic dual space. For defining a vector vector space, we need the
   basic operations $+$ (addition) and $\cdot$ scalar multiplication, which can be
   defined as follows: we take two functionals $f_1$ and $f_2$ and a scalar
   $\alpha$, is easy to write $(f_1 + f_2)(x) = f_1(x) + f_2(x)$ etc. This part
   is obvious.

   We can also consider the dual of the dual space $X^{**}$, the second
   algebraic dual space. It is clear that there is a canonical isomorphism
   between $X$ and $X^{**}$.

   There was an example with $V$ along with an orthogonal basis.
** Definition of isomorphism of vector spaces
** About finite linear operators
   Let us now consider finite dimensional vector spaces. Any linear operator
   between two finite-dimensional vector space can be regarded as a matrix. To
   see this we have two finite dimensional spaces $X$ and $Y$ and a linear
   operator $T \colon X \rightarrow Y$. let $\{x_1, \cdots, x_n\}$ be a basis of
   $X$ and $\{z_1, \cdots, z_n\}$ be a basis of $Y$. 

   Then for each $x\in X$, we can write $X = \sum \alpha_i X_i$ and $y = Tx =
   \sum \alpha_i Tx_i$ and define $Tx_i = y_i$. Thus by knowing the images
   $y_i$, $T$ is uniquely defined. For any $z \in Y$, it can be written as $z =
   \sum \beta_j z_j$ as well as $y_j = Tx_i = \sum \gamma_{ji}z_j$.

   Now we have two representations in $z_j$, $j = 1, \cdots, m$. It follows that
   $\beta_j = \sum_{i=1}{n} \gamma_{ji} \alpha_i$ and that $y = Tx$ is
   determined by knowing the coefficients $\gamma_{ji}$, which can be written in
   matrix form as

   $$T_\mu = [\gamma_{ji}]_{j=1, \cdots m; i =1, \cdots m}$$
   
   Then $\beta = T_\mu \alpha$.
** Example
   Consider the discrete dynamical system $\phi \colon \R^2 \rightarrow \R^2$ by
   $\phi(x) = [\lambda x_n, \mu x_2 + (\lambda^2 - \mu)x^2n]^T$

   The Korpman operator $K$ is an infinite-dimensional operator defined by $Kf =
   f\circ \phi$, i.e., $(Kf)(\lambda) = f(\phi(x))$ for $f \in L_{\infty}$. This
   operator apparently plays an important role in Dynamical system.
* Lecture 10 <2018-11-15 Thu>
** Koopman operator
   $Kf = f\circ I$

   Consider the discrete dynamical system $\phi \colon \R^2 \rightarrow \R^2$ by
   $\phi(x) = [\lambda x_n, \mu x_2 + (\lambda^2 - \mu)x^2n]^T$

   Then the space is spanned by functions $\{x_1, x_2, x_1^2\}$ forms a so-called
   Koopman-invariant subspace. Let $f_1(\bf{x}) = x_1$, $f_2(\bf{x}) = x_2$,
   $f_e(\bf{x}) = x_n^2$, then any function from this subspace can be written as
   $f = \sum_{i=1}^{3} \alpha_i f_i$ and $g=Kf = K\sum(\alpha_i f_i) = \sum
   \alpha_i Kf_i$. We call the term $Kf_i = g_i$.

   $g_1(x) = Kf_1(x) = f_1(\phi(x)) = \lambda x_1 =\lambda f_1(x)$.

   $g_2(x) = Kf_2(x) = f_2(\phi(x)) =\mu x_2 + (\lambda^2 - \mu)x_1^2 - \mu
   f_2(x) + (\lambda^2 - \mu)f_3(\lambda)$
   
   $g_3(x) = Kf_3(x) = f_3(\phi(x)) = \lambda^2 x_1^2 = \lambda^2 f_3(x)$

   Thus $g = Kf = \alpha_1 \lambda f_1 + \alpha_2[\mu f_2 + (\lambda^2 -
   \mu)f_3] + \alpha_3 \lambda^2 f_3 = \alpha_1\lambda f_1 + \alpha_2 \mu f_2 +
   (\alpha_2(\lambda^2 - \mu) + \alpha_3 \lambda^2)f_3$

   Now we can write this as a matrix.

   It follows that

   $\gamma_{11} = \lambda$, $\gamma_{12} = 0$, $\gamma_{13} = 0$

   $\gamma_{21} = 0, \gamma_{22} = \mu, \gamma_{13} = 0$

   $\gamma_{31} = 0, \gamma_{32} = \lambda^2 - \mu, \gamma_{23} = \lambda^2$

   The following is a matrix representation: 

   | $\mu$ | 0             | 0           |
   | 0     | $\mu$         | $0$         |
   | $0$   | $\mu^2 - \mu$ | $\lambda^2$ |
   
   That is defining, $\bar{f} = [f1, f2, f3]^T$ and $\alpha = [\alpha_1,
   \alpha_2, \alpha_3]^T$.

   We obtain $f = \alpha^T f$ and $g=(T_\mu \alpha)^T\bar{f}$.

   Note that $\varphi_1(x) = x_1$, $\varphi_2(x)=x_2 - x_n^2$, $\varphi_3(x) = x_1-x_n^2$ are
   eigenfunctions. Corresponding to the eigenvalues

   $\lambda_1 = \lambda \cdot; Ke_1 = \lambda \varphi_1$

   $\lambda_2 = \lambda^2; K\varphi_2 = \lambda^2e_2$

   $\lambda_3 = \mu; Ke_3 = [\mu X_2 + (\lambda^2 - \mu)x_1^2 - \lambda^2 x_n^2]
   = \mu x_2 - \mu x_n^2 = \mu(x_2 - x_n^2) = \mu \varphi_3$
** Matrix representation
   Assume now again that $X$ is a vector space with $\dim X = n$ and that
   $\{x_1, \cdots, x_n\}$ forms a basis. Given a linear functional $f$, we
   obtain for $X = \sum_{i=1}^{n} \alpha_i X_i$ that $f(x) = f(\sum \alpha_i
   x_i) = \sum \alpha_i f(x_i) = \sum\alpha_ic_i$. We call the last term our
   coefficient $c_i = f(x_i)$. Thus $f$ is uniquely determined by the values
   $c_i, i=1, \cdots, n$.

   Conversely, any set of values values $\alpha_i, i =1, \cdots, n$ uniquely
   defines a linear functional. A special set of functionals is defined as
   follows: $f_j(x_i) = \delta_{ij}$. It's one when $i=j$, and $0$ otherwise. We
   call this the dual basis of $\{x_1, \cdots, x_n\}$.
** Theorem
   Let $X$ be again an $n$ dimensional vector space with basis $\{x_1, \cdots,
   x_n\}$. Then $\{f_1, \cdots, f_n\}$ as defined above is a basis of $X^{*}$.
   As a result, we have $\dim X^{*} = \dim X$.
*** Proof
    It's kinda easy. We just show that the maps $f_j$ is a basis and we're done.
** Theorem
   A finite-dimensional vector space is algebraically reflexive, i.e., the
   canonical embedding is an isomorphism between $X$ and $X^{**}$.
** Normed space of operators
   Let $X$ and $Y$ be arbitrary normed spaces, then the set $B(X, Y)$ of all
   bounded linear operators from $X$ to $Y$ is again a normed space.

   We need addition, scalar multiplication and a norm, and define:
   1. $(T_1 + T_2)x = T_1 x + T_2x$ for any $T_1, T_2 \in B(X, Y)$
   2. $(\alpha T) x = \alpha Tx$ for any $T\in B(X, Y), \alpha \in K$
   3. $\Vert T\Vert$ is the supremum norm that we have already defined.
** Theorem
   $B(X, Y)$ is a Banach space if $Y$ is a Banach space, i.e., $B(X, Y)$ is
   complete if $Y$ is complete.[fn:10]
** Definition
   The set of all bounded linear functionals on $X$ is a normed space with
   $\Vert f \Vert = \sup \vert f x\vert / \Vert x \Vert$ for $x \neq 0$.

   From the above theorem, since $\R$ is complete, the space of all bounded
   linear functionals converge. This is called this *dual space* (the
   continuous/topological dual) and is denoted by  $X'$.

   Remark: The algebraic dual space $X^{*}$ contains all linear functionals of
   $X$, whereas $X'$ contains only the bounded linear operators. 
** About $X'$ and $X^{*}$
   The space of all bounded linear functionals on $X$, given by $X'$, forms a
   linear subspace of $X^{*}$.

   Assume that $f$ and $g$ are bounded by $a$ and $b$, $\vert f(x) \vert \le a
   \Vert x \Vert$, $\vert g(x) \vert \le b \Vert x \Vert \forall x\in X$.

   Using triangle inequality, we can see that $af + bg$ is bounded if $f$ and
   $g$ are bounded. For scalars multiplication, it is similarly true. Thus it
   forms a linear subspace.
** Examples
   1. $(\R^n)' = \R^n$
   2. $(l^1)' = l^{\infty}$
   3. For $1 < p < \infty$ and $\frac{1}{p} + \frac{1}{q} = 1$, $(l^p)' = l^q$.
      Here $=$ means there exists an isomorphism.
** About $l^p$ and $l^q$
   Given $1 \le p \le \infty$, with $\frac{1}{p} + \frac{1}{q} = 1$. Take any
   $(y_n)_n \in l^p$, then $f\colon l^p \rightarrow \R$, $(x_n)_n \mapsto \sum
   x_n y_n$ is a bounded linear functional. The norm of this functional is
   $\Vert f \Vert = \Vert y \Vert_q$.
* Lecture 11 <2018-11-20 Tue>
** About $l^p$ and $l^q$
   $\frac{1}{q} + \frac1p = 1$, $(y_n)_n \in L^q$, $f\colon l^p \rightarrow \R$,
   $(x_n)_n \mapsto \sum x_n y_n$

   Note that $\sum \vert x_n y_n\vert \le \Vert x \Vert_p \cdot \Vert x \Vert_q$
   done to the Holder inequality.
** Theorem
   For $1\le p < \infty$, $(l^p)' \equiv l^q$ and an isometric isomorphism is
   given by $T \colon l^q \rightarrow (l^p)'$, $(Ty)(x) = \sum_{n=1}^{\infty}
   x_n y_n$.
*** Proof
    $\Vert Tt \Vert_{l^p}' = \Vert y \Vert_q$ as shown above and $T$ is linear.
    The $N(T)$ is only the sequence $0$, meaning that the mapping is injective.
    Show that the mapping is also surjective. So for any given functional, we
    need to find a corresponding $y$.

    Take any $x' \in (l^p)^{*}$ and define $y = (y_n)_n$ with $y_n = x'(e_n)$
    then $y \in l^q$: Fix $N \in \N$ for $p > 1$, (can be shown for $p=1$)
    construct the sequence: $\sum_{n=1}^{N} \vert y_n \vert^q = \vert_{n=1}^{N}
    \vert y_n \vert^q = \sum \frac{\vert y_n \vert^q}{y_n} y_n = \sum
    \frac{\vert y_n\vert^q}{y_n} x'(e_n) = x'(\sum \frac{\vert y_n \vert^q}{y_n}
    e_n$ (this is using the linearity). Now we can use that $x'$ is bounded.

    $X'\left(\sum \frac{\vert y_n \vert^q}{y_n} e_n\right) \le \Vert X' \Vert \sum \vert
    y_n\vert^q}{y_n} e_n \Vert = \Vert X'\Vert \left(\sum (\vert
    y_n\vert^{(q-1)\cdot p})\right)^{1/p} = \Vert X'\Vert \left( \vert y_n
    \vert^q\right)^{1/p}$[fn:19]

    Here $\frac1p + \frac1q = 1$ implies that $\frac1p = \frac{q-1}{q} \implies
    (q-1)p = q$.

    Divide by $\sum_{i=1}^{N} \vert y_n \vert^q)^{1/p}$, then
    $\left(\sum_{n=1}^{N} \vert y_n \vert^q\right)^{1-1/p} \le \Vert X'\Vert$.
    For $N \rightarrow \infty$, we obtain $\Vert y \Vert_q \le \Vert X'\Vert <
    \infty$

    We have shown $y\in l^q$, i.e., for each functional $x'$, we can find such a
    $y$. Now, $(T_y)(e_n) = (y_n)$ and also we define $y_n = X'(e_n)$. This
    means that it is really an isomorphism.

    Thus $Ty = x'$ on span $\{e_1, \cdots,\}$ which is a basis of $l^p$.
** About reflexiveness
    The dual of $l^\infty$ is not $l^1$, but a different one.
** Inner Product spaces and Hilbert spaces 
*** Definition
    In vector spaces we have addition and scalar multiplication. In normed
    vector spaces, we additionally have a norm $\Vert . \Vert$ which gives us
    lengths of vectors. A norm induces a metric by $d(x, y) = \Vert x - y
    \Vert$.

    However, we have not yet defined the notion of orthogonality in $\R^n$. Two
    vectors are orthogonal if $\langle x, y \rangle = x^{T} y = x \cdot y = 0$.

    The inner product induces a norm and thus also a metric by $\Vert x \vert =
    \langle x, x \rangle^{1/2}$. Here $\langle x, x \rangle = x^{T} x =
    \sum_{i=1}^{n} \hat{x}_i = \Vert x \Vert_2^2$ induces the standard Euclidean
    norm.

    With the inner product, we can thus compute angles in $\R^n$. This concept
    will be generalized in what follows. The resulting space with the inner
    product is called an inner product space. Inner product spaces that are
    complete are called *Hilbert spaces*

    | normed space        | complete normed space = Banach Space         |
    | inner product space | complete inner product space = Hilbert space |
*** Definition
    Let $X$ be a vector space. A mapping $\langle ., \cdot \rangle \colon X
    \times X \rightarrow K$ ($=\R$ or $=\C$ as usual) is called an inner product
    space (or scalar product.) if 
    1. $\langle x_1 + x_2 , y \rangle = \langle x_n , y \rangle + \langle x_2, y\rangle$, $x_1, x_2, y \in X$
    2. $\langle \lambda x, y \rangle = \lambda \langle x, y \rangle$, $x, y, \in X, \lambda \in K$.
    3. $\langle x, y\rangle = \overline{\langle y, x \rangle}$
    4. $\langle x, x \rangle \ge 0$, for all $x\in X$.
    5. $\langle x, x \rangle = 0 \iff x = 0$.
*** Remark
    It follows $\langle x, y_1 + y_2 \rangle = \overline{\langle y_1 + y_2, x
    \rangle} = \overline{\langle y_1, x\rangle} + \overline{\langle y_2,
    x\rangle}$ and

    $\langle x, \lambda y\rangle = \lambda \langle x, y\rangle$ similarly.
*** Theorem
    Let $X$ be a vector space and $\langle . , . \rangle$ an inner product, then
    $\vert \langle x, y \rangle\vert^2 = \langle x, x \rangle \cdot \langle y, y
    \rangle$ for all $x, y \in X$. Equality when $x$ and $y$ are linearly
    dependent.
**** Proof
     For arbitrary $\lambda \in K$, we can write $0 \le \langle x + \lambda y,
     x + \lambda y \rangle = \langle x, x \rangle + \langle \lambda y, x
     \rangle + \langle x, \lambda y \rangle + \langle \lambda x, \lambda y
     \rangle$.

     Now it's kinda easy to see when we put $\lambda = -\frac{\langle x, y
     \rangle}{\langle y, y \rangle}$ for $y \neq 0$.
*** Lemma
    Assigning $x \mapsto \langle x, x \rangle^{1/2}$ indeed defines a norm.
**** Proof
     The proof is not too hard hence skipped.
* Lecture 12 <2018-11-22 Thu>
** Definition
   A normed space $(X, \Vert . \Vert)$ is called an inner product space (or
   pre-Hilbert space) if an inner product $\langle ., .\rangle$ exits such that
   $\langle x, x \rangle^{1/2} = \Vert X \Vert$ for all $x\in X$.

   An easy example is $\R^n$ with the standard inner product.

   We have seen that an inner product induces a norm. What about the other case?
   Given a norm, can we get an inner product? We can do this in the following way:

   $\langle x, y \rangle = \frac{1}{4}(\Vert x + y \Vert^2 - \Vert x - y
   \Vert^2)$ (for real numbers.)

   $\langle x, y\rangle = \frac{1}{4}\left(\Vert x + y\Vert^2 - \Vert x - y
   \Vert^2 + i \Vert x +iy\Vert^2 - i\Vert x - iy\Vert^2\right)$. For complex
   numbers.[fn:11]

   Furthermore, the so-called parallelogram law holds:

   $\Vert x + y \Vert^2 + \Vert x - y \Vert^2 = 2(\Vert x \Vert^2 + \Vert y
   \Vert^2)$, this can be noticed easily. It's called Parallelogram law because
   it has something to do with parallelograms.
** Lemma
   The inner product is a continuous mapping from $X \times X$ to $K$.
*** Proof
    Let $x_n \rightarrow x$ and $y_n \rightarrow y$, we need to show that
    $\langle x_n, y_n \rangle$ converges to $\langle x, y \rangle$. This is
    straightforward to verify. It involves triangle inequality and
    Cauchy-Schwarz.
** Theorem (When is a normed space, an inner product space?)
   A normed space $(\lambda, \Vert . \Vert)$ is an inner product space if and
   only if the parallelogram law holds for all vectors.
*** Proof
    We have already seen that an inner product satisfies the parallelogram law.
    Now, one have to prove this the other way around.

    We consider only $\R$. We need to show that $\langle x_1 + x_2, y \rangle =
    \langle x_1, y \rangle + \langle x_2, y \rangle$.

    Define $\langle x, y \rangle = \frac{1}{4}(\Vert x + y \Vert^2 - \Vert x-
    y\Vert^2)$ as shown, then, $\langle x_1 + x_2, y \rangle =
    \frac{1}{4}\left(\Vert x_1 + x_2 + y \Vert^2 - \Vert x_1 +x_2 - y \Vert^2$.

    Some lengthy calculations and we end up with the result.
** Examples
   1. $\C^n$ with the inner product $\langle x, y \rangle = \sum x_i \bar{y_i}$
      is a Hilbert space.
   2. $l_2$ is a Hilbert space with $\langle x, y \rangle = \sum x_i \bar{y}_i$
   3. $l_2(\Omega)$ where $\Omega \in \R^n$ is an open subset is a Hilbert space
      with $\langle f, g \rangle = \int f(x)\overline{g(x)} dx$
   4. $C([a, b])$ with $\langle f, g \rangle = \int_{a}^{b} f(x)g(x)\, dx$ (only
      real-valued functions.) This is apparently not a Hilbert space. This is an
      inner product space. This example is similar to an exercise in one of the
      tutorials.
   5. Let $\R^{m\times n}$ denote the set of all real $m\times n$ matrices.
      Define $\langle A, B\rangle = \tr(A^TB)$. For $A = (a_{ij}), B= (b_{ij})$,
      we obtain $[A^TB]_{ij} = \sum [A^T]_{ik} [B]_{kj} = \sum a_{ki}b{kj}$ and
      $\tr(A^TB) = \sum_{i}(A^TB)_{ii} = \sum_i\sum_j a_{ki}b_{ki}$. Then
      induced norm, is $\Vert A \Vert = \sum A, A\rangle^{1/2} = \left(\sum_i
      \sum_k a_{ki}^2\right)^{1/2} = \Vert A \Vert_F$, i.e., the Frobenius norm.

      Apparently, we can use the parallelogram law to show that $l^p$ when
      $p\neq 2$ is not a Hilbert space.
* Lecture 13 <2018-11-27 Tue>
** Example (parallelogram law is invalid)
   The space $l^p$ for $p \neq 2$ is not a Hilbert space since it does not
   satisfy the parallelogram law.

   Define $x = (1, 1, 0, \cdots)$ and $y = (1, -1,0, \cdots)$. Both of them are
   in $l^p$.

   The norm of $x$ is $2^{1/p}$ and it is same as the norm of $y$.

   The norm of $x +y$ is $2$.

   The norm is $x - y$ is $2$.

   The parallelogram law says $4 + 4 = 4(4^{1/p})$ which is not true for $p \neq
   2$.
** Orthogonality
   With the aid of the inner product, we can now introduce the notion of
   orthogonality as already mentioned above.
   
   Let $X$ be an inner product space, then $x, y \in X$ are called *orthogonal*
   denoted by $x \perp y$ if $\langle x, y \rangle = 0$.

   Two subsets $A$ and $B$ are orthogonal if $\langle x, y \rangle = 0$ for
   every $x\in A$ and $y\in B$.

   Now, given a set $A$, we want to know the set of all elements that are
   orthogonal to $A$. $A^{\perp} =\{y \in X\vert x \perp y, \forall x \in A\}$
   is called orthogonal complement of $A$.

   $A^{\perp}$ is a closed subset of $X$. (Kinda easy to see in terms of
   continuity of the inner product and $y$ being the inverse of a closed set)
   The proof in the class used the fact that if a sequence in $A^\perp$
   converges, then the limit will be orthogonal to $x$ as well.
** Definition
   Given the elements $x, y$ of a vector space $X$, the segment joining $X$ and
   $Y$ is defined as $\{z \vert z = \alpha x + (1-\alpha)y, 0 \le \alpha \le
   1\}$

   A subset $A \subset X$ is said to be *convex* if for every combination $x, y
   \in A$ is in the set $A$.
** Theorem
   Let $H$ be a Hilbert space and $K \subset H$ closed and convex. Furthermore,
   let $x_0 \in H$, then there exists a unique $x$ in the set $A$ which has the
   shortest distance with $\Vert x - x_0 \Vert = \inf_{y \in K} \Vert y =
   x_0\Vert$[fn:12]
*** Proof
    If $x_0 \in K$ simply choose $x = x_0$. The proof is easy.
** Lemma
   Let $K$ be a closed and convex subset of $H$. For $x \in K$, the following
   statements are equivalent:

   1. $\Vert x_0 - x\Vert = \inf_{y\in Y} \Vert x_0 - y\Vert$
   2. $Re \langle x_0 - x, y -x \rangle \le 0$ for all $y \in K$.
*** Proof
    Geometric interpretation: assume that $K=\R$, then $\langle a, b\rangle =
    \Vert a \Vert \Vert b\Vert \cos(a, b)$ and $\langle a, b \rangle <0$ implies
    the angle is obtuse.

    Thus $Re\langle x_0 - x, y - x\rangle \le 0$ means that the angle between
    $x_0 - x$ and $y-x$ is obtuse ($K = \R$)
** Definition
   A vector space $X$ is defined to be the direct sum of the subspaces $Y$ and
   $Z$, denoted by $Y = Y \oplus Z$ if each $x \in X$ have a unique
   decomposition such that $x = y + z$, where $y\in Y$ and $z \in Z$.

   The mapping defined above is in general a non-linear projection. (a diagram
   was drawn about circles)

   Reminder: a projection onto a vector space is a mapping $P$ with $P^2 = P$
** Theorem
   Let $U \neq \{0\}$ be a closed subspace (now just a subspace, not convex) of
   a Hilbert space $H$. Then there exists a linear projection $P_u$ from $H$
   onto $U$ with $\Vert P_u \Vert = 1$ and $N(P_u) = U^{\perp}$. Furthermore,
   $Id - P_u$ is a projection onto $U^{\perp}$ with $\Vert Id - P_u\Vert = 1$.
   It holds that we can split this Hilbert space into $H = U \oplus U^{\perp}$.
   And this i a linear projection [fn:13]
* Lecture 14 Skipped <2018-11-29 Thu>
* Lecture 15 <2018-12-06 Thu>
** Review (Bessel's inequality)
   $\sum \vert \langle x, e_i \rangle \vert^2 \le \Vert x\Vert^2$
   
   Some inequality that happened last Thursday.
** Fourier coefficients
   The inner product $\langle x_i, e_i\rangle$ are called Fourier coefficients.

   If $\{e_i \vert i \in \N\}$ is a orthonormal basis, we obtain equality $\Vert
   x \Vert^2 = \sum \vert \langle x_i, e_i \rangle \vert^2$.
** Lemma
   Let $\{e_i, i \in \N\}$ be an orthonormal system and $x,y \in H$. Then you
   can show that $\sum_{i=1}^{\infty} \vert\langle x_i, e_i \rangle \langle e_i
   y\rangle \vert < \infty$.
*** Proof
    The proof simply uses the Holder's inequality.

    $\sum \vert \langle x, e_i \rangle \langle e_i, y\rangle \vert \le (\sum
    \vert \langle x, e_i \rangle \vert^2)^{1/2} (\sum \vert \langle e_i, y
    \rangle \vert^2)^{1/2} \le \Vert x \Vert \Vert y \Vert < \infty$
** Difference between orthonormal basis and orthonormal system
   Consider $\R^3$, then $e_1$ and $e_2$ form an orthonormal system. But it is
   not a basis, clearly.

   One can calculate the bessel's inequality thing.

   There was something about Paiseval's equality which was mentioned in the
   lecture.
** Theorem
   For an infinite-dimensional Hilbert space $H$. The following statements are equivalent

   1. $H$ is separable.
   2. All orthonormal basis are countable.
   3. There is at least one orthonormal basis
*** Proof (idea)
    1 to 2. Start with an orthonormal basis, we must show that it must be
    countable. Take two vectors $e_i$ and $e_j \in S$, then $\Vert e_i - e_j
    \Vert^2 = \sum \langle e_i - e_j, e_i - e_j\rangle= \Vert e_i\Vert^2 + \Vert
    e_j \Vert^2 = 2$. This means that the distance between two basis vectors are
    always two.

    Take the neighbourhood $B(e_i, \frac{\sqrt{2}}{3}) \cap B(e_j \sqrt{2}, 3) =
    \emptyset$. It is empty in $S$.[fn:18]

    This is apparently similar to the fact that $l^\infty$ is not separable?

    So if $S$ were uncountable, then we could have an uncountable number of
    disjoint sets. This contradicts the fact that $H$ is separable. Why? $H$ has
    a countable basis, but then if we have a set of disjoint open sets, then we
    can define an injection between our uncountable set and the countable set.

    2 to 3 is clear.

    3 to 1. The idea is that we take linear coefficients that are rational. Then
    we show that this set is dense in $H$ and then you're done.
** How does non-separable Hilbert spaces?
   Consider the space of functions $f\colon \R \rightarrow \R$ with the property
   that $f(x)\neq 0$ only for a countable set and the property that $\Vert f
   \Vert < \infty$, where $\Vert \cdot \Vert$ is the norm induced by the inner
   product. $\langle f, g \rangle = \sum_{x\in \R} f(x)g(x)$

   How do we show that this is not separable? 

   Define $f_y(X) = 1$ when $x \neq y$ and $0$ otherwise. For $y_0 \neq y_1,
   \Vert f_{y_0} - f_{y_1} \Vert = \Vert f_{y_0}\Vert^2 - 2\langle f_{y_0},
   f_{y_1}\rangle + \Vert f_{y_1}\Vert^2 = 2$

   We obtain an uncountable number of disjoint sets. Thus the space cannot be
   separable.
** Fourier series
   We consider the space $L^2[0, 2\pi]$, we define the set of basis functions:
   $S = \{\frac{1}{\sqrt{2\pi}} I\} \cup \{\frac{1}{\sqrt{\pi} \cos(nx)}\} \cup
   \{\frac{1}{\sqrt{\pi}}\sin(nx)\}$ See example 3.18 (iii).

   The inner product is defined by $\langle f, g \rangle = \int_0^{2\pi}
   f(x)g(x)\ dx$. We can use the following identities to show that $S$ is indeed
   an orthonormal system.

   $\int_0^{2\pi} \cos(mx) \cos(nx)\ dx$ is $0$ when $m\neq n$ and $2\pi$, when
   $m$ and $n$ are $0$, and it is $\pi$ if $m = n$, but not both $0$.

   Similarly, for the $\sin$, $\int_{0}^{2\pi} \sin{mx}\sin{nx}\ dx$ is $0$,
   when $m\neq n$ or $m=n=0$ or $\pi$, when $m=n\neq 0$.

   Also $\int_0^{2\pi} \sin mx \cos nx\ dx = 0$. Thus $\langle
   \frac{1}{\sqrt{\pi}} \cos mx, \frac{1}{\sqrt{\pi}} \cos nx\rangle =
   \delta_{m, n}$ for $m \neq 0, n \neq 0$.

   Holds the same for all other combinations. Hence $S$ is an orthonormal
   system.
** Example
   A trigonometric series is defined by $a_0 + \sum_{n=1}^\infty][a_k \cos(kx) +
   b_k \sin(kx)]$. Let $f$ be $2\pi$ periodic and continuous. Then the
   coefficients are defined by

   $a_0 = \frac{1}{2 \pi} \int_{0}^{2\pi} f(x)\ dx, a_k = \frac{1}{\pi}
   \int_0^{2\pi} f(x) \cos(kx)\ dx, b_k = \frac{1}{\pi} \int_0^{2\pi}
   f(x)\sin(kx)\ dx$.

   We and write the Fourier series as $\tilde{a_0} \frac{1}{\sqrt{2\pi}} I
   +\sum_{n=1}^{\infty} \tilde{a}_n \frac{1}{\sqrt{\pi}} \cos(kx) + \tilde{b}_n
   \frac{1}{\sqrt{\pi} \sin(kx)}$

   We know that $\tilde{a}_0 = a_0 \sqrt{2\pi} = \langle f, \frac{1}{\sqrt{2\pi}}I\rangle$.

   Similarly, we can express everything as a an inner product.

   That is, we write $f$ as $\sum_{e\in S} \langle f, e \rangle e$
** Example
   Let us consider the so called square wave function defined by, $f(x) = 1$
   between $0$ and $\pi$ and it is $-1$ between $\pi$ and $2\pi$.

   We can get that $a_k = 0$. $b_k = 1$ if $k$ is odd and $-1$ if $k$ is even.

   Another function is the sawtooth function.
* Lecture 16 <2018-12-11 Tue>
** Theorem
   Suppose $e_n$ is an orthonormal sequence in a Hilbert space $H$ (if an
   orthonormal set is countable, then we can arrange it as a sequence)

   1. The series $\sum_{i=1}^{\infty} a_ne_n$ converges if and only if
      $\sum_{n=1}^{\infty} \vert a_n \vert^2$ converges.
   2. If $\sum_{n=1}^{\infty} a_ne_n$ converges, then the coefficients $a_n$ are
      the Fourier coefficients $\langle x, e_n\rangle$ where $x$ is the element
      in $H$ we are approximating. Thus we can write $x = \sum_{n=1}^{\infty}
      \langle x, e_n \rangle e_n$.
*** Proof
    $H$ and $\R$ are both complete. Thus, a Cauchy sequence in $H$ converge if
    and only if the corresponding sequence in $\R$ is cauchy. To show this,
    define, $s_n = \sum_{k=1}^{n} a_ke_k$ and $\delta_n = \sum_{k=1}^{n} \vert
    a_n \vert^2$.

    Due to orthonormality, $\Vert s_n - s_m \Vert^2= \Vert \sum_{n=m+1}^{n} a_k
    e_k \Vert^2 = \sum_{k=m+1}^{n} \vert a_k\vert^2 = \delta_n - \delta_m$. Here
    we assume without loss of generality that $n > m$. Thus, the second
    statement can be seen as follows: $\langle s_n, e_j \rangle = \langle
    \sum_{k=1}^{n} a_n e_n , e_j \rangle = \sum_{k=1}^{n} a_n \rangle e_n, e_j
    \rangle = a_j$ for ($j \le n$)

    Since by assumption, $s_n \rightarrow x$ for $n\rightarrow \infty$.

    $a_j = \langle s_n, e_j \rangle \rightarrow \langle x, e_j \rightarrow$, (we
    have shown that the inner product is continuous.)

    Thus $a_j = \langle x, e_j \rangle$
** Orthonormal Polynomials
   [fn:14] Instead of using trigonometric basis functions for the Fourier
   analysis, we can use sets of orthonormal polynomials. Example, Legendre,
   Hermite or Laguerre Polynomials.
** Legendre Polynomials
   The Legendre Polynomials can be defined in terms of the generating functions
   $H(x, \gamma) = \frac{1}{(1-2x\gamma + \gamma^2)^{1/2}}$ This can be
   developed in a power series in $r$ so that $H(x, \gamma) = P_0(x) +
   P_1(x)\gamma + P_2(x) \gamma^2 + \cdots$

   Set $y = 2x\gamma - \gamma^2$, use $(1-y)^{-1/2} = \sum \binom{2k}{k}
   \frac{y^k}{2^{2k}}$.

   This will eventually result in $P_0(x) = 1, P_1(x) = x, P_2(x) =
   \frac{1}{2}(3x^2 - 1), P_3(x) = \frac{1}{2}(5x^3 - 3x)$.

   These polynomials are called Legendre polynomials and it holds that $P_n(-x)
   = (-1)^nP_n(x)$. Furthermore, $P_n(1) =1$ and using the previous property,
   $P_n(-1) = (-1)^n$.

   The polynomials can be defined recursively by $P_0(x) = 1, P_0(x) = x$, and
   $(n+1)P_{n+1}(x) - (2n+1)xP_n(x) + nP_{n-1}(x) = 0$.

   For instance, $2P_2(x) = 3x\cdot x + 1 = 0 \implies P_2(x) =
   \frac{1}{2}(3x^2 - 1)$.

   If we can now define the inner product $\langle f, g\rangle = \int_{-1}^{1}
   f(x) g(x)\ dx$, then $\langle P_n(x), P_m(x) \rangle = 0$ for $m\neq n$. That
   is, we defined a set of orthogonal, but not yet orthonormal functions. The
   normalized Legendre polynomials, denoted by lower case $p_n$ are $p_n(x) =
   (\frac{2n+1}{2})^{1/2} P_n(x)$.

   Any function on $(-1, 1)$ can now be expanded in a series of Legendre
   polynomials, similar to the standard Fourier expansion.
** Hermite polynomials
   Hermite polynomials are orthogonal on $(-\infty, \infty)$ with the weight
   function $\exp{-x^2/2}$. Define $\phi(x) = \exp{-x^2/2}$, then $\phi'(x) =
   -x\phi(x)$, $\phi''(x) = (x^2 - 1)\phi(x)$. We define $H_n(x) = (-1)^n
   \exp(x^2/2) \frac{d^n}{d x^n2} \phi(x)$ to be the Hermite polynomial.

   This can be defined recursively by $H_{n+1}(x) = xH_n(x) - H_n(x')$. We then
   obtain, $\int_{-\infty}^{\infty} \exp{-x^2/2} H_m(x) H_n(x) = \delta_{m,n} n!
   \sqrt{2\pi}$.

   That is, the polynomials are orthogonal with respect to the inner product
   $\langle f, g \rangle = \int_{-\infty}^{\infty} f(x) g(x) \exp{-x^2/2}\ dx$,
   but not orthogonal.

   The normalized Hermite Polynomials are given by $H_n(x)/((n!) \sqrt{2\pi})$.
   We can expand, and arbitrary function $f(x)$ in an series of Hermite
   polynomials. $f(x) = \sum_{n=0}^{\infty} a_n H_n(x)$, where $a_n =
   \frac{1}{n! \sqrt{2\pi}} \langle f, H_k\rangle$
** Laguerre Polynomials
   The last set of orthonormal polynomials we want to consider are the so-called
   Laguerre polynomials which are orthogonal on $(0, \infty)$ with weight
   function $\rho(x) = x^ae^{-x}$, with $a> -1$. $a$ is typically zero.

   We define $\phi_n(x)=x^{a+n} \exp{-x}$ and $L_n^a(x) = (-1)^n x^{-a} \exp{x}
   \frac{d^n}{dx^n} \phi_n(x)$

   The laguerre Polynomials are orthogonal with respect to $\langle f, g\rangle
   = \int_{0}^{\infty} f(x) g(x) x^a \exp^{-x}\ dx$.

   The Laguerre polynomials can be defined recursively and be used for series
   expansions.
* Lecture 17 <2018-12-13 Thu>
** Adjoint operators
   We now want to introduce the adjoint operator of a bounded linear operator
   but first we need a few auxiliary results. (Something like Reiz's
   representation theorem for bilinear forms.)

   The representation theorem states that, given a bounded linear functional $f$
   on $H$, it can be written as $f(x) = \langle x, z\rangle$ for $z \in H$ (this
   element was uniquely, defined.)

   Let $h$ denote a sesquilinear functional (linear in the first,
   conjugate-linear in the second argument.) Here conjugate-linear means $h(x,
   ay_1 + by_2) = \overline{a}h(x, y_1) + \overline{b}h(x, y_2)$.

   For normed spaces, $X$ and $Y$, $h$ is said to be bounded if $\exists c$ such
   that $\vert h(x, y) \vert \le c \Vert x \Vert \Vert y \Vert$, for all $x, y$.
   If this property holds we call it a bounded sesquilinear form (form and
   functional are used interchangeably)

   Furthermore, $\Vert h \Vert = \sup \frac{\vert{h(x, h)}}{\Vert x \Vert \cdot
   \Vert y \Vert} = \sup_{\Vert x \Vert = 1, \Vert y \Vert = 1} \vert h(x,
   y)\vert$. That is, $\vert h(x, y) \vert \le \Vert h \Vert \cdot \Vert x \Vert
   \cdot \Vert y \Vert$.

   We can now extend the representation theorem to sesquilinear forms.
** Theorem
   Given a bounded sesquilinear form, $h \colon H_1 \times H_2 \rightarrow K$
   ($K$ is a field here), for two Hilbert spaces $H_1$ and $H_2$, then $h$ has a
   representation $h(x, y) = \langle S x, y \rangle$, where $S$ is uniquely
   defined linear operator with $\Vert S \Vert = \Vert h \Vert$.
*** Proof
    For $x$ fixed, $\overline{h(x, y)}$ is a linear functional (complex
    conjugate is required, otherwise, it would only be semi-linear in $y$.)

    Using the representation theorem, there exits a vector $z$ such that, we can
    write the linear functional in the form $\overline{h(x, y)} = \langle y, z
    \rangle$, now $h(x, y) = \overline{\langle y, z \rangle} = \langle z,
    y\rangle$ Here $x$ was fixed.

    If $x$ is not fixed, then $z$ depends on $x$ and define the operator $S$ by
    $Sx = z$, i.e., $h(x, h) = \langle Sx, y \rangle$.

    Now, we need to prove that $S$ is linear, is uniquely defined and is also
    bounded.

    The linearity of $S$ follows from the definition since $\langle S(ax_1 +
    bx_2), y \rangle = h(ax_1 + bx_2, y)$ and $h$ is linear in the first
    argument.

    Boundedness of $S$, $\Vert S \Vert = \sup_{x \neq 0} \frac{\Vert S x
    \Vert}{\Vert x \Vert} = \sup_{x \neq 0} \frac{\langle Sx,
    Sx\rangle^{1/2}}{\Vert x \Vert} = \sup_{x\neq 0, Sx \neq 0} \frac{\langle
    Sx, Sx \rangle}{\Vert x \Vert \cdot \Vert S x \Vert} \le \sup_{x \neq 0, y
    \neq 0} \frac{\vert \langle Sx, y \rangle \vert}{\Vert x \Vert \cdot \Vert y
    \Vert} = \sup_{x \neq 0, y \neq 0} \frac{\vert h(x, y)\vert}{\Vert y \Vert}
    = \Vert h \Vert$. Thus $\Vert S \Vert \le \Vert h \Vert$.

    $\Vert h \Vert = \sup_{x \neq 0, y \neq 0} \frac{h(x, h)}{\Vert x \Vert
    \cdot \Vert y \Vert} = \sup\frac{\Vert Sx, y \rangle}{\Vert x \Vert \Vert y
    \Vert} \le \sup \frac{\Vert Sx \Vert \cdot \Vert y \Vert}{\Vert x \Vert
    \cdot \Vert y \Vert} = \Vert S \Vert$.

    It follows that $\Vert S \Vert = \Vert h \Vert$.
    
    The uniqueness of $S$: Assume that there are two functions $S$ and $T$ with
    the same properties, then $\langle (S - T) x, y \rangle = 0$ for all $x, y$.
    This means that $S = T$.
** Definition
   Given two Hilbert spaces $H_1$ and $H_2$ and a bounded linear operator $T
   \colon H_1 \rightarrow H-2$, then adjoint operator, denoted by $T^{*}$ is
   defined by $\langle Tx, y\rangle = \langle x, T^{*}y \rangle$ for all $x \in
   H_1, y \in H_2$

   We need to show that the operator is really defined. But then we can use the
   previous theorem to show this.
** Theorem
   The operator $T^{*}$ exists and is unique. Furthermore, $\Vert T^{*}\Vert =
   \Vert T \Vert$.
*** Proof
    We define a sesquilinear form by $h(y, x) = \langle y, Tx\rangle$, then
    inner product is sesquilinear and $T$ is linear. Conjugate linearity can be
    seen as follows: $h(y, ax_1 + bx_2) = \langle y, T(ax_1+bx_2) \rangle =
    \langle y, aTx_2 + bTx_2 \rangle = \bar{a}\langle y, Tx_1\rangle +
    \bar{y}\langle y, Tx_2 \rangle = \bar{a}h(y, x_1) + \bar{y}h(y, x_2)$.
    
    Also, $\vert h(x, y) \vert = \vert \langle y, Tx \rangle \vert \le \Vert y
    \Vert \cdot \Vert Tx \Vert \le \Vert T \Vert \cdot \Vert x \Vert \cdot \Vert
    y\Vert$.

    Thus, $\Vert h \Vert \le \Vert T \Vert$. Now we will show that $\Vert T
    \Vert \le \Vert h\Vert$.

    On the other hand, $\Vert h \Vert = \sup_{x \neq 0, y \neq 0} \frac{\vert
    \langle y, Tx \rangle\vert}{\Vert y \Vert \cdot \Vert x \Vert} \ge \sup_{x
    \neq 0, Tx \neq 0} \frac{\langle Tx, Tx \rangle}{\Vert Tx \Vert \cdot \Vert
    Tx \Vert \Vert x \Vert} = \Vert T \Vert$.

    It follows that $\Vert h \Vert = \Vert T \Vert$.

    Now we can use the representation theorem for sesquilinear forms: there
    exits $S$ such that $h(y, x) = \langle Sy, x \rangle$ and we define $T^{*} =
    S$. Thus, $T^{*}$ is bounded and uniquely defined.

    It holds that $\Vert T^{*}\Vert = \Vert S \Vert = \Vert h \Vert$ and $\Vert
    h \Vert = \Vert T \Vert$. Together, $\Vert T^{*} = \Vert T \Vert$ as
    claimed. We also need to show that this satisfies the definition of the
    adjoint operator before.

    We need to show that $h(y, x) = \langle y , Tx\rangle$ and $h(y, x) =
    \langle T^{*}y, x \rangle$. Now $\langle x, T^{*}y \rangle =
    \overline{\langle T^{ *}y, x\rangle} = \overline{\langle y, Tx \rangle} =
    \langle Tx, y \rangle$ so that $T^{*}$ has the properties we were looking
    for $t$.
** Examples of Adjoint operators
*** Example 1
    Let $A \in \R^{n \times n}$, define $T \colon \R^n \rightarrow \R^n$ by $Tx
    = Ax$. The adjoint operator satisfies $\langle Tx, y \rangle = \langle x,
    T^{*}y\rangle$, i.e., $\langle Tx, y\rangle = \langle Ax, y \rangle =
    (Ax)^ty = x^t A^{t}y = x^{t} (A^{t}y) = \langle x, T^{*} y \rangle$.

    That is, the adjoint operator is given by the transposed matrix $A^{T}$ or
    $A^{H}$ if $A \in \C^{m \times n}$.
*** Example 2
    Let $X \subset \R^d$ and $p_\tau\colon X \times X \rightarrow \R$ be the
    transition density function associated with the stochastic process
    $\{X_t\}_t$. That is, $p_\tau(x, y)$ is the probability that the process
    starting in $x$ goes to $y$ in time $\tau$. We define two operators and
    assume that they are well defined on $L^2$.

    $(P_\tau) p(x) = \int_X p_\tau(y, x) p(y) dy$. (Perjon-Frobenius operator)

    $(K_\tau f)(x) = \int p_\tau(x, y) f(y) dy$. (Koopman operator.)

    Now, we want to show that $p_\tau$ and $K_\tau$ are adjoint with respect to
    $\langle f ,g \rangle = \int f(x)g(x) dx$.
*** Example 3
    For any $x \in \R^n$, define $T$ by $T(x_1, \cdots, x_n) = (0, x_1, \cdots,
    x_{n-1})$, i.e., right-shift operator. Now we want to find the adjoint
    $T^{*}$.

    The adjoint operator is the left shift operator. (One can see this by
    representing $T$ as a matrix, now use the fact that the adjoint of a matrix
    is the transpose.)

    $T^{*}(x_1, \cdots, x_n) = (x_2,\cdots, x_n, 0)$ (the left shift operator)
*** Example 5 (Adjoint operator does not exist)
    The adjoint operator does not always exist. Consider the space of all real
    valued polynomials along with the inner product $\langle f, g \rangle =
    \int_0^1 f(x) g(x)\ dx$. Define the operator $T = \frac{d}{dx}$ to be the
    differentiation operator. Recall that $\frac{d}{dx}$ is not bounded. We only
    showed that it exist for bounded.

    $\langle f, T^{*} g\rangle = \langle Tf, t \rangle = \int_0^1 f'(x)g(x)\ dx
    = [fg]_0^1 - \int_0^1 f(x)g'(x)\ dx = [fg](1) - [fg](0) - \langle f,
    Tg\rangle$ (integration by parts.) Thus $\langle f, (T^{*} + T) g \rangle =
    (fg)(1) - (fg)(0)$. We now define a function $f$ with $f(0) = f(1) = 0$.
* Lecture 18 <2018-12-18 Tue>
** Continuation on the example
   $\langle f, (T^{*} + T)g \rangle = (fg)(1) - (fg)(0), f(x) = x^2(1-x^2)p(x)$.

   $0 = \langle f, (T^{*} + T)g\rangle = \int_{0}^{1} x^2 (1-x^2)[(T +
   T^{*})g](x)\ dx = \langle x^2(1-x^2)(T + T^{*})g, p\rangle$

   Since the integral must be zero for all $p$, it follows that $x^2(1-x^2)[(T +
   T^{*})g](x) = 0$. The term $x^2(1-x^2)$ is positive for all $x \in (0, 1)$
   and only zero for $x=0$ and $x=1$. Thus $(T + T^{*})g$ must be zero for all
   $g$. This implies that $T + T^{*} = 0$ and as a consequence $\langle f, (T +
   T^{*})g \rangle = 0$.

   However, $\langle f, (T + T^{*}) g \rangle = (fg)(1) - (fg)(0)$ which is
   certainly not $0$ for all $f$ and $g$, thus $T^{*}$ cannot exist.
** Lemma
   Given two inner product spaces $X$ and $Y$ and a bounded linear operator $Q
   \colon X \rightarrow Y$

   1. $Q = 0 \iff \langle Qx, y \rangle  = 0 \forall x \in X, \forall y \in Y$
   2. If $X$ is complex and $Q \colon X \rightarrow X$ with $\langle Qx, x
      \rangle = 0, \forall x \in X$ then $Q = 0$.
*** Proof
    The first part is easy. We'll show the second part.

    $0 = \langle Q(ax_1, x_2), ax_1, x_2 \rangle = \vert a\vert^2 \langle Q x_1,
    x_1 \rangle + a \langle Qx_1, x_2\rangle + \bar{a} \langle Q x_2, x_1
    \rangle + \langle Q x_2, x_2\rangle$.

    For $a = 1$, we obtain that $\langle Q x_1, x_2 \rangle + \langle Q x_2, x_1
    \rangle = 0$.

    For $a = i$, we obtain $\langle Q x_1, x_2 \rangle - \langle Qx_2, x_1
    \rangle = 0$.

    The above two equations imply that $\langle Qx_1, x_2 \rangle = 0$ for all
    $x_1, x_2$.

    The property 2, does in general *not hold for real inner product* spaces.
    Define $Q \colon x \mapsto A x$, where $A$ is the following matrix

    | 0 | -1 |
    | 1 | 0  |

    For $x = (x_1, x_2)^T$, $Qx = (-x2, x_1)^T$ and $\langle Qx, x \rangle =
    -x_1 x_2 + x_2 x_1 = 0, \forall x \in X$, but $Q \neq 0$.
    
    The adjoint operator has the following properties:

    1. $\langle T^{*} y, x \rangle = \langle y, Tx\rangle$, we can just see this
       by applying complex conjugate.
    2. $(S + T)^{*} = S^{*} + T^{*}$ (use definition of adjointness and
       linearity of inner product.)
    3. $T^{**} = T$ since $\langle T^{**}x, s\rangle = \langle x, T^{*} y\rangle
       = \langle Tx, y \rangle$. Thus $\langle (T^{**} - T)x, y \rangle = 0$ for
       all $x, y$. From the previous lemma (part 1), we have that $T^{**} - T = 0$.
    4. $(aT)^{*} = \bar{a}T^{*}$ since $\langle (aT)^{*}x, y \rangle = \langle
       x, a Ty \rangle = \bar{a} \langle x, Tx\rangle = \bar{a}\langle T^{*}x, y
       \rangle = \langle \bar{a}T^{*}x, y \rangle$ Thus, $\langle [(aT)^* -
       \bar{a}T^{*}]x, y\rangle = 0$, $\forall x, y$, use argument from (3).
    5. If well-defined, i.e., $T \colon H_1 \rightarrow H_2$ $S \colon H_2
       \rightarrow H_3$: $(ST)^{*} = T^{*}S^{*}$ since $\langle x, (ST)^{*}y
       \rangle = \langle S(Tx), y \rangle = \langle Tx, S^{*}y\rangle = \langle
       x, T^{*}S^{*} y \rangle$
    6. $\Vert S S^{*} \Vert = \Vert S^{*} S \Vert = \Vert S \Vert^2$.
** Definition (unitary)
   Let $T \colon H_1 \rightarrow H_2$ be a bounded linear operator
   
   1. $T$ is called unitary if $T$ is invertible and $T T^{*} = Id_{H_2}$ and
      $T^{*}T = Id_{H_2}$. [fn:15]
   2. For $H_1 = H_2$, $T$ is called self-adjoint if $T = T^{*}$ [fn:16]
   3. For $H_1 = H_2$, then $T$ is called normal if $T^{*}T = TT^{*}$.

   unitary: $\langle T^{*}Tx, y \rangle = \langle x, y \rangle$ by definition.
   The length is preserved. Similar to linear algebra idea that length is preserved.

   Self-adjoint: $\langle Tx, y \rangle = \langle x, T^{**}y \rangle = \langle
   x, Ty \rangle$.

   Normal: $\langle Tx, Ty \rangle = \langle T^{*}Tx, y \rangle = \langle
   TT^{*}x, y \rangle = \langle T^{*}x, T^{*}y\rangle$

   Self-adjoint operators are obviously normal.
** Example
   From a previous example, we know that for $T \colon x \rightarrow Ax$ for a
   matrix $A \in \R^{n\times m}$, the adjoint is given by $T^{*} \colon y
   \rightarrow A^{T}y$.

   Thus self-adjointness means $A^{T} = A$ and $A$ must be symmetric. Unitary
   means $A^{T}A = I_n$ thus $m=n$ and $A$ must be orthogonal (unitary if $A$ is
   complex).

   The operator is normal if the matrix $A$ is normal, i..e, $A^{T}A = AA^{T}$.
** Example
   Let us consider the Peran-Frobenius and Koopman operator from a previous
   example again. A system is said to be reversible if the detailed balance
   condition is fulfilled.

   $\pi(x) p_\tau(x, y) = \pi(y) p_\tau(y, x) \forall x, y \in X$.

   Thus $\pi$ is an eigen function of $P_\tau$ with corresponding eigenvalue
   $l=1$.

   $P_\tau$ is self-adjoint with respect to $\langle \cdot, \cdot \rangle_{\pi -
   1}$ This can be seen as follows:

   $$\langle P_\tau f, g \rangle_{\pi^{-1}} = \int\int p_\tau(y, x)\ dx$$

   More calculations follow and we'll get the result.
** Example
   Suppose $A \in \C^{n \times n}$ is a self-adjoint matrix, i.e., $A = A^{*}$.
   Define $u = e^{iA} = \sum_{h=0}^{\infty} (iA)^n/{h!}$. Since for any $B \in
   \C^{n\times n}$: $(B^n)^{*} = (B^{*})^n$ and $(iA)^{*} = TA^{*} = -iA^{*} =
   -iA$ and thus $((iA)^{1/2})^{*} = ((iA)^{*})^{1/2} = (-iA)^n$. We obtain

   $u^{*} = \sum_{h=0}^{\infty} (-iA)^n/{h!} = e^{-iA}$. If two matrices $X$ and
   $Y$ commute, i.e., $XY = YX$, then $e^xe^y = e^{x+y}$.

   As $iA$ and $-iA$ commute, it holds that $e^{iA}e^{-iA} = I$. As a result,
   $U$ is unitary.
** Example
   For a function $f \in L^1(\mathbb{R}) \cap L^2(\R)$, we define the Fourier
   transform as follows:

   $F(\omega) = (\mathscr{F} f)(\omega) = \frac{1}{\sqrt{2\pi}}(\omega) =
   \frac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty e^{-i\omega x} f(x)\ dx$ Here
   $\mathscr{F}$ is the Fourier transformation operator.

   The inverse is given by $f(x) = (\mathscr{F}^{-1} F)(x) =
   \frac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty e^{i\omega x} F(\omega)\
   d\omega$.

   We then obtain $\langle \mathscr{F} f, \mathscr F g \rangle = \langle F,
   G\rangle = \langle f, g \rangle$ (Plancharel's theorem) and $\mathscr{F}$ is
   a unitary operator. For $f = g$, this results in $\Vert f \Vert = \Vert F
   \Vert$.

   (Compare this with Paiseval's identity, which can be viewed as a discrete
   version of this result. The identity says that the norm of the function is
   same as the $l_2$ norm of the coefficients. The above result says something
   similar.)
* Lecture 19 <2018-12-20 Thu>
** Planchevel's theorem
   $\langle F, G \rangle = \langle f, g \rangle$

   To prove Plancherel's theorem, we write $f(x) = \frac{1}{\sqrt{2\pi}}
   \int_{-\infty}^{\infty} F(\omega) e^{i\omega x}\ d\omega$ and use the
   definition of the Fourier transform

   $$f(x) = \frac{1}{2\pi} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(s)
   e^{-i\omega s}\ ds e^{i\omega x}\ d\omega$$

   Then $\langle f, g \rangle = \int_{-\infty}^{\infty}[\frac{1}{2\pi}
   \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(s) e^{-i\omega s}\ ds
   e^{i\omega x} dx \overline{g(x)}]\ dx$

   This is equal to $-\int_{-\infty}^{\infty}\left[\frac{1}{\sqrt{2\pi}}
   \int_{-\infty}^{\infty} f(s) e^{-i\omega s}\ ds\right]\left[
   \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} \overline{g(x)} e^{i\omega x}\
   dx\right] \ d\omega$

   Note that $\overline{\exp{z}} = \exp{\overline{z}}$ and thus.

   The above thing end up being $\int_{-\infty}^{\infty} F(\omega)
   \overline{G(\omega)}\ d\omega = \langle F, G \rangle$.

   Brief reminder: We used several properties of complex numbers in the last
   examples, e.g., $\overline{z_1 z_2} = \overline{z_1} \cdot \overline{z_2}$
   and $\overline{\exp{z}} = \exp{\overline{z}}$. They can be seen very easily.
** Lemma
   Given a bounded linear operator $T \colon H_1 \rightarrow H_2$, it holds that 

   1. $T$ is an isometry $\iff$
   2. $\langle Tx, Ty\rangle = \langle x, y \rangle$.
*** Proof
    1 to 2.

    Using the polarization identity (related to the Parallelogram law, for $K=
    \R$.)

    We start with $\langle Tx, Ty\rangle = \frac{1}{4}\left( \Vert Tx + Ty
    \Vert^2 - \Vert Tx - Ty\Vert^2\right)$ (this is the polarization identity)

    this can further be written as $\frac{1}{4} \left( \Vert T(x + y)\Vert^2 -
    \Vert T(x - y)\Vert^2\right)$

    Because of the isometry property, we see that this is equal to
    $\frac{1}{4}\left(\Vert x + y \Vert^2 - \Vert x - y \Vert^2\right) = \langle x, y
    \rangle$.

    Analogously, one can show it for $K= \C$.

    2 to 1.

    $\Vert Tx \Vert^2 = \langle Tx, Tx\rangle = \langle x, x \rangle = \Vert x
    \Vert^2$

    This implies that a length-preserving operator (an isometry) also preserves
    angles.
** Lemma
   For $K = \C$ and a linear $T \colon H \rightarrow H$ it holds that

   1. $T$ is self-adjoint if and only if $\langle Tx, x\rangle \in \R$ for all
      $x \in H$.

   This doesn't make sense in Real valued vector spaces and only make sense in
   Complex valued spaces.
*** Proof
    $\langle Tx, x \rangle = \langle x, Tx\rangle$ (here we are using
    self-adjointness)

    But $\overline{\langle x, Tx\rangle} = \langle Tx, x\rangle$. Thus clearly,
    it is real valued.

    To show the other way around, we use a commonly used trick
    
    First consider $\langle T(x + y), x + y \rangle \in \R$

    Now we use the linearity of the inner product $\langle Tx , x \rangle +
    \langle Tx , y \rangle + \langle Ty, x\rangle + \langle Ty, y\rangle$.

    Two terms on the right is in $\R$ by definition (the left most and the right
    most.)

    If we compute the complex conjugate, we obtain: $\langle T(x + y), x +
    y\rangle = \langle Tx, x\rangle + \langle y, Tx\rangle + \langle x,
    Ty\rangle + \langle Ty, y\rangle$.

    Hence, $\langle Tx, y\rangle + \langle Ty, x \rangle = \langle y,
    Tx\rangle + \langle x, Ty\rangle$

    Now we'll use the same thing for linear combination with $i$ included.

    Similarly $\langle T(x - iy), x - iy\rangle = \langle Tx, x\rangle + \langle
    -iTy, x\rangle + \langle Tx, -iy\rangle + \langle -iTy, -iy\rangle$

    This is equal to $\langle Tx, x \rangle - i \langle Ty, x \rangle + i\langle
    Tx, y \rangle + \langle Ty, y\rangle$.

    Complex conjugate and subtraction of the two equations results in: $\langle
    Tx, y \rangle + \langle Ty, x\rangle = - \langle y, Tx\rangle + \langle x,
    Ty\rangle$

    We have something similar before,

    Add the equations to get $\langle Tx, y\langle = \langle x, Ty\rangle$,
    which is what we wanted to show.
** Theorem
   If a bounded linear operator is self-adjoint, then $\Vert T \Vert =
   \sup_{\Vert x \Vert \le 1} \vert \langle Tx, x \rangle\vert$.
*** Proof
    The proof again uses the Parallelogram law.

    $\vert \langle Tx, x \rangle \vert \le \Vert Tx \Vert \Vert x \Vert \le
    \Vert T \Vert \cdot \Vert x \Vert^2$. Thus $\Vert T \Vert \ge \sup_{\Vert x
    \Vert \le 1} \Vert \langle Tx, x \rangle \vert$.

    On the other hand, define $M = \sup_{\Vert x \Vert \le 1} \vert \langle Tx,
    x\rangle \vert$.

    $\langle T(x + y), x + y\rangle - \langle T(x - y), x-y\rangle = \langle Tx,
    x\rangle + \langle Tx, y \rangle + \langle Ty, x \rangle + \langle Ty,
    y\rangle - [\langle \langle Tx, x\rangle - \langle Tx, y\rangle - \langle
    Ty, x\rangle + \langle Ty, y\rangle]$.

    This is equal to $\langle Tx, y \rangle + \langle Ty, x \rangle + \langle
    Tx, y\rangle + \langle Ty, x \rangle = \langle Tx, y\rangle +
    \overline{\langle Tx, y\rangle} + \langle Tx, y\rangle + \overline{\langle
    Tx, y\rangle} = 4 \operatorname{Re} \langle Tx, y\rangle$

    $\vert 4 \operatorname{Re} \langle Tx, y \rangle \vert \le \Vert \langle
    T(x + y), x+y\rangle \vert + \vert \langle T(x-y), x-y\rangle \vert \le M
    \cdot \left(\Vert x + y \Vert^2 + \Vert x- y \Vert^2\right) = 2 M(\Vert x
    \Vert^2 + \Vert y \Vert^2)$ This uses parallelogram law.

    Note that for arbitrary $z \colon \vert \langle Tz, z\rangle \vert = \Vert
    z\Vert^2 \langle T \frac{z}{\Vert z}, \frac{z}{\Vert z\Vert}\rangle \le
    \Vert z \Vert^2 \cdot M$

    For $\Vert x \Vert \le 1$ and $\Vert y \Vert \le 1$: $\Vert
    \operatorname{Re} \langle Tx, y \rangle \vert \le M$. Set $y =
    \frac{Tx}{\Vert Tx\vert}$. Set $y = \frac{Tx}{\Vert Tx \Vert}$, then $\vert
    \operatorname{Re} \langle Tx, \frac{Tx}{\Vert Tx\Vert}\vert = \Vert Tx \Vert
    \le M$
* Lecture 20 <2019-01-08 Tue>
  We finished Metric space, Banach spaces etc.
** Reproducing Kernel Hilbert spaces
*** Kernels and their properties
    We will consider similarity measures of the form $k \colon X \times X
    \rightarrow \R$, where $X$ is a non-empty set.
*** Definition
    A function $k \colon X \times X \rightarrow \R$ is called kernel if there
    exists a real Hilbert space $H$ and a map $\phi \colon X \rightarrow H$ such
    that for all $x, x' \in X$, we know that $k(x, x') = \langle \phi(x),
    \phi(x') \rangle_H$.

    We call $\phi$ feature map and $H$, the feature space of $k$.
*** Example
    A particularly simple similarity measure on $\R^n$ is the standard inner
    product on $\R^n$. $k(x, x') = \langle x, x'\rangle = x^T x' = \sum x_i
    x_i'$. Why is this a similarity measure? If we have two vectors that are
    orthogonal, then the similarity would be zero. Whereas if they are parallel,
    the inner product is closer. [fn:17]

    Let $x$ and $x'$ be vectors of unit length, then $k(x, x') = 1$ if $x = x'$
    (high similarity) and $h(x, x') = 0$ if $x \perp x'$ (low similarity).
*** Lemma
    Let $X$ be as above and $f_n \colon X \rightarrow \R$ and $n \in \N$ be
    functions such that $(f_n(x))_n\in l_2$ for all $x\in X$. Then $k(x, x') =
    \sum_{n = 1}^{\infty} f_n(x) f_n(x')$ defines a kernel.
**** Proof
     Holder's inequality yields that $\sum_{n=1}^{\infty} \vert f_n(x)
     f_n(x')\vert \le \Vert (f_n(x_n)) \Vert_{l_2} \cdot \Vert f_n(x')
     \Vert_{l_2}$. 

     Thus, by definition, of $f_n(x)$, the series converges absolutely, which
     means it also converges. Then one can define the Hilbert space to be $l^2$
     and $\phi \colon X\rightarrow H$ by $\phi(x) = (f_n(x))_n$, then $k(x, x') =
     \langle \phi(x), \phi(x')\rangle_H$ and defines a kernel. This completes
     the proof
*** Remark
    Almost all kernels have such a representation which can be constructed
    explicitly (Mercer feature space will be introduced later)

    Kernel based methods and reproducing kernel Hilbert spaces are often in
    machine learning. Let us consider a simple classification problem.
*** Example
    Assume that we have two classes of objects (e.g., sick/healthy) We are then
    given a new object and we have to assign it to one of the two classes.

    Mathematically, we can formalize this as follows: Give training data $x_1,
    \cdots, x_m$ and labels $y_1, \cdots, y_m$ with $x_i \in X$ and $y_i \in
    \{-1, 1\}$. Here $1$ would means that the person is sick and $-1$, that the
    person is healthy. Find a function $f\colon X \rightarrow \{-1, 1\}$ that
    assigns a new object a label (this is an instance of supervised learning)

    The first idea could be to assign a new data to the class with closer mean:

    $c_{+} = \frac{1}{m_{ +}} \sum_{y_i = 1} x_i$ and $c_{-} = \frac{1}{m_{-}}
    \sum_{y_i = 1} x_i$ where $m_{ +}$ and $m_{-}$ are the number of instances
    of objects with $y_i = 1$ and $y_i = -1$ respectively.

    We now compare the midpoint between $c_{+}$ and $c_{-}$, i.e., $c \colon
    \frac{1}{2}(c_{ +} + c_{-})$, and check whether a new data point $x$
    encloses an angle smaller than $\pi/2$ with the vector, $w = c_{ +} - c_{-}$
    which connects $c_{ +}$ and $c_{-}$.

    If the sign of the cosine of the angle between $x - c$ and $w$ is $1$, we
    assign by $+1$, otherwise to $-1$. Thus $y = \operatorname{sgn}(\langle x -
    c, w \rangle) = \operatorname{sgn}(\langle x - \frac{1}{c_{ +} + c_{-}}, c_{
    +} - c_{-})$, this can be shown to be same as $\operatorname{sgn} (\langle
    x, c_{ +} - \langle x, c_{-}\rangle + b)$ with $b = \frac{1}{2} \left(\Vert
    c_{-1}\Vert^2 - \Vert c_{ +}\Vert^2 \right)$.

    Instead of computing the inner product in the state space, we first
    translate the data using $\phi$ and compute the inner product in feature
    space using the kernel $k$, $y = \operatorname{sgn}[\langle x, \frac{1}{m_{
    +}} \sum_{y_i = 1} x_i \rangle - \langle x, \frac{1}{m_{-}} \sum_{y_i = -1}
    \sum x_i + b]$. This is same as $\operatorname{sgn}[\frac{1}{m_{ +}}
    \sum_{y_i = 1} \langle x, x_i \rangle - \frac{1}{m} \sum_{y_i = -1} \langle
    x, x_i \rangle + b]$.

    Kernelization, $y = \operatorname{sgn} [\frac{1}{m^{ +}} \sum_{y_i = 1}
    \langle \phi(x), \phi(x) \rangle - \frac{1}{m_{-}} \sum_{y_i = -1} \langle
    \phi(x), \phi(x_i) \rangle + \tilde{b}]$. Where $b$ is given by $\tilde{b} =
    \frac{1}{2} = \frac{1}{2} [ \frac{1}{m^2} \sum_{y_i = -1, y_j = -1} h(x_i,
    x_j) - \frac{1}{m^2_{ +}} \sum_{y_i = 1, y_j = 1} h(x_i, x_j)$

    This results in a non-linear version of the classification algorithm. Note
    that $\phi(x)$ does not need to be computed explicitly only kernel
    evaluations are required. This is (one aspect of) the so called kernel
    trick.

    If we now choose for instance, $k(x, x') = (1 + \langle x, x'\rangle)^2$
    also, the "circle example" introduced above can be classified.
*** Example
    Let $X = \R^2$ and the kernel $k(x, x') = (1 + \langle x, x'\rangle)^2$. We
    obtain, $k(x, x') = (1 + x_1 x_1' + x_2x_2')^2 = \cdots = 1 + 2x_1x_1' +
    2x_2 x_2' + 2x_n x_1'x_2x_2' + \lambda_1^2 \lambda'_1^2 + \lambda_2^2
    x_2'^2$.

    We can rewrite this as an inner product of two vectors.

    $(1, \sqrt(2)x_1, \sqrt{2}x_2, \sqrt{2}x_1x_2, x_1^2, x_2^2)^t$ and $(1,
    \sqrt{2}x_1', \sqrt{2}x_2', \sqrt{2}x_1'x_2', x_1^2' x_2'^2)^t$. This is
    $\langle \phi(x), \phi'(x)\rangle$.

    The feature space contains all monomials of order up to and including two.
    The inner product in the six dimensional feature space can be computed
    efficiently in the two dimensional state space. In general, for $\R^d$, the
    kernel $k(x, x') = (c + \langle x, x'\rangle)^p$ contains all monomials of
    order up to $p$.
*** Remark
    The feature space representation is not unqiue. That is, we have $h(x, x') =
    \langle \phi_1(x), \phi_1(x')\rangle$ and $h(x, x') = \langle \phi_2(x),
    \phi_2(x')\rangle$, $\phi_1$ and $\phi_2$ might be different 9also the
    associated hilbert spaces might be completely different) This however, does
    in general not matter since the goal is to write algorithms, in terms of
    kernel evaluations.
*** Lemma
    1. Given a kernel $k$, then $\alpha \cdot k$ for $\alpha > 0$ is a kernel.
    2. Given kernels $k_1$ and $k_2$, $k_1 + k_2$ is a kernel.
    3. Also $k_1 \cdot k_2$ is a kernel.
**** Proof
     $k(x, x') = \langle \phi(x), \phi(x') \rangle$, thus $(\alpha k)(x, x') =
     \langle \sqrt{\alpha} \phi(x), \sqrt{\alpha} \phi(x')\rangle$.

     $k_1(x, x') = \langle \phi_1(x), \phi_2(x) \rangle$, $k_2(x, x') = \langle
     \phi_2(x), \phi_2(x') \rangle$, define $\phi(x) = (\phi_1(x),
     \phi_2(x))^t$, then, $\langle \phi(x), \phi(x')\rangle = \langle
     [\phi_1(x), \phi_2(x)]^t, [\phi_1(x), \phi_2(x)]^t\rangle = \langle
     \phi_1(x), \phi_1(x') \rangle + \langle \phi_2(x), \phi_2(x') \rangle =
     k_1(x, x') + k_2(x, x')$

     $k_1(x, x')k_2(x, x') = \sum_{i}\phi_1^{i}(x) \phi_n^{i}(x')
     \sum_{j}\phi_2^j(x) \phi_2^j(x') = \sum_{i, j} [\phi_1^i(x)
     \phi_2^j(x)][\phi_1^i(x') \phi_2^j(x')]$

     This can be written as $\langle (\phi_1 \otimes \phi_2)(x), (\phi_1 \otimes
     \phi_2)(x') \rangle$. This is the tensor product of $\phi_1$ and $\phi_2$,
     i.e., $(\phi_1 \otimes \phi_2)_{ij} = \phi_i^{i} \phi_2^{j}$.
* Lecture 21 <2019-01-10 Thu>
** Positive Definite
   Given a Kernel $K$ and data $x_1, \cdots, x_m \in X$, the $m \times m$ matrix
   $G = (g_{ij})$ with $g_{ij} = k(X_i, X_j)$ is called Gram matrix.
** Definition
   A kernel $k \colon X \times X \rightarrow \R$ is called positive definite if
   for all $m$, for all $X_1, \cdots, X_m \in X$, for all $\alpha_1, \cdots,
   \alpha_m$, we have $\sum_{i, j = 1}^{n} \alpha_i\alpha_j k(x_i, x_j) =
   \alpha^T G \alpha \ge 0$ for all vectors $\alpha$.

   Furthermore, $k$ is said to be *strictly positive definite*, if for mutually
   different $X_1, \cdots, X_m$ equality $\alpha^T G \alpha = 0$ holds for
   $\alpha = 0$. (this is an additional condition) The kernel $k$ is called
   *symmetric* if $k(x, x') = k(x', x)$ for all $x, x' \in X$.
** Remark
   What we call positive definite is also sometimes called positive
   semi-definite and what we call strictly positive definite is called positive
   definite.
** Property
   A symmetric matrix $G$ is positive definite if and only if eigenvalues are
   non-negative. We're talking about Real, meaning that the Eigenvalues are
   positive.
** Remark
   The term kernel dates back to analysis of integral operators of the form
   $(T_k f)(x) = \int k(x, x') f(x') dx'$.
** Lemma (Cauchy-Schwarz for kernels)
   If $k$ is symmetric and positive definite (in short, s.p.d in what follows)
   then $k(x_1, x_2)^2 \le k(x_1, x_2)k(x_2, x_2)$
*** Proof
    Consider the Gram Matrix $G=$
    
    | $k(x_1, x_2)$ | $k(x_1, x_2)$ |
    | $k(x_2, x_1)$ | $k(x_2, x_2)$ |
    
    Which is by definition s.p.d. Remember that the product of all eigenvalues
    is equal to the determinant of the matrix. We now know that all the
    eigenvalues are non-negative, which means that the determinant is
    non-negative.

    The determinant is $k(x_1, x_2)k(x_2, x_2) - k(x_1, x_2)^2 \ge 0$. Thus we
    have the theorem.
** The reproducing kernel map
   Let $k$ be a s.p.d kernel and $X$ a non-empty set.
** Definition (Canonical feature map)
   Let $\R^X = \{ f\colon X \rightarrow \R \}$ denote the set of all functions
   from $X$ to $\R$. We define $\phi \colon X \rightarrow \R^X$ by $x\mapsto
   k(\cdot, x)$. (Note that we assign each point in $X$ a function.)

   Thus, $\phi(x)(x') = k(x', x) = k(x, x')$ We can now define functions by
   $f(\cdot) = \sum \alpha_i [\phi(x_i)](\cdot) = \sum_{i = 1}^{n} \alpha_i
   k(\cdot, x_i)$ for $m \in \N$, $\alpha_i \in \R$ and $x_1, \cdots, x_m \in
   X$.

   Given two functions $f = \sum_{i=1}^{m} \alpha_i k(\cdot, x_i)$ and $g =
   \sum_{j = 1}^{m'} \beta_j k(\cdot, x_j')$ we define an inner product by
   $\langle f, g \rangle = \sum_{i=1}^{m} \sum_{j = 1}^{m'} \alpha_i \beta_j
   k(x_i, x_j')$
** Lemma
   This new inner product $\langle \cdot, \cdot \rangle$ itself defines s.p.d
   kernel.
*** Proof
    Symmetry follows from the symmetry of the $k$. We have to show that for
    arbitrary function $f_1, \cdots, f_m$ and coefficients $\alpha_1, \cdots,
    \alpha_m$, $\sum_{i, j} \alpha_i \alpha_j \langle f_i, f_j \rangle = \langle
    \sum_{i} \alpha_i f_i, \sum_{j} \alpha_j f_j \rangle = \langle f,
    \tilde{f}\rangle$

    Let $\tilde{f}$ be within $\tilde{f} = \sum_{i = 1}^{m^2} \tilde{\alpha}_i
    k(\cdot, \tilde{x}_i)$, then $\langle \tilde{f}, \tilde{f}\rangle = \sum_{i,
    j} \tilde{\alpha}_i \tilde{\alpha}_i k(\tilde{x}_i, \tilde{x}_j) \ge 0$,
    since $k$ is pd. This completes the proof?

    Given $f = \sum_{i = 1}^{n} \alpha_i k(\cdot, x_i)$, we obtain $\langle f,
    k(\cdot, x) \rangle = \sum_{i=1}^{n} \alpha_i k(x, x_i) = f(x)$

    This is called the reproducing property. Thus, function evaluations can now
    be interpreted as inner products in an inner product space. In particular,
    we obtain $\langle k(\cdot, x), k(\cdot, x') \rangle = k(x, x')$ Since we
    defined $\phi(x) = k(\cdot, x)$, this yields, $\langle \phi(x), \phi(x')
    \rangle = k(x, x')$.

    This derivation showed that we can construct a feature map $\phi$ given a
    kernel $k$. Similarly, a feature map, defines as s.p.d. kernel, via $k(x,
    x') = \langle \phi(x), \phi(x') \rangle$ since $\sum_{i, j} \alpha_i
    \alpha_j(x_i, x_j) = \langle \sum_{i} \alpha_i \phi(x_i),
    \sum\alpha_j\phi(x_j)\rangle = \Vert \sum \alpha_i \phi(x_i) \Vert^2 \ge 0$.
** The reproducing kernel Hilbert space
   The space of functions given by $f = \sum_{i = 1}^{m} \alpha_i k(\cdot, x_i)$
   along with the inner product $\langle \cdot, \cdot \rangle$ defines an inner
   product space or pre-Hilbert space. We can now complete this space by adding
   limit points of sequences that converge in the norm $\Vert \cdot \Vert =
   \langle \cdot, \cdot \rangle^{1/2}$ induced by the inner product.

   1. $\langle f, h(\cdot, x)\rangle = f(x)$ for $f\in H$.
   2. $H = \operatorname{span}\{k(\cdot, x) \vert x\in X\}$

   There are equivalent definitions using evaluation functionals $\delta_x f =
   f(x)$ see Stienward/Christmann (Support Vector Machines.)
** Lemma
   The RKHS uniquely determines the reproducing kernel $k$
*** Proof
    Assume $k_1$ and $k_2$ are reproducing kernels, then $\langle h_1(\cdot, x),
    k(\cdot, x') \rangle$ The reproducing property of Kernel $k_2$.

    We may now swap the two arguments, $\langle k_2(\cdot, x'), k_1(\cdot, x)
    \rangle = k_2(x', x) = k_2(x, x')$
** Mercer Feature space
   It is possible to derive an explicit representation of the feature space
   where features are (possibly infinite-dimensional) vectors (unlike the
   canonical feature map, where we assign functions to all $x \in X$.) The
   resulting Mercer representation is helpful for understanding RKHS.

   Let $X$ be compact and $K$ a continuous kernel in what follows:.
** Definition
   Given a kernel $K$, the integral operator $S_k \colon L_2(\mu) \rightarrow H$
   is defined by the operator $S_k f(x) = \int h(x, x') f(x') d \mu(X')$ and
   $T_k(\mu) \rightarrow L_2(\mu)$ by $T_k = S_k^{*} S_k$.

   If you are not familiar with measures think of $d \mu(x')$ as $dx'$, i.e.,
   the standard Lebesgue measure.
* Lecture 22 Skipped <2019-01-17 Thu>
* Lecture 23 <2019-01-22 Tue>
** Example
   $X = \R^2$, then $p(x) = X_1 +\vert X_2 \vert$ is sublinear since $p(x + y) =
   X_1 + Y_1 + (X_2 + Y_2) \le X_1 + \vert X_1 \vert + \vert Y_2 \vert = p(x) +
   p(y)$ and $p(\alpha X) = \alpha X_1 + \vert \alpha X_2 \vert = \alpha X_1 +
   \alpha \vert X_2 \vert = \alpha p(x)$ for $\alpha > 0$.

   We now assume that there is such a functional $p$ that majorizes $f$ on $z$.
   We can then extend $f$ from $z$ to X$ such that the extension $\tilde{f}$ is
   still linear and majorized by on $X$. More precisely: 
** Theorem (Hahn-Banach for real vector spaces)
   Suppose $X$ is a real vector space and $p$ a sublinear functional on $X$. Let
   $f$ be a linear functional defined on a subspace $Z$ of $X$ with $f(x) \le
   p(x), \forall x \in Z$

   Then $f$ can be extended to a *linear* functional $\tilde{f}$ on $X$ such
   that $\tilde{l}(x) \le p(x)$.

   Extension here means that $\tilde{f}(x) = f(x), \forall x \in Z$
*** Proof
    The proof is lengthy. We first show that the set $E$ of all linear
    extensions $g$ of $f$ with $g(x) \le p(x)$ or the domain $D(g)$ defines a
    partial ordering. Clearly $E$ is non-empty since $f \in E$. We now need a
    relationship $\le$ on $E$, $g \le h$ if $h$ is an extension of $g$.

    Extension means that $D(g)$ is a subset of $D(h)$ ($D(g) \subset D(h)$) and
    $g(x) = h(x) \forall x \in D(g)$.

    Given any chain (totally ordered set). $C \subset E$, define $\hat{g}(x) =
    g(x), \forall x \in D(g)$, where $g \in C$. Now consider the union of all
    these extensions.

    The domain of $D(\hat{g}) = \cup_{g \in C} D(g)$ and $\hat{g}$ is linear
    (all functionals $g \in C$ are linear functionals and extensions of $f$)

    Observe that $g \le \hat{g}$ for all $g\in C$ since $D(\hat{g})$ is the
    union of all the domains of the $g \in C$.

    Thus $\hat{g}$ is an upper bound. As $C \subset E$ was an arbitrary chain,
    we can apply Zorn's lemma: this tells us that the partially ordered set has
    a maximal element, denoted by $\tilde{f}$ and $\tilde{f}(x) \le p(x)$ since
    all $g \in E$ are majorized by $p$ by definition.

    We now have to show that $D(\tilde{f}) = X$. This is shown by contradiction,
    by assuming that $D(\tilde{f}) \neq X$.

    If $D(\tilde{f}) \neq X$, we can find $y_n \in X \setminus D(\tilde{f})$.
    Let $y_1$ be the subspace of $X$ spanned by $D(\tilde{f})$ and $y_1$. Thus
    any element in this subspace can be written as $x = y + \alpha y_1$, $y \in
    D(\tilde{f})$.

    To show that the representation is unique, assume that $y + \alpha y_1 =
    \tilde{y} + \beta y_1, y, \tilde{y} \in D(\hat{f})$.

    Rewritten as $(y - \tilde{y}) = (\beta - \alpha)y_1$, this implies $y =
    \tilde{y}$ and $\alpha = \beta$. $0$ is the only element $D(\tilde{f})$ and
    span of $\{y_1\}$ have in common.

    Define a functional on $y_1$ by $g_1(y + \alpha y_1) = \tilde{f}(y) + \alpha
    c$. For a specific $c \in \R$, this functional extends $f$, which would
    contradict the Zorn's lemma, since an extension is possible and Zorn's lemma
    told us that this is the maximal element.

    $g_1(C_n(y + \alpha y_1) + c_2(\tilde{y} + \beta y_1)) = g_1((c_1y + c_2
    \tilde{y}) + (c_1 \alpha + c_2 \beta) \cdot y_1)$. This is equal to
    $\tilde{f}(c_1 y + c_2 \hat{y}) + (c_1 \alpha + c_2 \beta) \cdot c$.

    Now, we use the linearity of $\tilde{f}$ and write as $c_1 \tilde{f}(y) +
    c_2 \hat{f}(\tilde{y}) + c c_1\alpha + c c_2 \beta = c_1 g_1(y + \alpha
    y_1) + c_2g_1(\tilde{y} + \beta y_1)$. Now, we showed that the functional is
    linear.

    Additionally, $g_1(y) = \tilde{f}(y)$ for $\alpha = 0$.

    That is $y_1$ is an extension of $\tilde{f}$ and contradicts the maximality
    of $\tilde{f}$, provided that we can find $c$ such that $g_1(x) \le p(x)$.
    We haven't shown this part, we'll skip it since it's very technical. One can
    look at Kreizig's book.
** Theorem (Hahn-Banach generalized)
   Given a real or complex vector space and a real valued function $p$ with
   $p(x + y) \le p(x) + p(y)$ and and $p(\alpha x) = \vert \alpha \vert \cdot
   p(x)$. Let $f$ be again a linear functional defined on a subspace $z$ of $X$
   with $\vert f(x) \vert \le p(x)$. Then there exists a linear extension
   $\tilde{f}$ on $X$ such that $\vert \tilde{f}(x) \vert \le p(x)$, for all $x
   \in X$.
** Theorem (Hahn-Banach theorem for normed spaces)
   Suppose $f$ defined on a subspace $z$ of the normed space $X$ is a bounded
   linear functional. Then there is a bounded linear functional $\tilde{f}$ on
   the entire space $X$ that extends $f$ and $\Vert \tilde{f} \Vert_X = \Vert f
   \Vert_z$

   Here $\Vert \tilde{f} \Vert_X = \sup_{x \in X, \Vert X \Vert = 1} \vert
   \tilde{f}(x) \vert$, $\Vert f \Vert_z = \sup_{x \in z, \Vert x \Vert = 1}
   \vert f(x) \vert$.
*** Proof
    Assume $Z \neq \{0\}$ (otherwise $f = 0$ and $\tilde{f} = 0$.) We now define
    a sublinear functional, $p(x) = \Vert f \Vert_z \cdot \Vert X \Vert$ and
    show that $p(x + y) \le p(x) + p(y)$, and $p(\alpha x) = \vert \alpha \vert
    p(x)$ to apply Theorem 5.8 (The generalized Hahn-Banach theorem)

    $p(x + y) = \Vert f \Vert_z \cdot \Vert x + y\Vert \le \Vert f \Vert_z \Vert
    X \Vert + \Vert f \Vert_z \cdot \Vert y \Vert = p(x) + p(y)$

    $p(\alpha x) = \Vert f \Vert_z \cdot \Vert \alpha X \Vert = \Vert \alpha
    \Vert\cdot \Vert f \Vert_Z \cdot \Vert x \Vert = \vert \alpha \vert p(x)$.

    Since $\vert f(x) \vert \le \Vert f\Vert_z \cdot \Vert X \Vert = p(x)$, $f$
    is majorized by $p$ on $z$.

    Thus, there is $\tilde{f}$ on $X$ with $\vert \tilde{f}(x) \vert \le p(x)$.

    Now, $\Vert \tilde{f} \Vert_X = \sup_{x \in X, \Vert X \Vert = 1} \vert
    \hat{f}(x) \vert \le \sup_{x \in X, \Vert x \Vert = 1} \vert p(x) \vert =
    \sup_{x \in X, \Vert x \Vert = 1} \Vert f \Vert_z \cdot \Vert X \Vert =
    \Vert f \Vert_Z$.

    But $Z$ is as subset of $X$ so $\Vert f \Vert_Z \le \Vert \tilde{f} \Vert_x
    \implies \Vert \tilde{f} \Vert_X = \Vert f \Vert_z$.
** Theorem
   Given arbitrary $x_0 \in X$, where $X$ is a normed space, there exits a
   bounded linear functional $\tilde{f}$ on $X$ with $\Vert \tilde{f}\Vert = 1$,
   and $\tilde{f}(x_0) = \Vert X_0 \Vert$.
*** Proof
    Define $z = \span\{x_0\}$ and $f(x) = f(\alpha x_0) = \alpha \Vert X_0
    \Vert$, Then $f$ is obviously a bounded space.

    $\vert f(x) \vert = \vert f(\alpha x_0) \vert = \vert \alpha \vert \Vert X_0
    \Vert = \Vert \alpha X_0 \Vert = \Vert X \Vert$.

    So that $\Vert f \Vert = \sup \frac{\vert f(x) \vert}{\Vert X \Vert} =
    \frac{\Vert \lambda \Vert}{\Vert X \Vert} = 1$. Using Theorem 5.9
    (Hahn-Banach for normed spaces) $f$ can be extended to the entire $X$ such
    that $\Vert f \Vert_z = \Vert \tilde{f} \Vert_X = 1$. Since $\tilde{f} = f$,
    we obtain $\tilde{f}(x_0)= \Vert X_0\Vert$.[fn:20]
** Geometric version
   There are also geometric versions of Hahn-Banach theorems which we will
   consider now. The goal is to separate convex sets in a normed space by
   bounded linear functionals.

   First we need another version of Hahn-Banach: 
** Theorem
   Let $Z$ be again a subspace of the complex vector space $X$ and $p \colon X
   \rightarrow \R$ sublinear. Assume $f \colon Z \rightarrow \C$ is linear with
   $\operatorname{Re} f(x) \le p(x), \forall x \in Z$. Then there exists
   $\tilde{f} \colon X \rightarrow \C$ that extends $f$ with $\operatorname{Re}
   \tilde{f}(x) \le p(x)$, for all $x \in X$.

   *Reminder*: $U$ is called convex if for all $x, y \in U$, also $\lambda x +
    (1 - \lambda) y \in U, \forall \lambda \in [0, 1]$

    The separation problem is defined as follows: Assume $U$ and $V$ are two
    convex sets, we can find a functional $x' \in X', x' \neq 0$, with $\sup_{x
    \in U} X'(x) \le \inf_{x \in V} X'(x)$ for $K = \R$, or $\sup_{x \in U}
    \operatorname{Re} x'(x) \le \inf_{x \in V} \operatorname{Re} x'(x)$ for $K =
    \mathbb{C}$.

    A picture was drawn.

    #+BEGIN_SRC artist
                                     |        --/-------\--
                   -------           |       /             \
                 -/       \-         |       |             |
                /           \        |       \     V       /
               /       U     \       |        --\       /--
               |             |       |           -------
               \             /       |
                \           /        |
                 -\       /-         |
                   -------           |
                                     |
                                     |
    #+END_SRC
    
    Here $U$ and $V$ are convex sets $U$ and $V$ and we need a separating line
    that separates two convex sets.
* Lecture 24 <2019-01-24 Thu>
** An example about a question
   $X = \R^2$ and $p(x) = x_1 + x_2$. sublinear.

   Let $f$ on $\span \{(1, 0)^T\}$ be defined by $f(x) = x_1$.

   Define $g((\lambda_,0)^t + \alpha (0, 1)^t) = f((x_n, 0)^t) = x_1$, i.e., $c
   = 0$ as suggested.

   But now $g_1(x) \le p(x), \forall x \in X$. Take $x = (1, -1)^t$, then
   $g_1(x)= 1, p(x) = 0$. A contradiction.
** Minkowski Functional
   Let $X$ be a vector space and $A \subset X$ a subset. We define the Minkowski
   functional by $p_A(x) = \inf \{l > 0 \colon \frac{1}{l} x \in A\}$.

   The set $A$ is called absorbing if $p_A(x) < \infty \forall x \in X$.
** Example
   Consider $\R^2$ and $A = \{x, \vert x \vert < 1\}$.

   #+BEGIN_SRC artist
                  ---+---          /-
               --/   |   \--   /---
              /      |      X--
             /       |  /--- \
     --------+-------+-------+----------
             \       |       /
              \      |      /
               --\   |   /--
                  ---+---
                     |
                     |
                     |
   #+END_SRC

   Given any $x \in \R^2$, we write $x = \Vert x \Vert \cdot x_0$ where $\Vert
   x_0 \Vert = 1$, thus $x_0 \in A$. $p_A(x) = \inf \{\lambda > 0 \vert
   \frac{1}{\lambda}x \in A\} = \Vert x \Vert$. Since for $l = \Vert x \Vert$,
   $\frac{1}{\Vert x \Vert} \Vert x \Vert$, $x_0 \notin A$.

   Kind of easy to see.

   For $\lambda < \Vert x \Vert$, $\vert \frac{1}{\lambda} x \vert >
   \frac{1}{\Vert x \Vert} \cdot \Vert x \Vert = 1$. Thus $\frac{1}{\lambda}
   \notin A$.
** Lemma 5.14
   Given a normed space $X$ and $U \subset X$ convex. Assume $0$ is an interior
   point of $U$, then 
   1. $U$ is absorbing, more precisely, if $\{x \colon \Vert x \Vert < \epsilon
      \} \subset U$, then $p_u(x) \le \frac{1}{\varepsilon} \Vert x \Vert$.
   2. $p_n$ is sublinear.
   3. If $U$ is open, then $U = p_u^{-1}([0, 1))$.
*** Proof
    Note that $0$ must really be contained in $U$.

    1. Since $0$ is an interior point, there exists a neighbourhood of $0$ in $U$.
       We can then proceed as in the example above.
    2. $p_n(lx) = lp_n(x)$, for $l > 0$, clear from the definition, need to show
       that $p_n(x + y) \le p_n(x) + p_n(y)$. For arbitrarily $\varepsilon > 0$,
       choose $l$ and $\mu$ such that $l \le p_n(x) + \varepsilon$, $\mu \le
       p_n(y) + \varepsilon$ and $\frac{1}{l} x \in U$, $\frac{1}{\mu} y \in U$,
       $U$ is convex so that $\alpha \frac{1}{l} x + (1- \alpha) \frac{1}{\mu} y
       \in U$.

       $\frac{1}{1 + \mu} x + \frac{1}{1 + \mu} y = \frac{1}{1 + \mu}(x + y)$,
       now $\frac{1}{\lambda + \mu} (x + y) \in U$.

       $p_u(x + y)$ must be smaller than $l + \mu$ (since $p_n$ is the infimum),
       i.e., $p_u(x +y) \le l + \mu \le p_u(x) + \varepsilon + p_u(y) +
       \varepsilon$, but $\varepsilon > 0$ was arbitrary so that $p_u$ is indeed
       sublinear.
    3. $p_u(x) < 1$ implies that $\exists l < 1$ with $\frac{1}{l} x \in U$.
       Since $U$ is convex and $0 \in U$. $\lambda \frac{1}{\lambda} x + (1- l)
       0 \in U \implies x \in U$.

       $p_u(x) \ge 1$ implies $\exists l < 1 \colon \frac{1}{\lambda} x\in
       U^{c}$ (the complement of $U$)

       Now since $U$ is by definition open, $U^{c}$ is closed and it follows
       that $\lim_{l < 1, \lambda \rightarrow 1} \frac{1}{\lambda} x \in U^c$.
** Lemma 5.15
   Let $X$ be a normed space and $V \subset X$ convex and open with $0 \notin
   V$. Then there exists a linear functional $x' \in X'$ with the property that
   $\operatorname{Re}(x'(x)) < 0, \forall x \in V$
*** Proof
    We show only the real valued case. In what follows, we use the notation $A
    \pm B = \{a \pm b \colon a \in A, b \in B\}$

    Note that $A + B$ and $A - B$ are convex.

    Assume $c_i = a_i + b_i$, $i = 1, 2$, then $\lambda c_1 + (1-\lambda)c_2 =
    \lambda a_1 + (1-\lambda)a_2 + \lambda b_1 + (1-\lambda)b_2 \in A + B$. One
    can do the same for $A - B$.

    Now we can start with the proof: Choose arbitrary $x_0 \in V$, define $y_0 =
    -x_0$ and $U = V - \{x_0\}$. Since $V$ is open, so is $U$ (the same set,
    just moved by $x_0$)

    Furthermore, $U$ is convex (one-point sets are convex and the "difference"
    of two convex sets is convex.)

    It holds that $y_0 \in U = \{v - x_0, v \in V\}$ since otherwise $v = 0 \in
    V$.

    But $0 \in U$, choose $v = x_0$. Consider now $p_u(x) = \inf \{\lambda
    \colon \frac{1}{\lambda} x \in U\}$. According to Lemma 5.14, $p_n$ is a
    real-valued sublinear functional. Since $y_0 \in U$, $p_u(y_0) \ge 1$.
    Define $y = \span \{y_0\}$ and the functional $y'(ty_0) = t p_u(y_0)$.

    For $t \le 0$, $y'(ty_0) \le 0 \le p_u(ty_0)$.

    $t > 0 \colon y'(ty_0) = p_u(t, y_0)$. We combine these and get $y'(ty_0)
    \le p_u(t y_0)$.

    Due to the Hahn-Banach theorem, there exists $x' \in X'$ that extends $y'$
    and $x'(x) \le p_u(x)$.

    $x'$ is continuous with lemma 5.14,
    1. $\vert x'(x) \vert = \max\{x'(x), x'(-x)\} \le \max \{p_u(x), p_u(-x)\}
       \le \frac{1}{\varepsilon} \Vert x \Vert$.

    Now, $x'(y_0) = p_u(y_0) \ge 1$. Any $x \in V$ can be written as $x = u +
    x_0 = u - y_0$ and $x'$ is linear, thus, $x'(x) = x'(u) - x'(y_0) < 0.$ This
    is $\le p_u(u)$. $\in [0, 1]$ See Lemma 5.14 (iii)
** Theorem
   (Hahn-Banach separation version) Let $X$ be a normed space and $V_1, V_2
   \subset X$ convex. Moreover, assume $V_1$ is open and $V_1 \cap V_2 =
   \emptyset$. Then there exists $x' \in X'$ such that $\operatorname{Re}
   x'(v_1) < \operatorname{Re} x'(v_2) \forall v_1 \in V_1, v_2 \in V_2$.
*** Proof
    Define $V= V_1 - V_2$ and apply the previous theorem.
** Theorem 5.17 (Hahn-Banach separation version)
   Let $X$ be a normed space $V \subset X$ closed and convex. For $X \notin V$,
   there exists $x' \in X$ with $\Re x'(x) < \inf \{\Re x'(v) \vert v \in V\}$.
** Theorem (Uniform boundedness theorem)
   Unlike the Hahn-Banach theorem, the next fundamental theorems require
   completeness. The uniform boundedness theorem will be derived using the
   so-called Baire's category theorem.
** Definition (Category)
   We call a subset $M$ of a metric space $X$:
   1. *rare* (nowhere dense) in $X$ if the closure $\bar{M}$ has no interior
      points.
   2. *meager* (of first category) in $X$ if $M$ is the union of countably many
      rare sets.
   3. *nonmeager* (of second category) in $X$ if $M$ is not meager in $X$.

   First category means "small", e.g., $\Z$ is meager in $\R$. To see this,
   first note that $\Z$ is closed in $\R$ since $\R \setminus \Z = \cup_{n \in
   \Z} (n, n+1)$ and hence $\Z$ is closed.

   Consequently $\Z$ is closed. Now $\Z$ can be written as the union of
   countably many sets $\Z = \cup_{n \in Z} \{n\}$. Each set $\{n\}$ is nowhere
   dense in $\R$ because $\overline{\{n\}} = \{n\}$ and the interior of $\{n\} =
   \emptyset$. In a similar way, it can be shown that $Q$ is meager in $\R$.
* Lecture 25
** Baire Category theorem
   Something about pointwise convergence implying the global.

   Let $X \neq \emptyset$ be a complete metric space, then $X$ is non-meager (of
   second category) in itself.
   
   Thus for complete $X$ with $X = \cup_{n = 1}^{\infty} A_k$ where $A_k$ is
   closed, at least one $A_k$ must contain a non-empty open subset.

   As a result, $\R$ is of second category. (This is also clear before, but can
   now be deduced from the theorem), while $\N, \Z, and \Q$ are not of second
   category.

   We can now derive the *uniform boundedness theorem* which states that if a
   sequence of bounded linear operators $T_k \colon X \rightarrow Y$, $X$
   complete converges at every point $x \in X$, then the sequence is uniformly
   bounded.
** Theorem (Uniform boundedness theorem)
   Given a sequence of linear bounded operators $T_k \colon X \rightarrow Y$
   from Banach space $X$ into a normed space $Y$ such that $(\Vert T_k X
   \Vert)_k$ is bounded for all $x \in X$, i.e., $\Vert T_k X \Vert \le C_x$ for
   all $k$, then the sequence of the norms $(\Vert T_n \Vert)_n$ is bounded.
   That is, $\exists c \colon \Vert T_n \Vert \le c$ for all $n$. ("point wise
   boundedness implies uniform boundedness".)
*** Proof
    If we write the space $X = \cup_{k = 1}^{\infty} A_k$ at least one $A_k$
    contains a non-empty open set. For any $k\in N$, define $A_k = \{X \colon
    \Vert T_n x \Vert \le k, \forall n\}$.

    The sets $A_n$ are closed (use continuity of operators $T_k$ and continuity
    of the norm.) Since $\Vert T_n X \Vert \le c_x$, $\forall x \in X$, any $x$
    belongs to one of the sets $A_k$ and $X = \cup_{n = 1}^{\infty} A_k$.

    $X$ is by assumption complete, thus due to Baire's category theorem at least
    one of the sets $A_k$ contains an open ball, i.e., $\exists k_0 \in \N, x_0
    \in X, r > 0 \colon B_o = B(x_0, r) \in A_{k_0}$.

    Let $x \neq 0$ be arbitrary, define $z = x_0 + rx$, where $r = \frac{r}{2
    \Vert x \Vert}$, then $\Vert z - x_0 \Vert = \Vert r x\Vert = \Vert
    \frac{r}{2 \Vert x \Vert} \cdot x \Vert < r$. It follows that $z \in B_o$
    and that thus $\Vert T_k z \Vert \le k_0, \forall n \in \N$.

    By definition, also $x_0 \in B_0$, and $\Vert T_n x_0 \Vert \le n_0 \forall
    n \in \N$. We can write $x$ as $x = \frac{1}{r}(z - x_0)$, then $$\Vert T_n
    X \Vert = \Vert T_n (\frac{1}{r}(z - x_0)) \Vert = \frac{1}{r} \Vert T_n z -
    T_n x_0 \Vert < \frac{1}{\gamma}\left(\Vert T_n z \Vert + \Vert T_n x_0
    \Vert \right)\le \frac{2 \Vert x \Vert}{\gamma} \cdot 2 l_0 = 4 \frac{\Vert
    x \Vert}{r} k_0$$

    For all $n$, $\Vert T_n Vert = \sup_{\Vert x\Vert = 1} \Vert T_n x \Vert =
    \frac{4}{r}k_0 \equiv c$ We just showed that the sequence of operators is
    bounded by $c$.
** Example
   Suppose $1 < p < \infty$ and $\frac{1}{p} + \frac{1}{q} = 1$, as usual. If
   the sequence $\alpha_n$ is such that $\sum_{i = 1}^{\infty} \alpha_i x_i$
   converges for all $x \in l^p$, then $\alpha = (\alpha_i)_{i} \in l^q$.
*** Proof
    We define a $f_n(x) = \sum_{i =1}^{\infty} \alpha_i x_i$ and $f(x) = \sum_{i
    =1}^{\infty} \alpha_i x_i$. Both $f_n$ and $f$ are linear and using Holder's
    inequality $\vert f_n(x) \vert \le (\vert \alpha_i \vert^q)^{1/q} (\sum
    \vert x_i \vert)^{1/p} \le (\sum \vert \alpha_i\vert^q)^{1/q} \Vert x
    \Vert_q$

    We see that $f_n$ is bounded for every $x \in l^p$ with the previous
    theorem, it follows that $$\vert f(x) \vert = \vert \sum_{i = 1}^{\infty}
    \alpha_i x_i \vert \le \Vert f \Vert \left(\sum_{i=1}^{\infty} \vert x_i
    \vert^p \right)^{1/p}$$

    Note that $(f_n)_n$ converges pointwise to $f$.

    We now define $x_i = \bar{\alpha}_i \cdot \vert \alpha_i \vert^{q-2}$ if $1
    \le i \le n$ and $\alpha_i \neq 0$ and $0$ otherwise. For any fixed $n$,
    then $(x_i)_i \in l^p$ (only finitely many entries $\neq 0$) and $\vert x_i
    \vert^p = (\vert \bar{x_i} \vert \cdot \vert \alpha_i \vert^{q-2})^p = \vert
    \alpha_i \vert^{p(q-1)} = \vert \alpha_i \vert^{q} = \alpha_i x_i$ for $i =
    1, \cdots, n$.

    First $\frac{1}{p} + \frac{1}{q} = 1 \implies p(q-1) = q$, second, $\alpha_i
    x_i = \alpha_i \overline{\alpha}_i \vert \alpha_i \vert^{q-2} = \vert
    \alpha_i\vert^q$

    Thus, $$(\sum_{i = 1}^{n} \vert \alpha_i \vert^q)^{1/p}(\sum_{i=1}^{n} \vert
    \alpha_i \vert^{1})^{1/p} = \sum_{i=1}^{n} \vert \alpha_i \vert ^ q =
    \sum_{i=1}^{n} \aplha_i x_i \le \Vert f \Vert (\sum_{i=1}^{q} \vert x_i
    \vert)^{1/p} = \Vert f \Vert \left(\sum \vert \alpha_i \vert \right)^{1/p}$$

    It follows that $(\sum_{i=1}^{n} \vert \alpha_i \vert^q)^{1/q} \le \Vert
    f\Vert$ for any $n \in \N$ and thus $(\sum_{i=1}^{\infty} \vert \alpha_i
    \vert^q)^{1/q} < \infty$ and $\vert \alpha_i)_i \in l^q$.
** Example
   Let $p$ be the space of polynomials, write $p(x) = \sum_{i = 0}^{\infty}
   \alpha_i x^i$. (For a polynomial of degree $N_p$, $\alpha_i = 0 \forall i >
   N_p$)

   Define $\Vert p \Vert = \max_i \vert \alpha_i \vert$. Then $(P, \Vert \cdot
   \Vert)$ is not complete.
*** Proof
    We want to use the uniform boundedness theorem and thus define $f_n(0) = 0$
    and $f_n(p) = \sum_{i=0}^{n - 1}\alpha_i$, $f_n$ is a linear functional.
    Consider $p = \sum_{i=0}^{\infty} \alpha_i x^i$ and $q = \sum_{i=0}^{\infty}
    p_i x^i$, then $f_n(cp + dp) =f_n(\sum_{i = 0}^{\infty}(c\alpha_i + d
    \beta_i) x^i) = \sum_{i = 0}^{n-1}(c\alpha_i + d \beta_i) = c \sum_{i
    =0}^{n-1} \alpha_i + d \sum_{i = 0}^{n-1} \beta_i = c f_n(p) = d f_n(q)$.

    Furthermore, $\vert f_n(p) \vert = \vert \sum_{i=0}^{n-1} \alpha_i\vert \le
    n \cdot \max \vert \alpha_i \vert = n \cdot \Vert p \Vert$. We now fix $p$
    and analyse $(f_n(p))_n$. Let $N_p$ denote again the degree of $p$, then at
    most $N_p + 1$ coefficients are nonzero so that, $\vert f_n(p) \vert \le
    (N_p + 1) \Vert p \Vert$

    We have shown pointwise boundedness and want to prove that $\Vert f_n
    \Vert_n$ is not bounded. We select $p_n(x) = \sum x^i$. Thus, $\Vert p_n
    \Vert = 1$ and $f_n(p) = n$, but $\Vert f_n \Vert = \sup_{\Vert p \Vert = 1}
    \vert f_n(np) \vert \ge n$. $(\Vert f_n \Vert)_n$ is unbounded. Using the
    uniform boundedness theorem, $p$ cannot be complete,
** Open mapping theorem
   We can now approach the third fundamental theorem. We start with a definition.
** Definition (Open map)
   A mapping between metric spaces is called open if it maps open sets to open
   sets.
** Example
   Consider $x \mapsto \sin(x)$, then $(0, 2\pi)$ is mapped to $[0, 1]$, i.e.,
   non-open set.

   Let $f \colon \R^2 \rightarrow \R$, $(x, y) \mapsto x$ is open if it is a
   projection onto the first coordinate. The set, $A = \{(x, y) \vert x \ge 0, x
   \cdot y \ge 1\}$ is closed.

   (Note that the sets such as $[0, \infty)$ are closed, every sequence of
   non-negative numbers that converges has a non-negative limit in $[0,
   \infty)$. Another way to look at $(\infty, \infty)$ is clearly open.)

   $f(A) = (0, \infty)$ and thus open.
* Lecture 26
** Repetition
   1. Continuous map: open set set inverted to open. Similarly for closed
   2. Image of a compact set is compact
   3. continuous function maps Connected sets to connected set.
   4. Definition of disconnected. We can write $X$ and $A \cup B$ where $A$ and
      $B$ are open and they are disjoint.
   5. Some counter examples in the other direction.
      1. Preimages of connected sets need to not be connected. $f(x) = \vert x
         \vert$, then $f^{-1}([1, 2]) = [-2, -1] \cup [1, 2]$.
      2. $f(x) = \frac{1}{1 + x^2}$
** Theorem
   Let $X, Y$ be normed spaces and $T \colon X \rightarrow Y$ be a linear
   mapping. Then the following statements are equivalent.
   
   1. $T$ is open.
   2. $T$ maps open balls around $0$ to neighbourhoods of $0$, to be more
      precise. $U_r = \{x \in X \colon \Vert x \Vert < r\}$ and $V_\varepsilon =
      \{y \in Y \colon \Vert y \Vert < \varepsilon\}$
   3. There was something that I missed.
** Example
   Let $c_0 = \{(x_n)_n \colon x_n \in K \textup{ and } \lim_{n \rightarrow
   \infty} x_n = 0\}$ (set of all sequences converging to $0$) We define a
   mapping $T \colon l^\infty \rightarrow c_0$ by $x_n = \frac{1}{n} x_n$. Since
   every sequence in $l^\infty$ is bounded, $(\frac{1}{n} x_n)_n$ converges to
   zero. Consider $U_1 = \{(x_n)_n \in l^\infty \colon \Vert x_n \Vert < 1\}$

   $T(U_1) = \{(\frac{1}{n} x_n)_n \in c_0 \colon \Vert x \Vert_\infty < 1\}$
   This is same as $\{(x_n) \in c_0 \colon \vert x_n \vert < \frac{1}{n} \forall
   n \in \N\}$

   $V_\varepsilon = \{(y_n)_n \in c_0 \colon \Vert (y_n)\Vert < \varepsilon \}$

   For all $\varepsilon > 0$ we can find a sequence $V_\varepsilon$ that is not
   in $T(U_n)$. It follows that $T$ cannot be open. An open linear mapping is
   surjective. This can be seen as follows:

   Take any $y \in Y$. We have that $V_\varepsilon \subset T(U_1)$. We now
   define $\tilde{y} = \frac{\varepsilon}{2 \Vert y \Vert} \cdot y$ So that
   $\tilde{y} \in V_\varepsilon$. Thus, there exists an $x \in U_1$ with $T(x) =
   \tilde{y}$. Then $T(2 \frac{\Vert y \Vert}{\varepsilon} x}$ I missed the rest
   of the proof.
** Corollory
   Given Banach spaces $X$ And $Y$ and a bijective continuous linear operator $T
   \colon X \rightarrow Y$, then the inverse operator $T^{-1}$ is continuous.
** Corollory
   Let $\Vert \cdot \Vert_1$ and $\Vert \cdot \Vert_2$ be norms on $X$ such that
   $(X, \Vert \cdot \Vert)$ and $(X, \Vert \cdot \Vert_2)$ are complete, then if
   $\Vert \cdot \Vert_2 \le c \cdot \Vert \cdot \Vert_1$ for some $c > 0$, then
   also $\Vert \cdot \Vert_1 \le c' \Vert \cdot \Vert_2$ for some $c' > 0$.
* Lecture 27
  Mock exam
* Lecture 28 <2019-02-05 Tue>
** Corollary
   Let $\Vert \cdot \Vert_1$ and $\Vert \cdot \Vert_2$ be norms of $X$ such that
   $(X, \Vert \cdot \Vert_1)$ and $(X, \Vert \cdot \Vert_2)$ are complete. Then
   if $\Vert \cdot \Vert_2 \le c \cdot \Vert \cdot \Vert_1$ for some $c > 0$,
   then also $\Vert \cdot \Vert_1 \le c' \Vert \cdot \Vert_2$ for some $c' > 0$.
*** Proof
    Define $T$ to be the identity map $(X, \Vert \cdot \Vert_1) \mapsto (X,
    \Vert \cdot \Vert_2)$. Then $T$ is of course bounded. Hence the inverse
    $T^{-1}$ is also bounded, i.e., there exists a constant $c'$ such that the
    inverse $\Vert T^{-1} x\Vert_1 = \Vert x \Vert_1 \le c' \Vert x \Vert_2$

    This relies on the fact that the two norms are complete. Otherwise this need
    not be true.
** Example
   The following example illustrates that completeness is indeed required.

   Consider the space $C^{1}[0, 1]$ the space of continuously differentiable
   spaces on the interval $[0, 1]$ with the norms $\Vert \cdot \Vert_1 = \max_{x
   \in [0, 1]} \vert f(x) \vert$ and $\Vert f\Vert_2 = \Vert f \Vert_1+ \Vert
   f'\Vert_1$

   Now $\Vert f \Vert_1 \le \Vert f \Vert_2$, i.e., $c =1$ in the corollary
   before. But the norms are not equivalent. For $f_n(x) = \frac{1}{n} \sin
   (nx)$. These functions are continuously differentiable. Now $\Vert f_n
   \Vert_2 = \Vert f_n \Vert_1 + \Vert f_n'\Vert \rightarrow 1$ and not zero.

   It follows that $C^{-1}[0, 1]$ cannot be a Banach space for both norms.

   Here the the one norm is complete, but on the other hand, the two norm is
   complete.

   To see that the one norm is complete, construct a function that is not
   differentiable, i.e., $f(x) = \vert x - \frac{1}{2}\vert$. By the Weistrass
   approximation theorem, there exists a sequence of polynomials converging to
   $f$. All these polynomials are differentiable, but the limit is not. The
   completion would give use $C^{0}[0, 1]$, the space of all continuous
   functions.

   *This completes the fundamental theorem chapter*
** Spectral theory
   Eigenvalues and eigenvectors of matrices play an important role in the
   analysis of stability etc. We will now extend this framework to operators on
   normed and also in particular, Hilbert spaces.

   Reminder: Let $X$ and $Y$ be normed spaces. An operator $T \colon X
   \rightarrow Y$ is called compact linear operator if $T$ is linear and maps
   bounded subsets $M \subset X$ to relatively compact sets $T(M)$, i.e.
   $\overline{T(M)}$ is compact.

   Some properties:
   1. Every compact linear operator $T$ is bounded and thus continuous (consider
      $U = \{x \in X \vert \Vert x \Vert = 1\}$, $U$ is bounded and since $T$ is
      by definition compact, $\overlin{T(U)}$ is compact and hence bounded. 
   2. $T$ is compact if and only if it maps every bounded sequence $x_n$ in $X$
      onto a sequence $(Tx_n)$ in $Y$ which has a convergent subsequence.
   3. The compact linear operators from $X$ to $Y$ form a vector space, i.e.,
      $T_1 + T_2$ is compact if $T_1, T_2$ are compact.
** Definition
   Let $X$ be a normed space $X$ and $T \colon X \rightarrow X$ an operator. We
   call $X \neq 0$ with $Tx = \lambda x$ eigenvector (or eigenfunction) if $X$
   is a function space and $\lambda$ the corresponding eigenvalue.
** Example
   In some example long time ago, we computed the eigenfunctions of the Koopman
   operator given by $Kf = f \circ \phi$ for $\phi \colon \R^2 \rightarrow
   \R^2$, $\phi(x)=$

   | $\lambda x_1$                            |
   | $\mu x_2 + (\lambda^2 - \mu)\lambda_1^2$ |

   $e_1 = x_1, \varphi_2 = x_1^2, e_3 = x_2 - x_1^2$ are eigenfunctions
   corresponding to $l_1 = l, l_2 = \lamda^2, \lambda_3 = \mu$.

   $Tx = \lambda x$ implies that $tx - \lambda x = (T - \lambda Id) x = 0$.
   Thus, the operator $T - \lambda Id$ is not injective. Furthermore, for an
   eigenvalue $\lambda$ and eigenvector $x$, we obtain,$\Vert T x \Vert = \Vert
   \lambda x \Vert = \vert \lambda \vert \cdot \Vert x \Vert \implies \vert l
   \vert < \Vert T \Vert$.

   For an operator on a finite-dimensional space injectivity and subjectivity
   are equivalent. For inifinite dimensional operator, this is in general not
   the case, e.g., $l^2$ with the right shift operator $R$ is injective and not
   surjective. On the other hand, the left-shift operator: $(x_1, \x_2, \cdots)
   \mapsto (x_2, x_3, \cdots)$ surjective/not injective.

   Eigenvalues and eigenvectors of the right-shift operator.

   $Rx = (0, x_1, x_2, \cdots,) = \lambda(x_1, x_2, \cdots)$

   Thus $0 = \lambda x_1$, either $\lambda = 0$ or $x_1 = 0$. If $\lambda = 0
   \implies x = 0$ which is not a nonzero vector. If $x_1 = 0$, then $x_1 =
   \lambda x_2$. Thus $x = 0$, $R$ has no eigenvalues.

   Eigenvalues and eigenvectors of the left-shift operator. $Lx = (x_1, x_3,
   \cdots) = \lambda(x_1, x_2, \cdots)$ Thus $x_2 = \lambda x_1$, $x_3 = \lambda
   x_2$ and so on. Without loss of generality, we can say that $x_1$ is $1$, and
   eigenvectors are of the form $(1, \lambda, \lamba^2, \cdots)$ Now $\Vert x
   \Vert_2 = \sum_{n = 0}^{\infty} \vert \lambda^k \vert^2 < \infty$ for $\vert
   \lambda\vert < 1$

   The shift operator are not compact. (We can see this by taking a sequence of
   unit vectors.)

   Thus, we study not only eigenvalues, but the more general spectrum defined by
   $\sigma (T) = \{ \lambda \vert \lambda_{Id} - T \textup{ is not
   bijective}\}$. Sometimes $\lamba_{Id} - T$, where $Id$ is the identity
   operator is written in short as $\lambda - T$.

   The spectrum of the right shift operator is $\sigma(R) = \{\vert \lambda
   \vert \le 1\}$ For $\lambda = 0$, $-T$ is not bijective, take $y = (1, 0, 0,
   \cdots)$, then $y$ is not the image of a point $x \in X$. Assume $\vert
   lambda \vert \le 1, \lambda \neq 0$. Suppose there is $x \in X$ with
   $(\lambda_{Id} - \R)x = y$

   $\lambda x - Rx = (\lambda x_1, \lambda x_2 - x_1, \cdots) = (1, 0, 0,
   \cdots)$ This implies that $x_1 = \frac1\lambda = \frac{1}{\lambda^2},
   \cdots$ not in $l^2$ for $\vert \lambda \vert \le 1$.

   If $T$ is bijective and $X$ complete, then the inverse $T^{-1}$ is
   automatically continuous. Given continuous linear operator $T \colon X
   \rightarrow X$ on a Banach space $X$, we define $T^{0} = Id$ and $T^n = T
   \circl \cdots \circ T$ ($n$ times)
** Theorem
   Let $T$ be as above and $\Vert T \Vert \le 1$. Then $\sum_{n=0}^{\infty} T^n$
   converges and $Id - T$ is invertible with $(id - T)^{-1} =
   \sum_{n=0}^{\infty} T^n$ Furthermore $\Vert (Id - T)^{-1}\Vert \le (1 - \Vert
   T \Vert)^{-1}$.
*** Proof
    We define $S_n = \sum_{n = 0}^{n} T^k$, then for $n > m$, we obtain $\Vert
    S_n - S_m \Vert = \Vert \sum_{k = m+1}^{n} T^k\Vert \le \sum_{k = m+1}^{n}
    \Vert T^k\Vert \le \sum_{k=m+1}^{n} \Vert T \Vert^k$.

    Since $\Vert T \Vert < 1$, $(S_n)_n$ is a Cauchy Sequence in the space of
    linear operators on $X$. Now, $X$ is complete and consequently the space of
    linear operators, thus $S = \sum_{k=0}^{\infty} T^k =\lim_{n \rightarrow
    \infty} S_k$ exists.
* Lecture 29 <2019-02-07 Thu>
** Proof of Von Neumann series
   $(Id - T)S_n =S_n - TS_n = S_n(Id - T) = \sum_{n=0}^{n} T^k -\sum_{k =
   1}^{n+1} T^k = Id - T^{n+1} \rightarrow Id$ as $n \rightarrow \infty$ because
   $T^n \rightarrow 0$ as $n \rightarrow \infty$.

   /Quick summary/: Thus for $n \rightarrow \infty$, $(Id - T)S = S(Id - T)$ and
   $S = \sum_{n=0}^{\infty} T^k$ is the inverse of $Id - T$.

   The norm inequality follows from $\Vert (Id - T)^{-1}\Vert = \Vert
   \sum_{n=0}^{\infty} T^k \Vert \le \sum_{n=0}^{\infty} \Vert T \Vert^k =
   \frac{1}{1 - \Vert T\Vert}$

   (See example 2.8 convergence of geometric series.)
** Lemma
   Let $X$ be a normed space and $T\colon X \rightarrow X$ a compact operator.
   Furthermore, let $\lambda \neq 0$. Then we can show that the null space of
   this operator $\lambda - T$ is finite dimensional.
*** Proof
    Define $S = \lambda - T$. Let $(x_n)_n$ be a bounded sequence in the null
    space of this operator $N(S)$. Since $T$ is compact, we know that there
    exists a convergent subsequence $(Tx_{n_k})$. As the subsequence is in
    $N(S)$, $0 = Sx_{n_k} = (\lambda - T)x_{n_k} = \lambda x_{n_k} - T x_{n_k}$,
    $x_{n_k}$ converges in $X$ and since $N(S)$ is closed in $N(S)$.

    Since every bounded sequence has a convergent subsequence, $\dim N(S)$ is
    finite.
** Remark
    Let $X$ be a normed space. The following statements are equivalent.
    1. $\dim X < \infty$
    2. The closed unit ball $B = \{x \vert \Vert x \Vert \le 1\}$ is compact.
    3. Every bounded sequence in $X$ has a convergent subsequence.
** Dimension of $N(\lambda - T)$
   The dimension of $N(\lambda - T)$ is an indicator how far away from
   injectivity the operator $\lambda - T$ is. Similarly, the lack of subjectivity is
   characterized by the co-dimension, the co-dimension of the range $R(\lambda -
   T) = \dim X/R(\lambda - T)$.

   For compact operators and $\lambda \neq 0$, $\dim N(\lambda -T) =
   \operatorname{codim} R(\lambda - T)$.
** Brief reminder: Quotient space
   Let $X$ be a vector space and $Y$ a subspace. We write $x_1 \sim x_2$ and say
   that $x_1$ and $x_2$ are equivalent if $x_1 - x_2 \in y$. This is an
   equivalence relation: $x \sim x$ (reflexivity), $x \sim y \implies y \sim x$
   (symmetry), $x \sim y$ and $y \sim z \implies x \sim z$ (transitivity)

   Let $[x]$ denote the equivalence class containing $x$. The set of all
   equivalence classes is denoted by $X/Y$ and called quotient space.

   Defining $[x_1] + [x_2] = [x_1 + x_2]$ and $\alpha[x] = [\alpha x]$, we
   obtain a vector space. The codimension of $y$ as $\operatorname{codim} = \dim
   X / Y$.
** Simple example
   $X = \R^4$ and the subspace $Y = \{(a, b, 0, 0)^T, a, b \in \R\}$ Then $x_1
   \sim x_2$ if $x_1 - x_2 \in Y$, i.e., the last two entries of $x_1$ and $x_2$
   must be identical, e.g., $(1, 2, 3, 4, 5)^t \sim (-5, 7, 3, 4)^t$
** Lemma
   Given a Hilbert space $H$ and a compact linear operator $T \colon H
   \rightarrow H. If $\lambda \neq 0$, then $R(\lambda - T)$ is closed. This
   leads to the following result
** Theorem
   Let $H$ be again a Hilbert space and $T \colon H \rightarrow H$ a compact
   linear operator and $\lambda \neq 0$, then $\dim N(\lambda - T) = \dim
   N(\bar{\lambda} - T^{*}) = \operatorname{codim} R(\lambda - T)$

   The operator $\lambda - T$ is thus surjective if and only if it is injective.
** Corollory (Fredholm alternative)
   As before, $H$ is a Hilbert space, $T$ a compact operator, $\lambda \neq 0$.
   Then either the homogeneous equation $(\lambda - T)x = \lambda x - T x = 0$
   has only the trivial solution $x = 0$ and the inhomogeneous equation $\lambda
   x - Tx = y$ has a unique solution for all $y \in H$ or there exist $n = \dim
   N(\lambda - T) < \infty$ linearly independent solutions of the homogeneous
   equation an the adjoint equation $\{-T^{*}\} = 0$ has exactly $n$ linearly
   independent solutions; in this case, the inhomogeneous equation is solvable
   if $y \in (N(\bar{\lambda} - T^{*}))^\perp$.

   Let us consider a "linear algebra" version of Fredholm's alternative. We
   replace $\lambda - T$ by a real matrix $A$ so that $A^{*} = A^{T}$
** Corollary
   Let $A$ be a real $m\times n$ matrix and $b \in \R^m$. There exists a
   solution $x$ of the inhomogeneous equation $Ax = b$ if and only if $b \in
   N(A^{T})^\perp$.
*** Proof
    $\Leftarrow$ Assume $b \in N(A^{t})^\perp$. $N(A^{t}) = \{x \vert A^{t} x =
    0\}$. Given now $x$ with $A^{t}x = 0$ we know that $\langle b, x\rangle =
    b^{t}x = 0$.

    Thus $x^t A = 0$ implies $x^t b = 0$, hence $x^{t}(A | b) = 0$. It follows
    that $\operatorname{rank}(A|b) = \operatorname{rank}(A)$ and there is a
    solution $x$ of the system $Ax = b$.

    $\implies$ Suppose $Ax = b$ and let $z \in N(A^{t})$, i.e., $A^{t}z = 0$,
    $\langle b, z \rangle = \langle Ax, z\rangle = \langle x, A^{t} z \rangle =
    \langle x, 0 \rangle = 0$. Thus, $b \in N(A^{t})^\perp$.
** Corollary
   Given $A \in \R^{n\times m}$. Then $A$ maps $\R^n$ to $\R^m$. $A$ is
   subjective if and only if the solution $A^{t}x = 0$ is $x = 0$.
*** Proof
    $\Leftarrow$ Suppose $x = 0$ is the only solution of $A^{t}x = 0$, then
    $N(A^{t}) = \{0\}$ and $N(A^{t}) = \R^m$ using the previous corollary, there
    exists for arbitrary $b \in \R^m$ a solution such that $Ax = b$. If we can
    solve this for any given $b$, we can find a solution, that means that $A$ is
    surjective.
    
    ($\implies$) Assume $A$ is surjective for every $b \in \R^m$, we know that
    $b \in N(A^{t})^\perp$ using the previous corollary again. Thus $A^{t}x =
    0$, then it automatically follows that $b^{t}x = 0$. This means that $x\in
    N(A^{t})$.

    If we choose one particular $b$, then we can show something. $b = x$,
    $A^{t}x = 0 and x^{t}x = 0$, this means that $x = 0$
** Example
   Consider the equation 
   1. $u = Tu + f$ where $Tu = \int_D k(\cdot, y) u(y) dy$ and $D \subset \R^n$
      is compact. We define 
   2. $v = \mu T v + g$ where $T^{*}v = \int_D \overline{k(y,\cdot)} v(y) dy$.
      For $f = 0$ or $g = 0$, we denote the corresponding equations by $2'$ and
      $3'$ respectively. We thus have $\langle Tu, v\rangle = \langle u,
      T^{*}v\rangle$, with $\langle f, g \rangle = \int_D f(x) \overline{g(x)}\
      dx$. If $2'$ has a non-trivial solution, $u_0$, then $\mu$ is called
      characteristic value of $T$ and $u_0$ is called eigenfunction. The value
      $\frac{1}{\mu} =\lambda$ is the associated eigenvalue: $u_0 = \mu T u_0
      \iff (Id - \muT) u_0 = 0 \iff (\lambda - T)u_0 = 0$. Fredholms alternative
      now tells us that (1) is solvable for any $f$ as longs as (2') admits only
      the trivial $u_0 = 0$.
* Lecture 30 <2019-02-12 Tue>
** Theorem
   Let $X$ be a Banach space and $T \colon X \rightarrow X$ a compact operator.
   Then $T$ has at most countably many eigenvalues and the eigenvalues are
   either a finite set or a null sequence.

   For self-adjoint operators, the situation is slightly easier.
** Lemma
   Let $H$ be a Hilbert space and $T \colon H \rightarrow H$ a compact operator.
   1. The eigenvalues of $T$ are real.
   2. The eigenvalues corresponding to different eigenvalues are orthogonal to
      each other.
   3. Either $\Vert T \Vert$ or $-\Vert T \Vert$ is an eigenvalue of $T$.
*** Proof
    1. Similar to the proof of symmetric matrices. Assume that $\lambda$ be an
       eigenvalue. For $x \neq 0$, we obtain $\langle Tx, x \rangle = \langle x,
       Tx\rangle = \overline{\langle Tx, x \rangle} = \overline{\lambda} \langle
       x, x \rangle = \overline{\lambda} \Vert x \Vert^2$. But then $\langle Tx,
       x \rangle = \lambda \Vert x \Vert^2$ Thus $\lambda$ must be real.
    2. Assume $Tx = \lambda x$ and $Ty = \mu y$ for $\lambda \neq \mu$, $\langle
       Tx, y \rangle = \langle x, Ty \rangle = \langle x, \mu y\rangle = \mu
       \langle x, y \rangle$ 

       $\langle \lambda x, y \rangle = \lambda \langle x, y \rangle$. It follows
       that $(\lambda - \mu) \langle x, y \rangle = 0$ and $\langle x, y \rangle
       = 0$. Here $\lambda - \mu \neq 0$.
    3. For self-adjoint linear operators, it holds that $\Vert T \Vert =
       \sup_{\Vert x \Vert \le 1} \vert \langle Tx, x\rangle \vert$.

       There exists a sequence in $\{x \vert \Vert x \Vert \le 1\}$, denoted by
       $(x_n)_n$ such that $\vert \langle Tx_n, x_n \rangle \vert \rightarrow
       \Vert T \Vert$. (Since $T$ is compact, there exists a subsequence that
       converges.)

       Now, we define a scalar, $\lambda = \lim_{n\rightarrow \infty} \langle
       Tx_n, x_n\rangle$, $y = \lim_{n \rightarrow \infty} Tx_n$, then $\Vert
       Tx_n - \lambda x_n \Vert^2 = \langle Tx_n - \lambda x_n, Tx_n - \lambda
       x_n \rangle$ Now, we expand everything and use the definitions.

       $\Vert Tx_n\Vert^2 - 2\lambda \langle Tx_n, x_n\rangle + \lambda^2 \Vert
       x_n\Vert^2 \le 2\lambda^2 - 2\lambda \langle Tx_1, x_n\rangle \rightarrow
       0$.

       Thus $y = \lim_{n \rightarrow \infty} Tx = \lim_{n \rightarrow \infty}
       \lambda x_n = \lambda \lim_{n \rightarrow \infty} x_n$ and $Ty = \lambda
       \lim_{n \rightarrow \infty} Tx_n = \lambda y\vert$, we have proven the
       claim for $y \neq 0$. For $y = 0$, $(Tx_n)$ is a null sequence and $\Vert
       T \Vert = \lim \vert \langle Tx_n, x_n \rangle\vert = 0$, thus $T = 0$.
** Theorem (Spectral decomposition of compact self-adjoint operators)
   Given a Hilbert space $H$ and a compact self-adjoint operator $T \colon H
   \rightarrow H$. Then there exist an orthonormal system $e_1, e_2, \cdots$
   (possibly fininte) and a null sequence $\lambda_1, \lambda_2, \cdots$ in $\R
   \setminus \{0\}$ such that $H = N(T) \oplus \overline{\span \{e_1, e_2,
   \cdots\}}$ and $Tx = \sum \lambda_i \langle x, e_i \rangle e_i, \forall x \in
   H$.

   (We used this in Mercer feature space representation.)
*** Proof
    It holds that $\Vert T \Vert = \sup \vert \lambda_i \vert$. The set of
    eigenvalues $\{e_1, \cdots, e_2, \cdots, \}$ can be extended to form a basis
    of $H$. This requires a basis of $N(T)$, i.e., eigenvectors corresponding to
    the eigenvalue $0$ which can be infinite dimensional (even non-separable)
** Corollary
   Let $H$ be a separable Hilbert space and $T \colon H \rightarrow H$ compact
   and self-adjoint, then $H$ admits an orthonormal basis compromising
   eigenvectors of $T$.
** Example
   $A$ is the matrix

   |  1 | -1 | 0 |
   | -1 |  1 | 0 |
   |  0 |  0 | 1 |

   Then $e_1 = (\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}, 0)^t, e_2 = (0, 0, 1)^t,
   e_3= (-\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}, 0)^t$ and $\lambda_1 = 0,
   \lambda_2 = 1, \lambda_3 = 2$ Take any $v = [x, y, z]^{t}$, then $\langle v,
   e_1 \rangle = \frac{1}{\sqrt{2}} (x + y), \langle v, e_2\rangle = z, \langle
   v, e_3 \rangle = \frac{1}{\sqrt{2}}(-x + y)$ and $v = \alpha_1 e_1 + \alpha_2
   e_2 + \alpha_3 e_3$. Hence $Av = \lambda_1 a_1 e_1 + \lambda_2 a_2 e_2 +
   \lambda_3 a_3 e_3 = \lambda_2 a_2 e_2 + \lambda_3 a_3 e_3$.

   The *second example* $H = l^2$, $T \colon H \rightarrow H$ defined $(x_1,
   x_2, \cdots, ) \mapsto (x_1, \frac{x_2}{2}, \frac{x_3}_3, \cdots$. Let
   $(e_i)_i$ be the standard basis. Then $Te_i = \frac{1}{i}e_i$ and the set of
   eigenvalues is $\{\frac{1}{i}\}_{i\in\N}$ and the corresponding
   eigenfunctions are $\{e_i\}_{i \in N}$. (Note that $\langle Tx, y \rangle =
   \sum_{i} \frac{1}{i} x_i y_i = \langle x, Ty \rangle$
   
   *Third example* Assume that $\{\varphi_{i}}_{i=1}^{N}$ forms a set of
   functions that are orthogonal to each other with respect to $L^2$. We define
   a kernel $k(x, x') = \sum_{i} \phi_i(x) \phi(x')$ and the associated
   integral operator $Tf(\lambda) = \int k(x, x') f(x')\ dx'$. We then obtain.

   $T\phi_j(x) = \int \sum_i \phi_i(x) \phi_j(x') \phi_j(x') dx' = \sum
   \phi_i(x) \int \phi_i(x) \phi_j(x') dx' = \Vert \phi_j \Vert_L^2 =
   \phi_j(x)$. Thus $\phi_j$ is an eigenfunction. To see that the operator is
   self-adjoint,

   *New example*: Let $X = [-1, 1]$, $k(x, x') = xx'$, $Tf = \int k(\cdot, x')
   f(x') dx' = x \int x' f(x')\ dx'$ 

   $f(x) = x$ is an eigenfunction: $Tf(x) = x \int_{-1}^{1} {x'x'\ dx'} = x
   [\frac{1}{3} x'^3]_{-1}^{1} = \frac{2}{3} x = \frac{2}{3} f(x)$

   Furthermore, $f(x) = 1$, $Tf(x) = x \int_{-1}^{1}x'dx' = 0$. Any $f(x)$ of
   the form $x^{2n}$, $n \in N$, this is mapped to $0$. All the even polynomials
   are eigenfunctions.

   $f(x) = x^3 - \frac{2}{3} x$ is mapped to $0 = Tf(x) = x \int x'^4 -
   \frac{3}{5} x'^2\ dx' = 0$. Note that these functions are not orthonormal yet.
   
* Homeworks
** Homework 8
*** 8.1
    A subset $M$ of a vector space is said to be convex if for two points $m_1,
    m_2$ in $M$, the line segment joining $m_1$ and $m_2$ lies in $M$, i.e.,
    $\lambda m_1 + (1-\lambda)m_2 \in M$.

    Given $n$ vectors $x_1, \cdots, x_n\in M$ and $\alpha_i \in K$ such that
    $\sum_1^n \alpha_i = 1$, we need to show that $\sum_1^n \alpha_i x_i \in M$.

    We'll prove this by induction on $n$.

    The base case $n=1$ is trivially true, since the only case is $\alpha_1 = 1$
    and $1\cdot x_1 \in M$. Now we assume that the statement is true for $n-1$
    vectors.

    The sum $\alpha_1x_1 + \cdots + \alpha_{n-1}x_{n-1} + \alpha_nx_n=
    (\sum_1^{n-1}\alpha_i)(\lambda\alpha_1 x_1 + \cdots +
    \lambda\alpha{n-1}x_{n-1}) + \alpha_n x_n$. Where $\lambda =
    \frac{1}{\sum_1^{n-1} \alpha_i}$. Notice that $\sum_1^{n-1} \lambda_i
    \alpha_i = 1$, thus we can use the inductive hypothesis to see that
    $\lambda\alpha_1 x_1 + \cdots + \lambda\alpha{n-1}x_{n-1} = y \in M$. Now,
    $(\sum_1^{n-1}\alpha_i)y + \alpha_n x_n \in M$ by convexity, since
    $\sum_1^{n-1}\alpha_i + \alpha_n = 1$.
    
    Hence by induction, the statement is true for all values of $n$.
*** 8.2
    If $y \in M$ and $x - y \in M^{\perp}$, then $\langle x - y, y \rangle = 0
    \iff \langle x, y \rangle = \langle y, y \rangle$.

    From 1, we know that $y = P_Mx$ if and only if $y \in M$ and $x - y \in
    M^\perp$,

    Let $a = P_M x$ and $b = P_m y$, then $a, b \in M$ and $x-a, y-b \in
    M^\perp$.

    $$\langle P_M x, y \rangle = \langle a, y \rangle = \langle a, y \rangle - \langle a, y - b \rangle =
    \langle a, b\rangle$$

    Similarly

    $$\langle x, P_M y \rangle = \langle b, x \rangle = \langle b, x \rangle - \langle b, x - a \rangle =
    \langle a, b\rangle$$

    Thus $\langle a, b \rangle = \langle P_M x, y \rangle = \langle x, P_M y
    \rangle = \langle a, b\rangle$.
** Announcement
   24.1 last exercise sheet
   21.1 test exam
   
   4.2 no tutorial
   12.2 last tutorial.
   14.2 exam
** Test
   No koopman operator.
* Footnotes

[fn:20] Recall that the norm is not a linear functional, if it were, then things
would be lot easier.

[fn:19] Possible typo here

[fn:18] A possible typo

[fn:17] The notion of similarity measure is not precise.

[fn:16] In linear algebra, the equivalence is symmetric.

[fn:15] The equivalent property in linear algebra is orthogonality.

[fn:14] All the details were not proven. Just a detour.

[fn:13] In general projection need not be linear, but here it is linear.

[fn:12] This is similar to the theorem we did in Discrete Geometry 1

[fn:11] Apparently the $1$ norm cannot be induced by an inner product. There is
something more going on here. That's because the statement is only true if
$\langle x, x \rangle = \Vert x \Vert^2$, i.e., the space should already be
induced by an inner product. This is not true in the first place for $l^1$ norm.

[fn:10] We didn't prove this.

[fn:9] Idea of machine learning. We have two sets of points, we need to find a
function (often linear) that separates the points. But it may not be easy. So
one idea is to find a linear function to a higher dimensional space and then one
may be able to separate the points in the higher dimensional space.

[fn:8] These are called Kernels. But kernels in Machine learning is $k(x, y)
=\langle \phi(x), \phi(y)\rangle$, for example $h(x, y) = \exp(-\Vert x -
y\Vert/\varepsilon$ is called a Gaussian kernel. Another example is $h(x, y) =
(1+\langle x, y\rangle)^p$ polynomial kernel

[fn:7] This is an unbounded operator. We still haven't defined what bounded
operators are, though.

[fn:6] We might not need this property. 

[fn:5] The proof wasn't done in the class. But the idea is the all Cauchy
sequences converges.

[fn:4] This might be needed for the next exercises.

[fn:3] Is compactness needed?

[fn:2] Look up at proofs of this fact

[fn:1] A14 1126 tutorial
* Study
** Example absolute convergence
** About properties of $l_0$ and $l_\infty$
** Riesz's lemma
   Application to closed ball being compact and this being same as equivalent to
   infinite dimensionality.
** Derivative operator
** For linear operators injective and bijective are equivalent for finite dimensional case?
   Theorem 6.3
** About equivalence of $l^p$ and $l^q$
** Integral normed space
   Why is $C[a, b]$ with the integral norm not an inner product space?
** Besel's inequality and fourier coefficients
** Orthonormal Polynomials
** TODO Planchvechal's theorem
** When is $l_p$ a subset of $l_q$?
** Counterexamples
   1. $l^\infty$ is not separable.
   2. Absolute convergence: see 4.3 [[*Absolute convergence][Absolute convergence]]
   3. Unbounded operator. For the space of polynomials on $[a, b]$, the operator
      $Tf = f'$ is unbounded. This is because, for the sequence $p_n = x^n$,
      $\Vert p_n \Vert \rightarrow 0$ as $n \rightarrow \infty$, but $\Vert
      p_n'\Vert \rightarrow \infty$ as $n\rightarrow \infty$.
   4. Example of a normed space that is not an inner product space.
** Theorem 7.4
   I don't really understand theorem 7.4. I think 
** Lemma 12.5
   What is going on here?
